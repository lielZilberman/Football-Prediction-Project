{"version":3,"sources":["../../../src/generated/tree/DecisionTreeRegressor.ts"],"sourcesContent":["/* eslint-disable */\n/* NOTE: This file is auto-generated. Do not edit it directly. */\n\nimport crypto from 'node:crypto'\n\nimport { PythonBridge, NDArray, ArrayLike, SparseMatrix } from '@/sklearn/types'\n\n/**\n  A decision tree regressor.\n\n  Read more in the [User Guide](../tree.html#tree).\n\n  [Python Reference](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html)\n */\nexport class DecisionTreeRegressor {\n  id: string\n  opts: any\n\n  _py: PythonBridge\n  _isInitialized: boolean = false\n  _isDisposed: boolean = false\n\n  constructor(opts?: {\n    /**\n      The function to measure the quality of a split. Supported criteria are “squared\\_error” for the mean squared error, which is equal to variance reduction as feature selection criterion and minimizes the L2 loss using the mean of each terminal node, “friedman\\_mse”, which uses mean squared error with Friedman’s improvement score for potential splits, “absolute\\_error” for the mean absolute error, which minimizes the L1 loss using the median of each terminal node, and “poisson” which uses reduction in Poisson deviance to find splits.\n\n      @defaultValue `'squared_error'`\n     */\n    criterion?: 'squared_error' | 'friedman_mse' | 'absolute_error' | 'poisson'\n\n    /**\n      The strategy used to choose the split at each node. Supported strategies are “best” to choose the best split and “random” to choose the best random split.\n\n      @defaultValue `'best'`\n     */\n    splitter?: 'best' | 'random'\n\n    /**\n      The maximum depth of the tree. If `undefined`, then nodes are expanded until all leaves are pure or until all leaves contain less than min\\_samples\\_split samples.\n     */\n    max_depth?: number\n\n    /**\n      The minimum number of samples required to split an internal node:\n\n      @defaultValue `2`\n     */\n    min_samples_split?: number\n\n    /**\n      The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least `min\\_samples\\_leaf` training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.\n\n      @defaultValue `1`\n     */\n    min_samples_leaf?: number\n\n    /**\n      The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample\\_weight is not provided.\n\n      @defaultValue `0`\n     */\n    min_weight_fraction_leaf?: number\n\n    /**\n      The number of features to consider when looking for the best split:\n     */\n    max_features?: number | 'auto' | 'sqrt' | 'log2'\n\n    /**\n      Controls the randomness of the estimator. The features are always randomly permuted at each split, even if `splitter` is set to `\"best\"`. When `max\\_features < n\\_features`, the algorithm will select `max\\_features` at random at each split before finding the best split among them. But the best found split may vary across different runs, even if `max\\_features=n\\_features`. That is the case, if the improvement of the criterion is identical for several splits and one split has to be selected at random. To obtain a deterministic behaviour during fitting, `random\\_state` has to be fixed to an integer. See [Glossary](../../glossary.html#term-random_state) for details.\n     */\n    random_state?: number\n\n    /**\n      Grow a tree with `max\\_leaf\\_nodes` in best-first fashion. Best nodes are defined as relative reduction in impurity. If `undefined` then unlimited number of leaf nodes.\n     */\n    max_leaf_nodes?: number\n\n    /**\n      A node will be split if this split induces a decrease of the impurity greater than or equal to this value.\n\n      The weighted impurity decrease equation is the following:\n\n      @defaultValue `0`\n     */\n    min_impurity_decrease?: number\n\n    /**\n      Complexity parameter used for Minimal Cost-Complexity Pruning. The subtree with the largest cost complexity that is smaller than `ccp\\_alpha` will be chosen. By default, no pruning is performed. See [Minimal Cost-Complexity Pruning](../tree.html#minimal-cost-complexity-pruning) for details.\n\n      @defaultValue `0`\n     */\n    ccp_alpha?: any\n  }) {\n    this.id = `DecisionTreeRegressor${crypto.randomUUID().split('-')[0]}`\n    this.opts = opts || {}\n  }\n\n  get py(): PythonBridge {\n    return this._py\n  }\n\n  set py(pythonBridge: PythonBridge) {\n    this._py = pythonBridge\n  }\n\n  /**\n    Initializes the underlying Python resources.\n\n    This instance is not usable until the `Promise` returned by `init()` resolves.\n   */\n  async init(py: PythonBridge): Promise<void> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This DecisionTreeRegressor instance has already been disposed'\n      )\n    }\n\n    if (this._isInitialized) {\n      return\n    }\n\n    if (!py) {\n      throw new Error(\n        'DecisionTreeRegressor.init requires a PythonBridge instance'\n      )\n    }\n\n    this._py = py\n\n    await this._py.ex`\nimport numpy as np\nfrom sklearn.tree import DecisionTreeRegressor\ntry: bridgeDecisionTreeRegressor\nexcept NameError: bridgeDecisionTreeRegressor = {}\n`\n\n    // set up constructor params\n    await this._py.ex`ctor_DecisionTreeRegressor = {'criterion': ${\n      this.opts['criterion'] ?? undefined\n    }, 'splitter': ${this.opts['splitter'] ?? undefined}, 'max_depth': ${\n      this.opts['max_depth'] ?? undefined\n    }, 'min_samples_split': ${\n      this.opts['min_samples_split'] ?? undefined\n    }, 'min_samples_leaf': ${\n      this.opts['min_samples_leaf'] ?? undefined\n    }, 'min_weight_fraction_leaf': ${\n      this.opts['min_weight_fraction_leaf'] ?? undefined\n    }, 'max_features': ${\n      this.opts['max_features'] ?? undefined\n    }, 'random_state': ${\n      this.opts['random_state'] ?? undefined\n    }, 'max_leaf_nodes': ${\n      this.opts['max_leaf_nodes'] ?? undefined\n    }, 'min_impurity_decrease': ${\n      this.opts['min_impurity_decrease'] ?? undefined\n    }, 'ccp_alpha': ${this.opts['ccp_alpha'] ?? undefined}}\n\nctor_DecisionTreeRegressor = {k: v for k, v in ctor_DecisionTreeRegressor.items() if v is not None}`\n\n    await this._py\n      .ex`bridgeDecisionTreeRegressor[${this.id}] = DecisionTreeRegressor(**ctor_DecisionTreeRegressor)`\n\n    this._isInitialized = true\n  }\n\n  /**\n    Disposes of the underlying Python resources.\n\n    Once `dispose()` is called, the instance is no longer usable.\n   */\n  async dispose() {\n    if (this._isDisposed) {\n      return\n    }\n\n    if (!this._isInitialized) {\n      return\n    }\n\n    await this._py.ex`del bridgeDecisionTreeRegressor[${this.id}]`\n\n    this._isDisposed = true\n  }\n\n  /**\n    Return the index of the leaf that each sample is predicted as.\n   */\n  async apply(opts: {\n    /**\n      The input samples. Internally, it will be converted to `dtype=np.float32` and if a sparse matrix is provided to a sparse `csr\\_matrix`.\n     */\n    X?: ArrayLike | SparseMatrix[]\n\n    /**\n      Allow to bypass several input checking. Don’t use this parameter unless you know what you’re doing.\n\n      @defaultValue `true`\n     */\n    check_input?: boolean\n  }): Promise<ArrayLike> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This DecisionTreeRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('DecisionTreeRegressor must call init() before apply()')\n    }\n\n    // set up method params\n    await this._py.ex`pms_DecisionTreeRegressor_apply = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None, 'check_input': ${\n      opts['check_input'] ?? undefined\n    }}\n\npms_DecisionTreeRegressor_apply = {k: v for k, v in pms_DecisionTreeRegressor_apply.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_DecisionTreeRegressor_apply = bridgeDecisionTreeRegressor[${this.id}].apply(**pms_DecisionTreeRegressor_apply)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_DecisionTreeRegressor_apply.tolist() if hasattr(res_DecisionTreeRegressor_apply, 'tolist') else res_DecisionTreeRegressor_apply`\n  }\n\n  /**\n    Compute the pruning path during Minimal Cost-Complexity Pruning.\n\n    See [Minimal Cost-Complexity Pruning](../tree.html#minimal-cost-complexity-pruning) for details on the pruning process.\n   */\n  async cost_complexity_pruning_path(opts: {\n    /**\n      The training input samples. Internally, it will be converted to `dtype=np.float32` and if a sparse matrix is provided to a sparse `csc\\_matrix`.\n     */\n    X?: ArrayLike | SparseMatrix[]\n\n    /**\n      The target values (class labels) as integers or strings.\n     */\n    y?: ArrayLike\n\n    /**\n      Sample weights. If `undefined`, then samples are equally weighted. Splits that would create child nodes with net zero or negative weight are ignored while searching for a split in each node. Splits are also ignored if they would result in any single class carrying a negative weight in either child node.\n     */\n    sample_weight?: ArrayLike\n  }): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This DecisionTreeRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'DecisionTreeRegressor must call init() before cost_complexity_pruning_path()'\n      )\n    }\n\n    // set up method params\n    await this._py\n      .ex`pms_DecisionTreeRegressor_cost_complexity_pruning_path = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None, 'y': np.array(${\n      opts['y'] ?? undefined\n    }) if ${opts['y'] !== undefined} else None, 'sample_weight': np.array(${\n      opts['sample_weight'] ?? undefined\n    }) if ${opts['sample_weight'] !== undefined} else None}\n\npms_DecisionTreeRegressor_cost_complexity_pruning_path = {k: v for k, v in pms_DecisionTreeRegressor_cost_complexity_pruning_path.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_DecisionTreeRegressor_cost_complexity_pruning_path = bridgeDecisionTreeRegressor[${this.id}].cost_complexity_pruning_path(**pms_DecisionTreeRegressor_cost_complexity_pruning_path)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_DecisionTreeRegressor_cost_complexity_pruning_path.tolist() if hasattr(res_DecisionTreeRegressor_cost_complexity_pruning_path, 'tolist') else res_DecisionTreeRegressor_cost_complexity_pruning_path`\n  }\n\n  /**\n    Return the decision path in the tree.\n   */\n  async decision_path(opts: {\n    /**\n      The input samples. Internally, it will be converted to `dtype=np.float32` and if a sparse matrix is provided to a sparse `csr\\_matrix`.\n     */\n    X?: ArrayLike | SparseMatrix[]\n\n    /**\n      Allow to bypass several input checking. Don’t use this parameter unless you know what you’re doing.\n\n      @defaultValue `true`\n     */\n    check_input?: boolean\n  }): Promise<SparseMatrix[]> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This DecisionTreeRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'DecisionTreeRegressor must call init() before decision_path()'\n      )\n    }\n\n    // set up method params\n    await this._py\n      .ex`pms_DecisionTreeRegressor_decision_path = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None, 'check_input': ${\n      opts['check_input'] ?? undefined\n    }}\n\npms_DecisionTreeRegressor_decision_path = {k: v for k, v in pms_DecisionTreeRegressor_decision_path.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_DecisionTreeRegressor_decision_path = bridgeDecisionTreeRegressor[${this.id}].decision_path(**pms_DecisionTreeRegressor_decision_path)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_DecisionTreeRegressor_decision_path.tolist() if hasattr(res_DecisionTreeRegressor_decision_path, 'tolist') else res_DecisionTreeRegressor_decision_path`\n  }\n\n  /**\n    Build a decision tree regressor from the training set (X, y).\n   */\n  async fit(opts: {\n    /**\n      The training input samples. Internally, it will be converted to `dtype=np.float32` and if a sparse matrix is provided to a sparse `csc\\_matrix`.\n     */\n    X?: ArrayLike | SparseMatrix[]\n\n    /**\n      The target values (real numbers). Use `dtype=np.float64` and `order='C'` for maximum efficiency.\n     */\n    y?: ArrayLike\n\n    /**\n      Sample weights. If `undefined`, then samples are equally weighted. Splits that would create child nodes with net zero or negative weight are ignored while searching for a split in each node.\n     */\n    sample_weight?: ArrayLike\n\n    /**\n      Allow to bypass several input checking. Don’t use this parameter unless you know what you’re doing.\n\n      @defaultValue `true`\n     */\n    check_input?: boolean\n  }): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This DecisionTreeRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('DecisionTreeRegressor must call init() before fit()')\n    }\n\n    // set up method params\n    await this._py.ex`pms_DecisionTreeRegressor_fit = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None, 'y': np.array(${\n      opts['y'] ?? undefined\n    }) if ${opts['y'] !== undefined} else None, 'sample_weight': np.array(${\n      opts['sample_weight'] ?? undefined\n    }) if ${opts['sample_weight'] !== undefined} else None, 'check_input': ${\n      opts['check_input'] ?? undefined\n    }}\n\npms_DecisionTreeRegressor_fit = {k: v for k, v in pms_DecisionTreeRegressor_fit.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_DecisionTreeRegressor_fit = bridgeDecisionTreeRegressor[${this.id}].fit(**pms_DecisionTreeRegressor_fit)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_DecisionTreeRegressor_fit.tolist() if hasattr(res_DecisionTreeRegressor_fit, 'tolist') else res_DecisionTreeRegressor_fit`\n  }\n\n  /**\n    Return the depth of the decision tree.\n\n    The depth of a tree is the maximum distance between the root and any leaf.\n   */\n  async get_depth(opts: {}): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This DecisionTreeRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'DecisionTreeRegressor must call init() before get_depth()'\n      )\n    }\n\n    // set up method params\n    await this._py.ex`pms_DecisionTreeRegressor_get_depth = {}\n\npms_DecisionTreeRegressor_get_depth = {k: v for k, v in pms_DecisionTreeRegressor_get_depth.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_DecisionTreeRegressor_get_depth = bridgeDecisionTreeRegressor[${this.id}].get_depth(**pms_DecisionTreeRegressor_get_depth)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_DecisionTreeRegressor_get_depth.tolist() if hasattr(res_DecisionTreeRegressor_get_depth, 'tolist') else res_DecisionTreeRegressor_get_depth`\n  }\n\n  /**\n    Return the number of leaves of the decision tree.\n   */\n  async get_n_leaves(opts: {}): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This DecisionTreeRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'DecisionTreeRegressor must call init() before get_n_leaves()'\n      )\n    }\n\n    // set up method params\n    await this._py.ex`pms_DecisionTreeRegressor_get_n_leaves = {}\n\npms_DecisionTreeRegressor_get_n_leaves = {k: v for k, v in pms_DecisionTreeRegressor_get_n_leaves.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_DecisionTreeRegressor_get_n_leaves = bridgeDecisionTreeRegressor[${this.id}].get_n_leaves(**pms_DecisionTreeRegressor_get_n_leaves)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_DecisionTreeRegressor_get_n_leaves.tolist() if hasattr(res_DecisionTreeRegressor_get_n_leaves, 'tolist') else res_DecisionTreeRegressor_get_n_leaves`\n  }\n\n  /**\n    Predict class or regression value for X.\n\n    For a classification model, the predicted class for each sample in X is returned. For a regression model, the predicted value based on X is returned.\n   */\n  async predict(opts: {\n    /**\n      The input samples. Internally, it will be converted to `dtype=np.float32` and if a sparse matrix is provided to a sparse `csr\\_matrix`.\n     */\n    X?: ArrayLike | SparseMatrix[]\n\n    /**\n      Allow to bypass several input checking. Don’t use this parameter unless you know what you’re doing.\n\n      @defaultValue `true`\n     */\n    check_input?: boolean\n  }): Promise<ArrayLike> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This DecisionTreeRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('DecisionTreeRegressor must call init() before predict()')\n    }\n\n    // set up method params\n    await this._py.ex`pms_DecisionTreeRegressor_predict = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None, 'check_input': ${\n      opts['check_input'] ?? undefined\n    }}\n\npms_DecisionTreeRegressor_predict = {k: v for k, v in pms_DecisionTreeRegressor_predict.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_DecisionTreeRegressor_predict = bridgeDecisionTreeRegressor[${this.id}].predict(**pms_DecisionTreeRegressor_predict)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_DecisionTreeRegressor_predict.tolist() if hasattr(res_DecisionTreeRegressor_predict, 'tolist') else res_DecisionTreeRegressor_predict`\n  }\n\n  /**\n    Return the coefficient of determination of the prediction.\n\n    The coefficient of determination \\\\(R^2\\\\) is defined as \\\\((1 - \\\\frac{u}{v})\\\\), where \\\\(u\\\\) is the residual sum of squares `((y\\_true \\- y\\_pred)\\*\\* 2).sum()` and \\\\(v\\\\) is the total sum of squares `((y\\_true \\- y\\_true.mean()) \\*\\* 2).sum()`. The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of `y`, disregarding the input features, would get a \\\\(R^2\\\\) score of 0.0.\n   */\n  async score(opts: {\n    /**\n      Test samples. For some estimators this may be a precomputed kernel matrix or a list of generic objects instead with shape `(n\\_samples, n\\_samples\\_fitted)`, where `n\\_samples\\_fitted` is the number of samples used in the fitting for the estimator.\n     */\n    X?: ArrayLike[]\n\n    /**\n      True values for `X`.\n     */\n    y?: ArrayLike\n\n    /**\n      Sample weights.\n     */\n    sample_weight?: ArrayLike\n  }): Promise<number> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This DecisionTreeRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('DecisionTreeRegressor must call init() before score()')\n    }\n\n    // set up method params\n    await this._py.ex`pms_DecisionTreeRegressor_score = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None, 'y': np.array(${\n      opts['y'] ?? undefined\n    }) if ${opts['y'] !== undefined} else None, 'sample_weight': np.array(${\n      opts['sample_weight'] ?? undefined\n    }) if ${opts['sample_weight'] !== undefined} else None}\n\npms_DecisionTreeRegressor_score = {k: v for k, v in pms_DecisionTreeRegressor_score.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_DecisionTreeRegressor_score = bridgeDecisionTreeRegressor[${this.id}].score(**pms_DecisionTreeRegressor_score)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_DecisionTreeRegressor_score.tolist() if hasattr(res_DecisionTreeRegressor_score, 'tolist') else res_DecisionTreeRegressor_score`\n  }\n\n  /**\n    The inferred value of max\\_features.\n   */\n  get max_features_(): Promise<number> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This DecisionTreeRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'DecisionTreeRegressor must call init() before accessing max_features_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_DecisionTreeRegressor_max_features_ = bridgeDecisionTreeRegressor[${this.id}].max_features_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_DecisionTreeRegressor_max_features_.tolist() if hasattr(attr_DecisionTreeRegressor_max_features_, 'tolist') else attr_DecisionTreeRegressor_max_features_`\n    })()\n  }\n\n  /**\n    Number of features seen during [fit](../../glossary.html#term-fit).\n   */\n  get n_features_in_(): Promise<number> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This DecisionTreeRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'DecisionTreeRegressor must call init() before accessing n_features_in_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_DecisionTreeRegressor_n_features_in_ = bridgeDecisionTreeRegressor[${this.id}].n_features_in_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_DecisionTreeRegressor_n_features_in_.tolist() if hasattr(attr_DecisionTreeRegressor_n_features_in_, 'tolist') else attr_DecisionTreeRegressor_n_features_in_`\n    })()\n  }\n\n  /**\n    Names of features seen during [fit](../../glossary.html#term-fit). Defined only when `X` has feature names that are all strings.\n   */\n  get feature_names_in_(): Promise<NDArray> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This DecisionTreeRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'DecisionTreeRegressor must call init() before accessing feature_names_in_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_DecisionTreeRegressor_feature_names_in_ = bridgeDecisionTreeRegressor[${this.id}].feature_names_in_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_DecisionTreeRegressor_feature_names_in_.tolist() if hasattr(attr_DecisionTreeRegressor_feature_names_in_, 'tolist') else attr_DecisionTreeRegressor_feature_names_in_`\n    })()\n  }\n\n  /**\n    The number of outputs when `fit` is performed.\n   */\n  get n_outputs_(): Promise<number> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This DecisionTreeRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'DecisionTreeRegressor must call init() before accessing n_outputs_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_DecisionTreeRegressor_n_outputs_ = bridgeDecisionTreeRegressor[${this.id}].n_outputs_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_DecisionTreeRegressor_n_outputs_.tolist() if hasattr(attr_DecisionTreeRegressor_n_outputs_, 'tolist') else attr_DecisionTreeRegressor_n_outputs_`\n    })()\n  }\n\n  /**\n    The underlying Tree object. Please refer to `help(sklearn.tree.\\_tree.Tree)` for attributes of Tree object and [Understanding the decision tree structure](../../auto_examples/tree/plot_unveil_tree_structure.html#sphx-glr-auto-examples-tree-plot-unveil-tree-structure-py) for basic usage of these attributes.\n   */\n  get tree_(): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This DecisionTreeRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'DecisionTreeRegressor must call init() before accessing tree_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_DecisionTreeRegressor_tree_ = bridgeDecisionTreeRegressor[${this.id}].tree_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_DecisionTreeRegressor_tree_.tolist() if hasattr(attr_DecisionTreeRegressor_tree_, 'tolist') else attr_DecisionTreeRegressor_tree_`\n    })()\n  }\n}\n"],"mappings":";AAGA,OAAO,YAAY;AAWZ,IAAM,wBAAN,MAA4B;AAAA,EAQjC,YAAY,MAuET;AA1EH,0BAA0B;AAC1B,uBAAuB;AA0ErB,SAAK,KAAK,wBAAwB,OAAO,WAAW,EAAE,MAAM,GAAG,EAAE,CAAC;AAClE,SAAK,OAAO,QAAQ,CAAC;AAAA,EACvB;AAAA,EAEA,IAAI,KAAmB;AACrB,WAAO,KAAK;AAAA,EACd;AAAA,EAEA,IAAI,GAAG,cAA4B;AACjC,SAAK,MAAM;AAAA,EACb;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,KAAK,IAAiC;AAC1C,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,KAAK,gBAAgB;AACvB;AAAA,IACF;AAEA,QAAI,CAAC,IAAI;AACP,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,SAAK,MAAM;AAEX,UAAM,KAAK,IAAI;AAAA;AAAA;AAAA;AAAA;AAAA;AAQf,UAAM,KAAK,IAAI,gDACb,KAAK,KAAK,WAAW,KAAK,uBACX,KAAK,KAAK,UAAU,KAAK,wBACxC,KAAK,KAAK,WAAW,KAAK,gCAE1B,KAAK,KAAK,mBAAmB,KAAK,+BAElC,KAAK,KAAK,kBAAkB,KAAK,uCAEjC,KAAK,KAAK,0BAA0B,KAAK,2BAEzC,KAAK,KAAK,cAAc,KAAK,2BAE7B,KAAK,KAAK,cAAc,KAAK,6BAE7B,KAAK,KAAK,gBAAgB,KAAK,oCAE/B,KAAK,KAAK,uBAAuB,KAAK,wBACtB,KAAK,KAAK,WAAW,KAAK;AAAA;AAAA;AAI5C,UAAM,KAAK,IACR,iCAAiC,KAAK;AAEzC,SAAK,iBAAiB;AAAA,EACxB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,UAAU;AACd,QAAI,KAAK,aAAa;AACpB;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB;AAAA,IACF;AAEA,UAAM,KAAK,IAAI,qCAAqC,KAAK;AAEzD,SAAK,cAAc;AAAA,EACrB;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,MAAM,MAYW;AACrB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,uDAAuD;AAAA,IACzE;AAGA,UAAM,KAAK,IAAI,sDACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,oCACpB,KAAK,aAAa,KAAK;AAAA;AAAA;AAMzB,UAAM,KAAK,IACR,mEAAmE,KAAK;AAG3E,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,6BAA6B,MAelB;AACf,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IACR,6EACD,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,mCACpB,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,+CACpB,KAAK,eAAe,KAAK,cACnB,KAAK,eAAe,MAAM;AAAA;AAAA;AAKlC,UAAM,KAAK,IACR,0FAA0F,KAAK;AAGlG,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,cAAc,MAYQ;AAC1B,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IACR,8DACD,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,oCACpB,KAAK,aAAa,KAAK;AAAA;AAAA;AAMzB,UAAM,KAAK,IACR,2EAA2E,KAAK;AAGnF,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,IAAI,MAsBO;AACf,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,qDAAqD;AAAA,IACvE;AAGA,UAAM,KAAK,IAAI,oDACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,mCACpB,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,+CACpB,KAAK,eAAe,KAAK,cACnB,KAAK,eAAe,MAAM,oCAChC,KAAK,aAAa,KAAK;AAAA;AAAA;AAMzB,UAAM,KAAK,IACR,iEAAiE,KAAK;AAGzE,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,UAAU,MAAwB;AACtC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IAAI;AAAA;AAAA;AAKf,UAAM,KAAK,IACR,uEAAuE,KAAK;AAG/E,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,aAAa,MAAwB;AACzC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IAAI;AAAA;AAAA;AAKf,UAAM,KAAK,IACR,0EAA0E,KAAK;AAGlF,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,QAAQ,MAYS;AACrB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,yDAAyD;AAAA,IAC3E;AAGA,UAAM,KAAK,IAAI,wDACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,oCACpB,KAAK,aAAa,KAAK;AAAA;AAAA;AAMzB,UAAM,KAAK,IACR,qEAAqE,KAAK;AAG7E,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,MAAM,MAeQ;AAClB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,uDAAuD;AAAA,IACzE;AAGA,UAAM,KAAK,IAAI,sDACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,mCACpB,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,+CACpB,KAAK,eAAe,KAAK,cACnB,KAAK,eAAe,MAAM;AAAA;AAAA;AAKlC,UAAM,KAAK,IACR,mEAAmE,KAAK;AAG3E,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,gBAAiC;AACnC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,4EAA4E,KAAK;AAGpF,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,iBAAkC;AACpC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,6EAA6E,KAAK;AAGrF,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,oBAAsC;AACxC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,gFAAgF,KAAK;AAGxF,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,aAA8B;AAChC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,yEAAyE,KAAK;AAGjF,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,QAAsB;AACxB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,oEAAoE,KAAK;AAG5E,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AACF;","names":[]}