import { PythonBridge, NDArray, ArrayLike, SparseMatrix } from '@/sklearn/types';
/**
  An extremely randomized tree regressor.

  Extra-trees differ from classic decision trees in the way they are built. When looking for the best split to separate the samples of a node into two groups, random splits are drawn for each of the `max\_features` randomly selected features and the best split among those is chosen. When `max\_features` is set 1, this amounts to building a totally random decision tree.

  Warning: Extra-trees should only be used within ensemble methods.

  Read more in the [User Guide](../tree.html#tree).

  [Python Reference](https://scikit-learn.org/stable/modules/generated/sklearn.tree.ExtraTreeRegressor.html)
 */
export declare class ExtraTreeRegressor {
    id: string;
    opts: any;
    _py: PythonBridge;
    _isInitialized: boolean;
    _isDisposed: boolean;
    constructor(opts?: {
        /**
          The function to measure the quality of a split. Supported criteria are “squared\_error” for the mean squared error, which is equal to variance reduction as feature selection criterion and minimizes the L2 loss using the mean of each terminal node, “friedman\_mse”, which uses mean squared error with Friedman’s improvement score for potential splits, “absolute\_error” for the mean absolute error, which minimizes the L1 loss using the median of each terminal node, and “poisson” which uses reduction in Poisson deviance to find splits.
    
          @defaultValue `'squared_error'`
         */
        criterion?: 'squared_error' | 'friedman_mse' | 'absolute_error' | 'poisson';
        /**
          The strategy used to choose the split at each node. Supported strategies are “best” to choose the best split and “random” to choose the best random split.
    
          @defaultValue `'random'`
         */
        splitter?: 'random' | 'best';
        /**
          The maximum depth of the tree. If `undefined`, then nodes are expanded until all leaves are pure or until all leaves contain less than min\_samples\_split samples.
         */
        max_depth?: number;
        /**
          The minimum number of samples required to split an internal node:
    
          @defaultValue `2`
         */
        min_samples_split?: number;
        /**
          The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least `min\_samples\_leaf` training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.
    
          @defaultValue `1`
         */
        min_samples_leaf?: number;
        /**
          The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample\_weight is not provided.
    
          @defaultValue `0`
         */
        min_weight_fraction_leaf?: number;
        /**
          The number of features to consider when looking for the best split:
    
          @defaultValue `1`
         */
        max_features?: number | 'sqrt';
        /**
          Used to pick randomly the `max\_features` used at each split. See [Glossary](../../glossary.html#term-random_state) for details.
         */
        random_state?: number;
        /**
          A node will be split if this split induces a decrease of the impurity greater than or equal to this value.
    
          The weighted impurity decrease equation is the following:
    
          @defaultValue `0`
         */
        min_impurity_decrease?: number;
        /**
          Grow a tree with `max\_leaf\_nodes` in best-first fashion. Best nodes are defined as relative reduction in impurity. If `undefined` then unlimited number of leaf nodes.
         */
        max_leaf_nodes?: number;
        /**
          Complexity parameter used for Minimal Cost-Complexity Pruning. The subtree with the largest cost complexity that is smaller than `ccp\_alpha` will be chosen. By default, no pruning is performed. See [Minimal Cost-Complexity Pruning](../tree.html#minimal-cost-complexity-pruning) for details.
    
          @defaultValue `0`
         */
        ccp_alpha?: any;
    });
    get py(): PythonBridge;
    set py(pythonBridge: PythonBridge);
    /**
      Initializes the underlying Python resources.
  
      This instance is not usable until the `Promise` returned by `init()` resolves.
     */
    init(py: PythonBridge): Promise<void>;
    /**
      Disposes of the underlying Python resources.
  
      Once `dispose()` is called, the instance is no longer usable.
     */
    dispose(): Promise<void>;
    /**
      Return the index of the leaf that each sample is predicted as.
     */
    apply(opts: {
        /**
          The input samples. Internally, it will be converted to `dtype=np.float32` and if a sparse matrix is provided to a sparse `csr\_matrix`.
         */
        X?: ArrayLike | SparseMatrix[];
        /**
          Allow to bypass several input checking. Don’t use this parameter unless you know what you’re doing.
    
          @defaultValue `true`
         */
        check_input?: boolean;
    }): Promise<ArrayLike>;
    /**
      Compute the pruning path during Minimal Cost-Complexity Pruning.
  
      See [Minimal Cost-Complexity Pruning](../tree.html#minimal-cost-complexity-pruning) for details on the pruning process.
     */
    cost_complexity_pruning_path(opts: {
        /**
          The training input samples. Internally, it will be converted to `dtype=np.float32` and if a sparse matrix is provided to a sparse `csc\_matrix`.
         */
        X?: ArrayLike | SparseMatrix[];
        /**
          The target values (class labels) as integers or strings.
         */
        y?: ArrayLike;
        /**
          Sample weights. If `undefined`, then samples are equally weighted. Splits that would create child nodes with net zero or negative weight are ignored while searching for a split in each node. Splits are also ignored if they would result in any single class carrying a negative weight in either child node.
         */
        sample_weight?: ArrayLike;
    }): Promise<any>;
    /**
      Return the decision path in the tree.
     */
    decision_path(opts: {
        /**
          The input samples. Internally, it will be converted to `dtype=np.float32` and if a sparse matrix is provided to a sparse `csr\_matrix`.
         */
        X?: ArrayLike | SparseMatrix[];
        /**
          Allow to bypass several input checking. Don’t use this parameter unless you know what you’re doing.
    
          @defaultValue `true`
         */
        check_input?: boolean;
    }): Promise<SparseMatrix[]>;
    /**
      Build a decision tree regressor from the training set (X, y).
     */
    fit(opts: {
        /**
          The training input samples. Internally, it will be converted to `dtype=np.float32` and if a sparse matrix is provided to a sparse `csc\_matrix`.
         */
        X?: ArrayLike | SparseMatrix[];
        /**
          The target values (real numbers). Use `dtype=np.float64` and `order='C'` for maximum efficiency.
         */
        y?: ArrayLike;
        /**
          Sample weights. If `undefined`, then samples are equally weighted. Splits that would create child nodes with net zero or negative weight are ignored while searching for a split in each node.
         */
        sample_weight?: ArrayLike;
        /**
          Allow to bypass several input checking. Don’t use this parameter unless you know what you’re doing.
    
          @defaultValue `true`
         */
        check_input?: boolean;
    }): Promise<any>;
    /**
      Return the depth of the decision tree.
  
      The depth of a tree is the maximum distance between the root and any leaf.
     */
    get_depth(opts: {}): Promise<any>;
    /**
      Return the number of leaves of the decision tree.
     */
    get_n_leaves(opts: {}): Promise<any>;
    /**
      Predict class or regression value for X.
  
      For a classification model, the predicted class for each sample in X is returned. For a regression model, the predicted value based on X is returned.
     */
    predict(opts: {
        /**
          The input samples. Internally, it will be converted to `dtype=np.float32` and if a sparse matrix is provided to a sparse `csr\_matrix`.
         */
        X?: ArrayLike | SparseMatrix[];
        /**
          Allow to bypass several input checking. Don’t use this parameter unless you know what you’re doing.
    
          @defaultValue `true`
         */
        check_input?: boolean;
    }): Promise<ArrayLike>;
    /**
      Return the coefficient of determination of the prediction.
  
      The coefficient of determination \\(R^2\\) is defined as \\((1 - \\frac{u}{v})\\), where \\(u\\) is the residual sum of squares `((y\_true \- y\_pred)\*\* 2).sum()` and \\(v\\) is the total sum of squares `((y\_true \- y\_true.mean()) \*\* 2).sum()`. The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of `y`, disregarding the input features, would get a \\(R^2\\) score of 0.0.
     */
    score(opts: {
        /**
          Test samples. For some estimators this may be a precomputed kernel matrix or a list of generic objects instead with shape `(n\_samples, n\_samples\_fitted)`, where `n\_samples\_fitted` is the number of samples used in the fitting for the estimator.
         */
        X?: ArrayLike[];
        /**
          True values for `X`.
         */
        y?: ArrayLike;
        /**
          Sample weights.
         */
        sample_weight?: ArrayLike;
    }): Promise<number>;
    /**
      The inferred value of max\_features.
     */
    get max_features_(): Promise<number>;
    /**
      Number of features seen during [fit](../../glossary.html#term-fit).
     */
    get n_features_in_(): Promise<number>;
    /**
      Names of features seen during [fit](../../glossary.html#term-fit). Defined only when `X` has feature names that are all strings.
     */
    get feature_names_in_(): Promise<NDArray>;
    /**
      The number of outputs when `fit` is performed.
     */
    get n_outputs_(): Promise<number>;
    /**
      The underlying Tree object. Please refer to `help(sklearn.tree.\_tree.Tree)` for attributes of Tree object and [Understanding the decision tree structure](../../auto_examples/tree/plot_unveil_tree_structure.html#sphx-glr-auto-examples-tree-plot-unveil-tree-structure-py) for basic usage of these attributes.
     */
    get tree_(): Promise<any>;
}
//# sourceMappingURL=ExtraTreeRegressor.d.ts.map