import { PythonBridge, ArrayLike, SparseMatrix } from '@/sklearn/types';
/**
  Convert a collection of raw documents to a matrix of TF-IDF features.

  Equivalent to [`CountVectorizer`](sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer "sklearn.feature_extraction.text.CountVectorizer") followed by [`TfidfTransformer`](sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer "sklearn.feature_extraction.text.TfidfTransformer").

  Read more in the [User Guide](../feature_extraction.html#text-feature-extraction).

  [Python Reference](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)
 */
export declare class TfidfVectorizer {
    id: string;
    opts: any;
    _py: PythonBridge;
    _isInitialized: boolean;
    _isDisposed: boolean;
    constructor(opts?: {
        /**
          If `'filename'`, the sequence passed as an argument to fit is expected to be a list of filenames that need reading to fetch the raw content to analyze.
    
          @defaultValue `'content'`
         */
        input?: 'filename' | 'file' | 'content';
        /**
          If bytes or files are given to analyze, this encoding is used to decode.
    
          @defaultValue `'utf-8'`
         */
        encoding?: string;
        /**
          Instruction on what to do if a byte sequence is given to analyze that contains characters not of the given `encoding`. By default, it is ‘strict’, meaning that a UnicodeDecodeError will be raised. Other values are ‘ignore’ and ‘replace’.
    
          @defaultValue `'strict'`
         */
        decode_error?: 'strict' | 'ignore' | 'replace';
        /**
          Remove accents and perform other character normalization during the preprocessing step. ‘ascii’ is a fast method that only works on characters that have a direct ASCII mapping. ‘unicode’ is a slightly slower method that works on any characters. `undefined` (default) does nothing.
    
          Both ‘ascii’ and ‘unicode’ use NFKD normalization from [`unicodedata.normalize`](https://docs.python.org/3/library/unicodedata.html#unicodedata.normalize "(in Python v3.11)").
         */
        strip_accents?: 'ascii' | 'unicode';
        /**
          Convert all characters to lowercase before tokenizing.
    
          @defaultValue `true`
         */
        lowercase?: boolean;
        /**
          Override the preprocessing (string transformation) stage while preserving the tokenizing and n-grams generation steps. Only applies if `analyzer` is not callable.
         */
        preprocessor?: any;
        /**
          Override the string tokenization step while preserving the preprocessing and n-grams generation steps. Only applies if `analyzer \== 'word'`.
         */
        tokenizer?: any;
        /**
          Whether the feature should be made of word or character n-grams. Option ‘char\_wb’ creates character n-grams only from text inside word boundaries; n-grams at the edges of words are padded with space.
    
          If a callable is passed it is used to extract the sequence of features out of the raw, unprocessed input.
    
          @defaultValue `'word'`
         */
        analyzer?: 'word' | 'char' | 'char_wb';
        /**
          If a string, it is passed to \_check\_stop\_list and the appropriate stop list is returned. ‘english’ is currently the only supported string value. There are several known issues with ‘english’ and you should consider an alternative (see [Using stop words](../feature_extraction.html#stop-words)).
    
          If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens. Only applies if `analyzer \== 'word'`.
    
          If `undefined`, no stop words will be used. In this case, setting `max\_df` to a higher value, such as in the range (0.7, 1.0), can automatically detect and filter stop words based on intra corpus document frequency of terms.
         */
        stop_words?: 'english' | any[];
        /**
          Regular expression denoting what constitutes a “token”, only used if `analyzer \== 'word'`. The default regexp selects tokens of 2 or more alphanumeric characters (punctuation is completely ignored and always treated as a token separator).
    
          If there is a capturing group in token\_pattern then the captured group content, not the entire match, becomes the token. At most one capturing group is permitted.
         */
        token_pattern?: string;
        /**
          The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that min\_n <= n <= max\_n will be used. For example an `ngram\_range` of `(1, 1)` means only unigrams, `(1, 2)` means unigrams and bigrams, and `(2, 2)` means only bigrams. Only applies if `analyzer` is not callable.
         */
        ngram_range?: any;
        /**
          When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words). If float in range \[0.0, 1.0\], the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not `undefined`.
    
          @defaultValue `1`
         */
        max_df?: number;
        /**
          When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float in range of \[0.0, 1.0\], the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not `undefined`.
    
          @defaultValue `1`
         */
        min_df?: number;
        /**
          If not `undefined`, build a vocabulary that only consider the top `max\_features` ordered by term frequency across the corpus. Otherwise, all features are used.
    
          This parameter is ignored if vocabulary is not `undefined`.
         */
        max_features?: number;
        /**
          Either a Mapping (e.g., a dict) where keys are terms and values are indices in the feature matrix, or an iterable over terms. If not given, a vocabulary is determined from the input documents.
         */
        vocabulary?: any;
        /**
          If `true`, all non-zero term counts are set to 1. This does not mean outputs will have only 0/1 values, only that the tf term in tf-idf is binary. (Set idf and normalization to `false` to get 0/1 outputs).
    
          @defaultValue `false`
         */
        binary?: boolean;
        /**
          Type of the matrix returned by fit\_transform() or transform().
         */
        dtype?: any;
        /**
          Each output row will have unit norm, either:
    
          @defaultValue `'l2'`
         */
        norm?: 'l1' | 'l2';
        /**
          Enable inverse-document-frequency reweighting. If `false`, idf(t) = 1.
    
          @defaultValue `true`
         */
        use_idf?: boolean;
        /**
          Smooth idf weights by adding one to document frequencies, as if an extra document was seen containing every term in the collection exactly once. Prevents zero divisions.
    
          @defaultValue `true`
         */
        smooth_idf?: boolean;
        /**
          Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).
    
          @defaultValue `false`
         */
        sublinear_tf?: boolean;
    });
    get py(): PythonBridge;
    set py(pythonBridge: PythonBridge);
    /**
      Initializes the underlying Python resources.
  
      This instance is not usable until the `Promise` returned by `init()` resolves.
     */
    init(py: PythonBridge): Promise<void>;
    /**
      Disposes of the underlying Python resources.
  
      Once `dispose()` is called, the instance is no longer usable.
     */
    dispose(): Promise<void>;
    /**
      Return a callable to process input data.
  
      The callable handles preprocessing, tokenization, and n-grams generation.
     */
    build_analyzer(opts: {}): Promise<any>;
    /**
      Return a function to preprocess the text before tokenization.
     */
    build_preprocessor(opts: {}): Promise<any>;
    /**
      Return a function that splits a string into a sequence of tokens.
     */
    build_tokenizer(opts: {}): Promise<any>;
    /**
      Decode the input into a string of unicode symbols.
  
      The decoding strategy depends on the vectorizer parameters.
     */
    decode(opts: {
        /**
          The string to decode.
         */
        doc?: string;
    }): Promise<any>;
    /**
      Learn vocabulary and idf from training set.
     */
    fit(opts: {
        /**
          An iterable which generates either str, unicode or file objects.
         */
        raw_documents?: any;
        /**
          This parameter is not needed to compute tfidf.
         */
        y?: any;
    }): Promise<any>;
    /**
      Learn vocabulary and idf, return document-term matrix.
  
      This is equivalent to fit followed by transform, but more efficiently implemented.
     */
    fit_transform(opts: {
        /**
          An iterable which generates either str, unicode or file objects.
         */
        raw_documents?: any;
        /**
          This parameter is ignored.
         */
        y?: any;
    }): Promise<any>;
    /**
      Get output feature names for transformation.
     */
    get_feature_names_out(opts: {
        /**
          Not used, present here for API consistency by convention.
         */
        input_features?: any;
    }): Promise<any>;
    /**
      Build or fetch the effective stop words list.
     */
    get_stop_words(opts: {}): Promise<any>;
    /**
      Return terms per document with nonzero entries in X.
     */
    inverse_transform(opts: {
        /**
          Document-term matrix.
         */
        X?: ArrayLike | SparseMatrix[];
    }): Promise<any[]>;
    /**
      Transform documents to document-term matrix.
  
      Uses the vocabulary and document frequencies (df) learned by fit (or fit\_transform).
     */
    transform(opts: {
        /**
          An iterable which generates either str, unicode or file objects.
         */
        raw_documents?: any;
    }): Promise<any>;
    /**
      A mapping of terms to feature indices.
     */
    get vocabulary_(): Promise<any>;
    /**
      True if a fixed vocabulary of term to indices mapping is provided by the user.
     */
    get fixed_vocabulary_(): Promise<boolean>;
    /**
      Terms that were ignored because they either:
     */
    get stop_words_(): Promise<any>;
}
//# sourceMappingURL=TfidfVectorizer.d.ts.map