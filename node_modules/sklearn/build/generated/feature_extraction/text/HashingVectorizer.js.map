{"version":3,"sources":["../../../../src/generated/feature_extraction/text/HashingVectorizer.ts"],"sourcesContent":["/* eslint-disable */\n/* NOTE: This file is auto-generated. Do not edit it directly. */\n\nimport crypto from 'node:crypto'\n\nimport { PythonBridge, NDArray, ArrayLike, SparseMatrix } from '@/sklearn/types'\n\n/**\n  Convert a collection of text documents to a matrix of token occurrences.\n\n  It turns a collection of text documents into a scipy.sparse matrix holding token occurrence counts (or binary occurrence information), possibly normalized as token frequencies if norm=’l1’ or projected on the euclidean unit sphere if norm=’l2’.\n\n  This text vectorizer implementation uses the hashing trick to find the token string name to feature integer index mapping.\n\n  This strategy has several advantages:\n\n  [Python Reference](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html)\n */\nexport class HashingVectorizer {\n  id: string\n  opts: any\n\n  _py: PythonBridge\n  _isInitialized: boolean = false\n  _isDisposed: boolean = false\n\n  constructor(opts?: {\n    /**\n      If `'filename'`, the sequence passed as an argument to fit is expected to be a list of filenames that need reading to fetch the raw content to analyze.\n\n      @defaultValue `'content'`\n     */\n    input?: 'filename' | 'file' | 'content'\n\n    /**\n      If bytes or files are given to analyze, this encoding is used to decode.\n\n      @defaultValue `'utf-8'`\n     */\n    encoding?: string\n\n    /**\n      Instruction on what to do if a byte sequence is given to analyze that contains characters not of the given `encoding`. By default, it is ‘strict’, meaning that a UnicodeDecodeError will be raised. Other values are ‘ignore’ and ‘replace’.\n\n      @defaultValue `'strict'`\n     */\n    decode_error?: 'strict' | 'ignore' | 'replace'\n\n    /**\n      Remove accents and perform other character normalization during the preprocessing step. ‘ascii’ is a fast method that only works on characters that have a direct ASCII mapping. ‘unicode’ is a slightly slower method that works on any character. `undefined` (default) does nothing.\n\n      Both ‘ascii’ and ‘unicode’ use NFKD normalization from [`unicodedata.normalize`](https://docs.python.org/3/library/unicodedata.html#unicodedata.normalize \"(in Python v3.11)\").\n     */\n    strip_accents?: 'ascii' | 'unicode'\n\n    /**\n      Convert all characters to lowercase before tokenizing.\n\n      @defaultValue `true`\n     */\n    lowercase?: boolean\n\n    /**\n      Override the preprocessing (string transformation) stage while preserving the tokenizing and n-grams generation steps. Only applies if `analyzer` is not callable.\n     */\n    preprocessor?: any\n\n    /**\n      Override the string tokenization step while preserving the preprocessing and n-grams generation steps. Only applies if `analyzer \\== 'word'`.\n     */\n    tokenizer?: any\n\n    /**\n      If ‘english’, a built-in stop word list for English is used. There are several known issues with ‘english’ and you should consider an alternative (see [Using stop words](../feature_extraction.html#stop-words)).\n\n      If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens. Only applies if `analyzer \\== 'word'`.\n     */\n    stop_words?: 'english' | any[]\n\n    /**\n      Regular expression denoting what constitutes a “token”, only used if `analyzer \\== 'word'`. The default regexp selects tokens of 2 or more alphanumeric characters (punctuation is completely ignored and always treated as a token separator).\n\n      If there is a capturing group in token\\_pattern then the captured group content, not the entire match, becomes the token. At most one capturing group is permitted.\n     */\n    token_pattern?: string\n\n    /**\n      The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that min\\_n <= n <= max\\_n will be used. For example an `ngram\\_range` of `(1, 1)` means only unigrams, `(1, 2)` means unigrams and bigrams, and `(2, 2)` means only bigrams. Only applies if `analyzer` is not callable.\n     */\n    ngram_range?: any\n\n    /**\n      Whether the feature should be made of word or character n-grams. Option ‘char\\_wb’ creates character n-grams only from text inside word boundaries; n-grams at the edges of words are padded with space.\n\n      If a callable is passed it is used to extract the sequence of features out of the raw, unprocessed input.\n\n      @defaultValue `'word'`\n     */\n    analyzer?: 'word' | 'char' | 'char_wb'\n\n    /**\n      The number of features (columns) in the output matrices. Small numbers of features are likely to cause hash collisions, but large numbers will cause larger coefficient dimensions in linear learners.\n     */\n    n_features?: number\n\n    /**\n      If `true`, all non zero counts are set to 1. This is useful for discrete probabilistic models that model binary events rather than integer counts.\n\n      @defaultValue `false`\n     */\n    binary?: boolean\n\n    /**\n      Norm used to normalize term vectors. `undefined` for no normalization.\n\n      @defaultValue `'l2'`\n     */\n    norm?: 'l1' | 'l2'\n\n    /**\n      When `true`, an alternating sign is added to the features as to approximately conserve the inner product in the hashed space even for small n\\_features. This approach is similar to sparse random projection.\n\n      @defaultValue `true`\n     */\n    alternate_sign?: boolean\n\n    /**\n      Type of the matrix returned by fit\\_transform() or transform().\n     */\n    dtype?: any\n  }) {\n    this.id = `HashingVectorizer${crypto.randomUUID().split('-')[0]}`\n    this.opts = opts || {}\n  }\n\n  get py(): PythonBridge {\n    return this._py\n  }\n\n  set py(pythonBridge: PythonBridge) {\n    this._py = pythonBridge\n  }\n\n  /**\n    Initializes the underlying Python resources.\n\n    This instance is not usable until the `Promise` returned by `init()` resolves.\n   */\n  async init(py: PythonBridge): Promise<void> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This HashingVectorizer instance has already been disposed'\n      )\n    }\n\n    if (this._isInitialized) {\n      return\n    }\n\n    if (!py) {\n      throw new Error('HashingVectorizer.init requires a PythonBridge instance')\n    }\n\n    this._py = py\n\n    await this._py.ex`\nimport numpy as np\nfrom sklearn.feature_extraction.text import HashingVectorizer\ntry: bridgeHashingVectorizer\nexcept NameError: bridgeHashingVectorizer = {}\n`\n\n    // set up constructor params\n    await this._py.ex`ctor_HashingVectorizer = {'input': ${\n      this.opts['input'] ?? undefined\n    }, 'encoding': ${this.opts['encoding'] ?? undefined}, 'decode_error': ${\n      this.opts['decode_error'] ?? undefined\n    }, 'strip_accents': ${\n      this.opts['strip_accents'] ?? undefined\n    }, 'lowercase': ${this.opts['lowercase'] ?? undefined}, 'preprocessor': ${\n      this.opts['preprocessor'] ?? undefined\n    }, 'tokenizer': ${this.opts['tokenizer'] ?? undefined}, 'stop_words': ${\n      this.opts['stop_words'] ?? undefined\n    }, 'token_pattern': ${\n      this.opts['token_pattern'] ?? undefined\n    }, 'ngram_range': ${this.opts['ngram_range'] ?? undefined}, 'analyzer': ${\n      this.opts['analyzer'] ?? undefined\n    }, 'n_features': ${this.opts['n_features'] ?? undefined}, 'binary': ${\n      this.opts['binary'] ?? undefined\n    }, 'norm': ${this.opts['norm'] ?? undefined}, 'alternate_sign': ${\n      this.opts['alternate_sign'] ?? undefined\n    }, 'dtype': ${this.opts['dtype'] ?? undefined}}\n\nctor_HashingVectorizer = {k: v for k, v in ctor_HashingVectorizer.items() if v is not None}`\n\n    await this._py\n      .ex`bridgeHashingVectorizer[${this.id}] = HashingVectorizer(**ctor_HashingVectorizer)`\n\n    this._isInitialized = true\n  }\n\n  /**\n    Disposes of the underlying Python resources.\n\n    Once `dispose()` is called, the instance is no longer usable.\n   */\n  async dispose() {\n    if (this._isDisposed) {\n      return\n    }\n\n    if (!this._isInitialized) {\n      return\n    }\n\n    await this._py.ex`del bridgeHashingVectorizer[${this.id}]`\n\n    this._isDisposed = true\n  }\n\n  /**\n    Return a callable to process input data.\n\n    The callable handles preprocessing, tokenization, and n-grams generation.\n   */\n  async build_analyzer(opts: {}): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This HashingVectorizer instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'HashingVectorizer must call init() before build_analyzer()'\n      )\n    }\n\n    // set up method params\n    await this._py.ex`pms_HashingVectorizer_build_analyzer = {}\n\npms_HashingVectorizer_build_analyzer = {k: v for k, v in pms_HashingVectorizer_build_analyzer.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_HashingVectorizer_build_analyzer = bridgeHashingVectorizer[${this.id}].build_analyzer(**pms_HashingVectorizer_build_analyzer)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_HashingVectorizer_build_analyzer.tolist() if hasattr(res_HashingVectorizer_build_analyzer, 'tolist') else res_HashingVectorizer_build_analyzer`\n  }\n\n  /**\n    Return a function to preprocess the text before tokenization.\n   */\n  async build_preprocessor(opts: {}): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This HashingVectorizer instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'HashingVectorizer must call init() before build_preprocessor()'\n      )\n    }\n\n    // set up method params\n    await this._py.ex`pms_HashingVectorizer_build_preprocessor = {}\n\npms_HashingVectorizer_build_preprocessor = {k: v for k, v in pms_HashingVectorizer_build_preprocessor.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_HashingVectorizer_build_preprocessor = bridgeHashingVectorizer[${this.id}].build_preprocessor(**pms_HashingVectorizer_build_preprocessor)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_HashingVectorizer_build_preprocessor.tolist() if hasattr(res_HashingVectorizer_build_preprocessor, 'tolist') else res_HashingVectorizer_build_preprocessor`\n  }\n\n  /**\n    Return a function that splits a string into a sequence of tokens.\n   */\n  async build_tokenizer(opts: {}): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This HashingVectorizer instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'HashingVectorizer must call init() before build_tokenizer()'\n      )\n    }\n\n    // set up method params\n    await this._py.ex`pms_HashingVectorizer_build_tokenizer = {}\n\npms_HashingVectorizer_build_tokenizer = {k: v for k, v in pms_HashingVectorizer_build_tokenizer.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_HashingVectorizer_build_tokenizer = bridgeHashingVectorizer[${this.id}].build_tokenizer(**pms_HashingVectorizer_build_tokenizer)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_HashingVectorizer_build_tokenizer.tolist() if hasattr(res_HashingVectorizer_build_tokenizer, 'tolist') else res_HashingVectorizer_build_tokenizer`\n  }\n\n  /**\n    Decode the input into a string of unicode symbols.\n\n    The decoding strategy depends on the vectorizer parameters.\n   */\n  async decode(opts: {\n    /**\n      The string to decode.\n     */\n    doc?: string\n  }): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This HashingVectorizer instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('HashingVectorizer must call init() before decode()')\n    }\n\n    // set up method params\n    await this._py.ex`pms_HashingVectorizer_decode = {'doc': ${\n      opts['doc'] ?? undefined\n    }}\n\npms_HashingVectorizer_decode = {k: v for k, v in pms_HashingVectorizer_decode.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_HashingVectorizer_decode = bridgeHashingVectorizer[${this.id}].decode(**pms_HashingVectorizer_decode)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_HashingVectorizer_decode.tolist() if hasattr(res_HashingVectorizer_decode, 'tolist') else res_HashingVectorizer_decode`\n  }\n\n  /**\n    Only validates estimator’s parameters.\n\n    This method allows to: (i) validate the estimator’s parameters and (ii) be consistent with the scikit-learn transformer API.\n   */\n  async fit(opts: {\n    /**\n      Training data.\n     */\n    X?: any\n\n    /**\n      Not used, present for API consistency by convention.\n     */\n    y?: any\n  }): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This HashingVectorizer instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('HashingVectorizer must call init() before fit()')\n    }\n\n    // set up method params\n    await this._py.ex`pms_HashingVectorizer_fit = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None, 'y': ${opts['y'] ?? undefined}}\n\npms_HashingVectorizer_fit = {k: v for k, v in pms_HashingVectorizer_fit.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_HashingVectorizer_fit = bridgeHashingVectorizer[${this.id}].fit(**pms_HashingVectorizer_fit)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_HashingVectorizer_fit.tolist() if hasattr(res_HashingVectorizer_fit, 'tolist') else res_HashingVectorizer_fit`\n  }\n\n  /**\n    Transform a sequence of documents to a document-term matrix.\n   */\n  async fit_transform(opts: {\n    /**\n      Samples. Each sample must be a text document (either bytes or unicode strings, file name or file object depending on the constructor argument) which will be tokenized and hashed.\n     */\n    X?: any\n\n    /**\n      Ignored. This parameter exists only for compatibility with sklearn.pipeline.Pipeline.\n     */\n    y?: any\n  }): Promise<SparseMatrix[]> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This HashingVectorizer instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'HashingVectorizer must call init() before fit_transform()'\n      )\n    }\n\n    // set up method params\n    await this._py.ex`pms_HashingVectorizer_fit_transform = {'X': ${\n      opts['X'] ?? undefined\n    }, 'y': ${opts['y'] ?? undefined}}\n\npms_HashingVectorizer_fit_transform = {k: v for k, v in pms_HashingVectorizer_fit_transform.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_HashingVectorizer_fit_transform = bridgeHashingVectorizer[${this.id}].fit_transform(**pms_HashingVectorizer_fit_transform)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_HashingVectorizer_fit_transform.tolist() if hasattr(res_HashingVectorizer_fit_transform, 'tolist') else res_HashingVectorizer_fit_transform`\n  }\n\n  /**\n    Build or fetch the effective stop words list.\n   */\n  async get_stop_words(opts: {}): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This HashingVectorizer instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'HashingVectorizer must call init() before get_stop_words()'\n      )\n    }\n\n    // set up method params\n    await this._py.ex`pms_HashingVectorizer_get_stop_words = {}\n\npms_HashingVectorizer_get_stop_words = {k: v for k, v in pms_HashingVectorizer_get_stop_words.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_HashingVectorizer_get_stop_words = bridgeHashingVectorizer[${this.id}].get_stop_words(**pms_HashingVectorizer_get_stop_words)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_HashingVectorizer_get_stop_words.tolist() if hasattr(res_HashingVectorizer_get_stop_words, 'tolist') else res_HashingVectorizer_get_stop_words`\n  }\n\n  /**\n    Only validates estimator’s parameters.\n\n    This method allows to: (i) validate the estimator’s parameters and (ii) be consistent with the scikit-learn transformer API.\n   */\n  async partial_fit(opts: {\n    /**\n      Training data.\n     */\n    X?: any\n\n    /**\n      Not used, present for API consistency by convention.\n     */\n    y?: any\n  }): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This HashingVectorizer instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('HashingVectorizer must call init() before partial_fit()')\n    }\n\n    // set up method params\n    await this._py.ex`pms_HashingVectorizer_partial_fit = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None, 'y': ${opts['y'] ?? undefined}}\n\npms_HashingVectorizer_partial_fit = {k: v for k, v in pms_HashingVectorizer_partial_fit.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_HashingVectorizer_partial_fit = bridgeHashingVectorizer[${this.id}].partial_fit(**pms_HashingVectorizer_partial_fit)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_HashingVectorizer_partial_fit.tolist() if hasattr(res_HashingVectorizer_partial_fit, 'tolist') else res_HashingVectorizer_partial_fit`\n  }\n\n  /**\n    Set output container.\n\n    See [Introducing the set\\_output API](../../auto_examples/miscellaneous/plot_set_output.html#sphx-glr-auto-examples-miscellaneous-plot-set-output-py) for an example on how to use the API.\n   */\n  async set_output(opts: {\n    /**\n      Configure output of `transform` and `fit\\_transform`.\n     */\n    transform?: 'default' | 'pandas'\n  }): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This HashingVectorizer instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('HashingVectorizer must call init() before set_output()')\n    }\n\n    // set up method params\n    await this._py.ex`pms_HashingVectorizer_set_output = {'transform': ${\n      opts['transform'] ?? undefined\n    }}\n\npms_HashingVectorizer_set_output = {k: v for k, v in pms_HashingVectorizer_set_output.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_HashingVectorizer_set_output = bridgeHashingVectorizer[${this.id}].set_output(**pms_HashingVectorizer_set_output)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_HashingVectorizer_set_output.tolist() if hasattr(res_HashingVectorizer_set_output, 'tolist') else res_HashingVectorizer_set_output`\n  }\n\n  /**\n    Transform a sequence of documents to a document-term matrix.\n   */\n  async transform(opts: {\n    /**\n      Samples. Each sample must be a text document (either bytes or unicode strings, file name or file object depending on the constructor argument) which will be tokenized and hashed.\n     */\n    X?: any\n  }): Promise<SparseMatrix[]> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This HashingVectorizer instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('HashingVectorizer must call init() before transform()')\n    }\n\n    // set up method params\n    await this._py.ex`pms_HashingVectorizer_transform = {'X': ${\n      opts['X'] ?? undefined\n    }}\n\npms_HashingVectorizer_transform = {k: v for k, v in pms_HashingVectorizer_transform.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_HashingVectorizer_transform = bridgeHashingVectorizer[${this.id}].transform(**pms_HashingVectorizer_transform)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_HashingVectorizer_transform.tolist() if hasattr(res_HashingVectorizer_transform, 'tolist') else res_HashingVectorizer_transform`\n  }\n}\n"],"mappings":";AAGA,OAAO,YAAY;AAeZ,IAAM,oBAAN,MAAwB;AAAA,EAQ7B,YAAY,MAwGT;AA3GH,0BAA0B;AAC1B,uBAAuB;AA2GrB,SAAK,KAAK,oBAAoB,OAAO,WAAW,EAAE,MAAM,GAAG,EAAE,CAAC;AAC9D,SAAK,OAAO,QAAQ,CAAC;AAAA,EACvB;AAAA,EAEA,IAAI,KAAmB;AACrB,WAAO,KAAK;AAAA,EACd;AAAA,EAEA,IAAI,GAAG,cAA4B;AACjC,SAAK,MAAM;AAAA,EACb;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,KAAK,IAAiC;AAC1C,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,KAAK,gBAAgB;AACvB;AAAA,IACF;AAEA,QAAI,CAAC,IAAI;AACP,YAAM,IAAI,MAAM,yDAAyD;AAAA,IAC3E;AAEA,SAAK,MAAM;AAEX,UAAM,KAAK,IAAI;AAAA;AAAA;AAAA;AAAA;AAAA;AAQf,UAAM,KAAK,IAAI,wCACb,KAAK,KAAK,OAAO,KAAK,uBACP,KAAK,KAAK,UAAU,KAAK,2BACxC,KAAK,KAAK,cAAc,KAAK,4BAE7B,KAAK,KAAK,eAAe,KAAK,wBACd,KAAK,KAAK,WAAW,KAAK,2BAC1C,KAAK,KAAK,cAAc,KAAK,wBACb,KAAK,KAAK,WAAW,KAAK,yBAC1C,KAAK,KAAK,YAAY,KAAK,4BAE3B,KAAK,KAAK,eAAe,KAAK,0BACZ,KAAK,KAAK,aAAa,KAAK,uBAC9C,KAAK,KAAK,UAAU,KAAK,yBACR,KAAK,KAAK,YAAY,KAAK,qBAC5C,KAAK,KAAK,QAAQ,KAAK,mBACZ,KAAK,KAAK,MAAM,KAAK,6BAChC,KAAK,KAAK,gBAAgB,KAAK,oBACnB,KAAK,KAAK,OAAO,KAAK;AAAA;AAAA;AAIpC,UAAM,KAAK,IACR,6BAA6B,KAAK;AAErC,SAAK,iBAAiB;AAAA,EACxB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,UAAU;AACd,QAAI,KAAK,aAAa;AACpB;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB;AAAA,IACF;AAEA,UAAM,KAAK,IAAI,iCAAiC,KAAK;AAErD,SAAK,cAAc;AAAA,EACrB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,eAAe,MAAwB;AAC3C,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IAAI;AAAA;AAAA;AAKf,UAAM,KAAK,IACR,oEAAoE,KAAK;AAG5E,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,mBAAmB,MAAwB;AAC/C,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IAAI;AAAA;AAAA;AAKf,UAAM,KAAK,IACR,wEAAwE,KAAK;AAGhF,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,gBAAgB,MAAwB;AAC5C,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IAAI;AAAA;AAAA;AAKf,UAAM,KAAK,IACR,qEAAqE,KAAK;AAG7E,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,OAAO,MAKI;AACf,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,oDAAoD;AAAA,IACtE;AAGA,UAAM,KAAK,IAAI,4CACb,KAAK,KAAK,KAAK;AAAA;AAAA;AAMjB,UAAM,KAAK,IACR,4DAA4D,KAAK;AAGpE,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,IAAI,MAUO;AACf,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,iDAAiD;AAAA,IACnE;AAGA,UAAM,KAAK,IAAI,gDACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,0BAA6B,KAAK,GAAG,KAAK;AAAA;AAAA;AAKhE,UAAM,KAAK,IACR,yDAAyD,KAAK;AAGjE,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,cAAc,MAUQ;AAC1B,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IAAI,iDACb,KAAK,GAAG,KAAK,gBACL,KAAK,GAAG,KAAK;AAAA;AAAA;AAKvB,UAAM,KAAK,IACR,mEAAmE,KAAK;AAG3E,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,eAAe,MAAwB;AAC3C,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IAAI;AAAA;AAAA;AAKf,UAAM,KAAK,IACR,oEAAoE,KAAK;AAG5E,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,YAAY,MAUD;AACf,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,yDAAyD;AAAA,IAC3E;AAGA,UAAM,KAAK,IAAI,wDACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,0BAA6B,KAAK,GAAG,KAAK;AAAA;AAAA;AAKhE,UAAM,KAAK,IACR,iEAAiE,KAAK;AAGzE,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,WAAW,MAKA;AACf,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,wDAAwD;AAAA,IAC1E;AAGA,UAAM,KAAK,IAAI,sDACb,KAAK,WAAW,KAAK;AAAA;AAAA;AAMvB,UAAM,KAAK,IACR,gEAAgE,KAAK;AAGxE,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,UAAU,MAKY;AAC1B,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,uDAAuD;AAAA,IACzE;AAGA,UAAM,KAAK,IAAI,6CACb,KAAK,GAAG,KAAK;AAAA;AAAA;AAMf,UAAM,KAAK,IACR,+DAA+D,KAAK;AAGvE,WAAO,KACJ;AAAA,EACL;AACF;","names":[]}