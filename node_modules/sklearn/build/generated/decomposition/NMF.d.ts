import { PythonBridge, NDArray, ArrayLike, SparseMatrix } from '@/sklearn/types';
/**
  Non-Negative Matrix Factorization (NMF).

  Find two non-negative matrices, i.e. matrices with all non-negative elements, (W, H) whose product approximates the non-negative matrix X. This factorization can be used for example for dimensionality reduction, source separation or topic extraction.

  The objective function is:

  [Python Reference](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html)
 */
export declare class NMF {
    id: string;
    opts: any;
    _py: PythonBridge;
    _isInitialized: boolean;
    _isDisposed: boolean;
    constructor(opts?: {
        /**
          Number of components, if n\_components is not set all features are kept.
         */
        n_components?: number;
        /**
          Method used to initialize the procedure. Valid options:
         */
        init?: 'random' | 'nndsvd' | 'nndsvda' | 'nndsvdar' | 'custom';
        /**
          Numerical solver to use:
    
          @defaultValue `'cd'`
         */
        solver?: 'cd' | 'mu';
        /**
          Beta divergence to be minimized, measuring the distance between X and the dot product WH. Note that values different from ‘frobenius’ (or 2) and ‘kullback-leibler’ (or 1) lead to significantly slower fits. Note that for beta\_loss <= 0 (or ‘itakura-saito’), the input matrix X cannot contain zeros. Used only in ‘mu’ solver.
    
          @defaultValue `'frobenius'`
         */
        beta_loss?: number | 'frobenius' | 'kullback-leibler' | 'itakura-saito';
        /**
          Tolerance of the stopping condition.
    
          @defaultValue `0.0001`
         */
        tol?: number;
        /**
          Maximum number of iterations before timing out.
    
          @defaultValue `200`
         */
        max_iter?: number;
        /**
          Used for initialisation (when `init` == ‘nndsvdar’ or ‘random’), and in Coordinate Descent. Pass an int for reproducible results across multiple function calls. See [Glossary](../../glossary.html#term-random_state).
         */
        random_state?: number;
        /**
          Constant that multiplies the regularization terms of `W`. Set it to zero (default) to have no regularization on `W`.
    
          @defaultValue `0`
         */
        alpha_W?: number;
        /**
          Constant that multiplies the regularization terms of `H`. Set it to zero to have no regularization on `H`. If “same” (default), it takes the same value as `alpha\_W`.
    
          @defaultValue `'same'`
         */
        alpha_H?: number | 'same';
        /**
          The regularization mixing parameter, with 0 <= l1\_ratio <= 1. For l1\_ratio = 0 the penalty is an elementwise L2 penalty (aka Frobenius Norm). For l1\_ratio = 1 it is an elementwise L1 penalty. For 0 < l1\_ratio < 1, the penalty is a combination of L1 and L2.
    
          @defaultValue `0`
         */
        l1_ratio?: number;
        /**
          Whether to be verbose.
    
          @defaultValue `0`
         */
        verbose?: number;
        /**
          If true, randomize the order of coordinates in the CD solver.
    
          @defaultValue `false`
         */
        shuffle?: boolean;
    });
    get py(): PythonBridge;
    set py(pythonBridge: PythonBridge);
    /**
      Initializes the underlying Python resources.
  
      This instance is not usable until the `Promise` returned by `init()` resolves.
     */
    init(py: PythonBridge): Promise<void>;
    /**
      Disposes of the underlying Python resources.
  
      Once `dispose()` is called, the instance is no longer usable.
     */
    dispose(): Promise<void>;
    /**
      Learn a NMF model for the data X.
     */
    fit(opts: {
        /**
          Training vector, where `n\_samples` is the number of samples and `n\_features` is the number of features.
         */
        X?: ArrayLike | SparseMatrix[];
        /**
          Not used, present for API consistency by convention.
         */
        y?: any;
        /**
          Parameters (keyword arguments) and values passed to the fit\_transform instance.
         */
        params?: any;
    }): Promise<any>;
    /**
      Learn a NMF model for the data X and returns the transformed data.
  
      This is more efficient than calling fit followed by transform.
     */
    fit_transform(opts: {
        /**
          Training vector, where `n\_samples` is the number of samples and `n\_features` is the number of features.
         */
        X?: ArrayLike | SparseMatrix[];
        /**
          Not used, present for API consistency by convention.
         */
        y?: any;
        /**
          If init=’custom’, it is used as initial guess for the solution.
         */
        W?: ArrayLike[];
        /**
          If init=’custom’, it is used as initial guess for the solution.
         */
        H?: ArrayLike[];
    }): Promise<NDArray[]>;
    /**
      Get output feature names for transformation.
  
      The feature names out will prefixed by the lowercased class name. For example, if the transformer outputs 3 features, then the feature names out are: `\["class\_name0", "class\_name1", "class\_name2"\]`.
     */
    get_feature_names_out(opts: {
        /**
          Only used to validate feature names with the names seen in [`fit`](#sklearn.decomposition.NMF.fit "sklearn.decomposition.NMF.fit").
         */
        input_features?: any;
    }): Promise<any>;
    /**
      Transform data back to its original space.
     */
    inverse_transform(opts: {
        /**
          Transformed data matrix.
         */
        W?: NDArray | SparseMatrix[];
    }): Promise<NDArray | SparseMatrix[]>;
    /**
      Set output container.
  
      See [Introducing the set\_output API](../../auto_examples/miscellaneous/plot_set_output.html#sphx-glr-auto-examples-miscellaneous-plot-set-output-py) for an example on how to use the API.
     */
    set_output(opts: {
        /**
          Configure output of `transform` and `fit\_transform`.
         */
        transform?: 'default' | 'pandas';
    }): Promise<any>;
    /**
      Transform the data X according to the fitted NMF model.
     */
    transform(opts: {
        /**
          Training vector, where `n\_samples` is the number of samples and `n\_features` is the number of features.
         */
        X?: ArrayLike | SparseMatrix[];
    }): Promise<NDArray[]>;
    /**
      Factorization matrix, sometimes called ‘dictionary’.
     */
    get components_(): Promise<NDArray[]>;
    /**
      The number of components. It is same as the `n\_components` parameter if it was given. Otherwise, it will be same as the number of features.
     */
    get n_components_(): Promise<number>;
    /**
      Frobenius norm of the matrix difference, or beta-divergence, between the training data `X` and the reconstructed data `WH` from the fitted model.
     */
    get reconstruction_err_(): Promise<number>;
    /**
      Actual number of iterations.
     */
    get n_iter_(): Promise<number>;
    /**
      Number of features seen during [fit](../../glossary.html#term-fit).
     */
    get n_features_in_(): Promise<number>;
    /**
      Names of features seen during [fit](../../glossary.html#term-fit). Defined only when `X` has feature names that are all strings.
     */
    get feature_names_in_(): Promise<NDArray>;
}
//# sourceMappingURL=NMF.d.ts.map