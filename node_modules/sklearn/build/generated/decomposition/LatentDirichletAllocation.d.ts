import { PythonBridge, NDArray, ArrayLike, SparseMatrix } from '@/sklearn/types';
/**
  Latent Dirichlet Allocation with online variational Bayes algorithm.

  The implementation is based on [\[1\]](#re25e5648fc37-1) and [\[2\]](#re25e5648fc37-2).

  [Python Reference](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html)
 */
export declare class LatentDirichletAllocation {
    id: string;
    opts: any;
    _py: PythonBridge;
    _isInitialized: boolean;
    _isDisposed: boolean;
    constructor(opts?: {
        /**
          Number of topics.
    
          @defaultValue `10`
         */
        n_components?: number;
        /**
          Prior of document topic distribution `theta`. If the value is `undefined`, defaults to `1 / n\_components`. In [\[1\]](#re25e5648fc37-1), this is called `alpha`.
         */
        doc_topic_prior?: number;
        /**
          Prior of topic word distribution `beta`. If the value is `undefined`, defaults to `1 / n\_components`. In [\[1\]](#re25e5648fc37-1), this is called `eta`.
         */
        topic_word_prior?: number;
        /**
          Method used to update `\_component`. Only used in [`fit`](#sklearn.decomposition.LatentDirichletAllocation.fit "sklearn.decomposition.LatentDirichletAllocation.fit") method. In general, if the data size is large, the online update will be much faster than the batch update.
    
          Valid options:
    
          @defaultValue `'batch'`
         */
        learning_method?: 'batch' | 'online';
        /**
          It is a parameter that control learning rate in the online learning method. The value should be set between (0.5, 1.0\] to guarantee asymptotic convergence. When the value is 0.0 and batch\_size is `n\_samples`, the update method is same as batch learning. In the literature, this is called kappa.
    
          @defaultValue `0.7`
         */
        learning_decay?: number;
        /**
          A (positive) parameter that downweights early iterations in online learning. It should be greater than 1.0. In the literature, this is called tau\_0.
    
          @defaultValue `10`
         */
        learning_offset?: number;
        /**
          The maximum number of passes over the training data (aka epochs). It only impacts the behavior in the [`fit`](#sklearn.decomposition.LatentDirichletAllocation.fit "sklearn.decomposition.LatentDirichletAllocation.fit") method, and not the [`partial\_fit`](#sklearn.decomposition.LatentDirichletAllocation.partial_fit "sklearn.decomposition.LatentDirichletAllocation.partial_fit") method.
    
          @defaultValue `10`
         */
        max_iter?: number;
        /**
          Number of documents to use in each EM iteration. Only used in online learning.
    
          @defaultValue `128`
         */
        batch_size?: number;
        /**
          How often to evaluate perplexity. Only used in `fit` method. set it to 0 or negative number to not evaluate perplexity in training at all. Evaluating perplexity can help you check convergence in training process, but it will also increase total training time. Evaluating perplexity in every iteration might increase training time up to two-fold.
    
          @defaultValue `-1`
         */
        evaluate_every?: number;
        /**
          Total number of documents. Only used in the [`partial\_fit`](#sklearn.decomposition.LatentDirichletAllocation.partial_fit "sklearn.decomposition.LatentDirichletAllocation.partial_fit") method.
    
          @defaultValue `1000000`
         */
        total_samples?: number;
        /**
          Perplexity tolerance in batch learning. Only used when `evaluate\_every` is greater than 0.
    
          @defaultValue `0.1`
         */
        perp_tol?: number;
        /**
          Stopping tolerance for updating document topic distribution in E-step.
    
          @defaultValue `0.001`
         */
        mean_change_tol?: number;
        /**
          Max number of iterations for updating document topic distribution in the E-step.
    
          @defaultValue `100`
         */
        max_doc_update_iter?: number;
        /**
          The number of jobs to use in the E-step. `undefined` means 1 unless in a [`joblib.parallel\_backend`](https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend "(in joblib v1.3.0.dev0)") context. `\-1` means using all processors. See [Glossary](../../glossary.html#term-n_jobs) for more details.
         */
        n_jobs?: number;
        /**
          Verbosity level.
    
          @defaultValue `0`
         */
        verbose?: number;
        /**
          Pass an int for reproducible results across multiple function calls. See [Glossary](../../glossary.html#term-random_state).
         */
        random_state?: number;
    });
    get py(): PythonBridge;
    set py(pythonBridge: PythonBridge);
    /**
      Initializes the underlying Python resources.
  
      This instance is not usable until the `Promise` returned by `init()` resolves.
     */
    init(py: PythonBridge): Promise<void>;
    /**
      Disposes of the underlying Python resources.
  
      Once `dispose()` is called, the instance is no longer usable.
     */
    dispose(): Promise<void>;
    /**
      Learn model for the data X with variational Bayes method.
  
      When `learning\_method` is ‘online’, use mini-batch update. Otherwise, use batch update.
     */
    fit(opts: {
        /**
          Document word matrix.
         */
        X?: ArrayLike | SparseMatrix[];
        /**
          Not used, present here for API consistency by convention.
         */
        y?: any;
    }): Promise<any>;
    /**
      Fit to data, then transform it.
  
      Fits transformer to `X` and `y` with optional parameters `fit\_params` and returns a transformed version of `X`.
     */
    fit_transform(opts: {
        /**
          Input samples.
         */
        X?: ArrayLike[];
        /**
          Target values (`undefined` for unsupervised transformations).
         */
        y?: ArrayLike;
        /**
          Additional fit parameters.
         */
        fit_params?: any;
    }): Promise<any[]>;
    /**
      Get output feature names for transformation.
  
      The feature names out will prefixed by the lowercased class name. For example, if the transformer outputs 3 features, then the feature names out are: `\["class\_name0", "class\_name1", "class\_name2"\]`.
     */
    get_feature_names_out(opts: {
        /**
          Only used to validate feature names with the names seen in [`fit`](#sklearn.decomposition.LatentDirichletAllocation.fit "sklearn.decomposition.LatentDirichletAllocation.fit").
         */
        input_features?: any;
    }): Promise<any>;
    /**
      Online VB with Mini-Batch update.
     */
    partial_fit(opts: {
        /**
          Document word matrix.
         */
        X?: ArrayLike | SparseMatrix[];
        /**
          Not used, present here for API consistency by convention.
         */
        y?: any;
    }): Promise<any>;
    /**
      Calculate approximate perplexity for data X.
  
      Perplexity is defined as exp(-1. \* log-likelihood per word)
     */
    perplexity(opts: {
        /**
          Document word matrix.
         */
        X?: ArrayLike | SparseMatrix[];
        /**
          Do sub-sampling or not.
         */
        sub_sampling?: boolean;
    }): Promise<number>;
    /**
      Calculate approximate log-likelihood as score.
     */
    score(opts: {
        /**
          Document word matrix.
         */
        X?: ArrayLike | SparseMatrix[];
        /**
          Not used, present here for API consistency by convention.
         */
        y?: any;
    }): Promise<number>;
    /**
      Set output container.
  
      See [Introducing the set\_output API](../../auto_examples/miscellaneous/plot_set_output.html#sphx-glr-auto-examples-miscellaneous-plot-set-output-py) for an example on how to use the API.
     */
    set_output(opts: {
        /**
          Configure output of `transform` and `fit\_transform`.
         */
        transform?: 'default' | 'pandas';
    }): Promise<any>;
    /**
      Transform data X according to the fitted model.
     */
    transform(opts: {
        /**
          Document word matrix.
         */
        X?: ArrayLike | SparseMatrix[];
    }): Promise<NDArray[]>;
    /**
      Variational parameters for topic word distribution. Since the complete conditional for topic word distribution is a Dirichlet, `components\_\[i, j\]` can be viewed as pseudocount that represents the number of times word `j` was assigned to topic `i`. It can also be viewed as distribution over the words for each topic after normalization: `model.components\_ / model.components\_.sum(axis=1)\[:, np.newaxis\]`.
     */
    get components_(): Promise<NDArray[]>;
    /**
      Exponential value of expectation of log topic word distribution. In the literature, this is `exp(E\[log(beta)\])`.
     */
    get exp_dirichlet_component_(): Promise<NDArray[]>;
    /**
      Number of iterations of the EM step.
     */
    get n_batch_iter_(): Promise<number>;
    /**
      Number of features seen during [fit](../../glossary.html#term-fit).
     */
    get n_features_in_(): Promise<number>;
    /**
      Names of features seen during [fit](../../glossary.html#term-fit). Defined only when `X` has feature names that are all strings.
     */
    get feature_names_in_(): Promise<NDArray>;
    /**
      Number of passes over the dataset.
     */
    get n_iter_(): Promise<number>;
    /**
      Final perplexity score on training set.
     */
    get bound_(): Promise<number>;
    /**
      Prior of document topic distribution `theta`. If the value is `undefined`, it is `1 / n\_components`.
     */
    get doc_topic_prior_(): Promise<number>;
    /**
      RandomState instance that is generated either from a seed, the random number generator or by `np.random`.
     */
    get random_state_(): Promise<any>;
    /**
      Prior of topic word distribution `beta`. If the value is `undefined`, it is `1 / n\_components`.
     */
    get topic_word_prior_(): Promise<number>;
}
//# sourceMappingURL=LatentDirichletAllocation.d.ts.map