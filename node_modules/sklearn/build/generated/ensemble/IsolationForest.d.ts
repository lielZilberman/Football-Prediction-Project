import { PythonBridge, NDArray, ArrayLike, SparseMatrix } from '@/sklearn/types';
/**
  Isolation Forest Algorithm.

  Return the anomaly score of each sample using the IsolationForest algorithm

  The IsolationForest ‘isolates’ observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature.

  Since recursive partitioning can be represented by a tree structure, the number of splittings required to isolate a sample is equivalent to the path length from the root node to the terminating node.

  This path length, averaged over a forest of such random trees, is a measure of normality and our decision function.

  Random partitioning produces noticeably shorter paths for anomalies. Hence, when a forest of random trees collectively produce shorter path lengths for particular samples, they are highly likely to be anomalies.

  Read more in the [User Guide](../outlier_detection.html#isolation-forest).

  [Python Reference](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html)
 */
export declare class IsolationForest {
    id: string;
    opts: any;
    _py: PythonBridge;
    _isInitialized: boolean;
    _isDisposed: boolean;
    constructor(opts?: {
        /**
          The number of base estimators in the ensemble.
    
          @defaultValue `100`
         */
        n_estimators?: number;
        /**
          If int, then draw `max\_samples` samples.
    
          @defaultValue `'auto'`
         */
        max_samples?: 'auto' | number | number;
        /**
          The amount of contamination of the data set, i.e. the proportion of outliers in the data set. Used when fitting to define the threshold on the scores of the samples.
    
          @defaultValue `'auto'`
         */
        contamination?: 'auto' | number;
        /**
          The number of features to draw from X to train each base estimator.
    
          @defaultValue `1`
         */
        max_features?: number;
        /**
          If `true`, individual trees are fit on random subsets of the training data sampled with replacement. If `false`, sampling without replacement is performed.
    
          @defaultValue `false`
         */
        bootstrap?: boolean;
        /**
          The number of jobs to run in parallel for both [`fit`](#sklearn.ensemble.IsolationForest.fit "sklearn.ensemble.IsolationForest.fit") and [`predict`](#sklearn.ensemble.IsolationForest.predict "sklearn.ensemble.IsolationForest.predict"). `undefined` means 1 unless in a [`joblib.parallel\_backend`](https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend "(in joblib v1.3.0.dev0)") context. `\-1` means using all processors. See [Glossary](../../glossary.html#term-n_jobs) for more details.
         */
        n_jobs?: number;
        /**
          Controls the pseudo-randomness of the selection of the feature and split values for each branching step and each tree in the forest.
    
          Pass an int for reproducible results across multiple function calls. See [Glossary](../../glossary.html#term-random_state).
         */
        random_state?: number;
        /**
          Controls the verbosity of the tree building process.
    
          @defaultValue `0`
         */
        verbose?: number;
        /**
          When set to `true`, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new forest. See [the Glossary](../../glossary.html#term-warm_start).
    
          @defaultValue `false`
         */
        warm_start?: boolean;
    });
    get py(): PythonBridge;
    set py(pythonBridge: PythonBridge);
    /**
      Initializes the underlying Python resources.
  
      This instance is not usable until the `Promise` returned by `init()` resolves.
     */
    init(py: PythonBridge): Promise<void>;
    /**
      Disposes of the underlying Python resources.
  
      Once `dispose()` is called, the instance is no longer usable.
     */
    dispose(): Promise<void>;
    /**
      Average anomaly score of X of the base classifiers.
  
      The anomaly score of an input sample is computed as the mean anomaly score of the trees in the forest.
  
      The measure of normality of an observation given a tree is the depth of the leaf containing this observation, which is equivalent to the number of splittings required to isolate this point. In case of several observations n\_left in the leaf, the average path length of a n\_left samples isolation tree is added.
     */
    decision_function(opts: {
        /**
          The input samples. Internally, it will be converted to `dtype=np.float32` and if a sparse matrix is provided to a sparse `csr\_matrix`.
         */
        X?: ArrayLike | SparseMatrix[];
    }): Promise<NDArray>;
    /**
      Fit estimator.
     */
    fit(opts: {
        /**
          The input samples. Use `dtype=np.float32` for maximum efficiency. Sparse matrices are also supported, use sparse `csc\_matrix` for maximum efficiency.
         */
        X?: ArrayLike | SparseMatrix[];
        /**
          Not used, present for API consistency by convention.
         */
        y?: any;
        /**
          Sample weights. If `undefined`, then samples are equally weighted.
         */
        sample_weight?: ArrayLike;
    }): Promise<any>;
    /**
      Perform fit on X and returns labels for X.
  
      Returns -1 for outliers and 1 for inliers.
     */
    fit_predict(opts: {
        /**
          The input samples.
         */
        X?: ArrayLike | SparseMatrix[];
        /**
          Not used, present for API consistency by convention.
         */
        y?: any;
    }): Promise<NDArray>;
    /**
      Predict if a particular sample is an outlier or not.
     */
    predict(opts: {
        /**
          The input samples. Internally, it will be converted to `dtype=np.float32` and if a sparse matrix is provided to a sparse `csr\_matrix`.
         */
        X?: ArrayLike | SparseMatrix[];
    }): Promise<NDArray>;
    /**
      Opposite of the anomaly score defined in the original paper.
  
      The anomaly score of an input sample is computed as the mean anomaly score of the trees in the forest.
  
      The measure of normality of an observation given a tree is the depth of the leaf containing this observation, which is equivalent to the number of splittings required to isolate this point. In case of several observations n\_left in the leaf, the average path length of a n\_left samples isolation tree is added.
     */
    score_samples(opts: {
        /**
          The input samples.
         */
        X?: ArrayLike | SparseMatrix[];
    }): Promise<NDArray>;
    /**
      The child estimator template used to create the collection of fitted sub-estimators.
     */
    get estimator_(): Promise<any>;
    /**
      The collection of fitted sub-estimators.
     */
    get estimators_(): Promise<any>;
    /**
      The subset of drawn features for each base estimator.
     */
    get estimators_features_(): Promise<any>;
    /**
      The actual number of samples.
     */
    get max_samples_(): Promise<number>;
    /**
      Offset used to define the decision function from the raw scores. We have the relation: `decision\_function \= score\_samples \- offset\_`. `offset\_` is defined as follows. When the contamination parameter is set to “auto”, the offset is equal to -0.5 as the scores of inliers are close to 0 and the scores of outliers are close to -1. When a contamination parameter different than “auto” is provided, the offset is defined in such a way we obtain the expected number of outliers (samples with decision function < 0) in training.
     */
    get offset_(): Promise<number>;
    /**
      Number of features seen during [fit](../../glossary.html#term-fit).
     */
    get n_features_in_(): Promise<number>;
    /**
      Names of features seen during [fit](../../glossary.html#term-fit). Defined only when `X` has feature names that are all strings.
     */
    get feature_names_in_(): Promise<NDArray>;
}
//# sourceMappingURL=IsolationForest.d.ts.map