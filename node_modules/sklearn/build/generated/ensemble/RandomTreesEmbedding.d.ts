import { PythonBridge, NDArray, ArrayLike, SparseMatrix } from '@/sklearn/types';
/**
  An ensemble of totally random trees.

  An unsupervised transformation of a dataset to a high-dimensional sparse representation. A datapoint is coded according to which leaf of each tree it is sorted into. Using a one-hot encoding of the leaves, this leads to a binary coding with as many ones as there are trees in the forest.

  The dimensionality of the resulting representation is `n\_out <= n\_estimators \* max\_leaf\_nodes`. If `max\_leaf\_nodes \== None`, the number of leaf nodes is at most `n\_estimators \* 2 \*\* max\_depth`.

  Read more in the [User Guide](../ensemble.html#random-trees-embedding).

  [Python Reference](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomTreesEmbedding.html)
 */
export declare class RandomTreesEmbedding {
    id: string;
    opts: any;
    _py: PythonBridge;
    _isInitialized: boolean;
    _isDisposed: boolean;
    constructor(opts?: {
        /**
          Number of trees in the forest.
    
          @defaultValue `100`
         */
        n_estimators?: number;
        /**
          The maximum depth of each tree. If `undefined`, then nodes are expanded until all leaves are pure or until all leaves contain less than min\_samples\_split samples.
    
          @defaultValue `5`
         */
        max_depth?: number;
        /**
          The minimum number of samples required to split an internal node:
    
          @defaultValue `2`
         */
        min_samples_split?: number;
        /**
          The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least `min\_samples\_leaf` training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.
    
          @defaultValue `1`
         */
        min_samples_leaf?: number;
        /**
          The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample\_weight is not provided.
    
          @defaultValue `0`
         */
        min_weight_fraction_leaf?: number;
        /**
          Grow trees with `max\_leaf\_nodes` in best-first fashion. Best nodes are defined as relative reduction in impurity. If `undefined` then unlimited number of leaf nodes.
         */
        max_leaf_nodes?: number;
        /**
          A node will be split if this split induces a decrease of the impurity greater than or equal to this value.
    
          The weighted impurity decrease equation is the following:
    
          @defaultValue `0`
         */
        min_impurity_decrease?: number;
        /**
          Whether or not to return a sparse CSR matrix, as default behavior, or to return a dense array compatible with dense pipeline operators.
    
          @defaultValue `true`
         */
        sparse_output?: boolean;
        /**
          The number of jobs to run in parallel. [`fit`](#sklearn.ensemble.RandomTreesEmbedding.fit "sklearn.ensemble.RandomTreesEmbedding.fit"), [`transform`](#sklearn.ensemble.RandomTreesEmbedding.transform "sklearn.ensemble.RandomTreesEmbedding.transform"), [`decision\_path`](#sklearn.ensemble.RandomTreesEmbedding.decision_path "sklearn.ensemble.RandomTreesEmbedding.decision_path") and [`apply`](#sklearn.ensemble.RandomTreesEmbedding.apply "sklearn.ensemble.RandomTreesEmbedding.apply") are all parallelized over the trees. `undefined` means 1 unless in a [`joblib.parallel\_backend`](https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend "(in joblib v1.3.0.dev0)") context. `\-1` means using all processors. See [Glossary](../../glossary.html#term-n_jobs) for more details.
         */
        n_jobs?: number;
        /**
          Controls the generation of the random `y` used to fit the trees and the draw of the splits for each feature at the treesâ€™ nodes. See [Glossary](../../glossary.html#term-random_state) for details.
         */
        random_state?: number;
        /**
          Controls the verbosity when fitting and predicting.
    
          @defaultValue `0`
         */
        verbose?: number;
        /**
          When set to `true`, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new forest. See [Glossary](../../glossary.html#term-warm_start) and [Fitting additional weak-learners](../ensemble.html#gradient-boosting-warm-start) for details.
    
          @defaultValue `false`
         */
        warm_start?: boolean;
    });
    get py(): PythonBridge;
    set py(pythonBridge: PythonBridge);
    /**
      Initializes the underlying Python resources.
  
      This instance is not usable until the `Promise` returned by `init()` resolves.
     */
    init(py: PythonBridge): Promise<void>;
    /**
      Disposes of the underlying Python resources.
  
      Once `dispose()` is called, the instance is no longer usable.
     */
    dispose(): Promise<void>;
    /**
      Apply trees in the forest to X, return leaf indices.
     */
    apply(opts: {
        /**
          The input samples. Internally, its dtype will be converted to `dtype=np.float32`. If a sparse matrix is provided, it will be converted into a sparse `csr\_matrix`.
         */
        X?: ArrayLike | SparseMatrix[];
    }): Promise<NDArray[]>;
    /**
      Return the decision path in the forest.
     */
    decision_path(opts: {
        /**
          The input samples. Internally, its dtype will be converted to `dtype=np.float32`. If a sparse matrix is provided, it will be converted into a sparse `csr\_matrix`.
         */
        X?: ArrayLike | SparseMatrix[];
    }): Promise<SparseMatrix[]>;
    /**
      Fit estimator.
     */
    fit(opts: {
        /**
          The input samples. Use `dtype=np.float32` for maximum efficiency. Sparse matrices are also supported, use sparse `csc\_matrix` for maximum efficiency.
         */
        X?: ArrayLike | SparseMatrix[];
        /**
          Not used, present for API consistency by convention.
         */
        y?: any;
        /**
          Sample weights. If `undefined`, then samples are equally weighted. Splits that would create child nodes with net zero or negative weight are ignored while searching for a split in each node. In the case of classification, splits are also ignored if they would result in any single class carrying a negative weight in either child node.
         */
        sample_weight?: ArrayLike;
    }): Promise<any>;
    /**
      Fit estimator and transform dataset.
     */
    fit_transform(opts: {
        /**
          Input data used to build forests. Use `dtype=np.float32` for maximum efficiency.
         */
        X?: ArrayLike | SparseMatrix[];
        /**
          Not used, present for API consistency by convention.
         */
        y?: any;
        /**
          Sample weights. If `undefined`, then samples are equally weighted. Splits that would create child nodes with net zero or negative weight are ignored while searching for a split in each node. In the case of classification, splits are also ignored if they would result in any single class carrying a negative weight in either child node.
         */
        sample_weight?: ArrayLike;
    }): Promise<SparseMatrix[]>;
    /**
      Get output feature names for transformation.
     */
    get_feature_names_out(opts: {
        /**
          Only used to validate feature names with the names seen in [`fit`](#sklearn.ensemble.RandomTreesEmbedding.fit "sklearn.ensemble.RandomTreesEmbedding.fit").
         */
        input_features?: any;
    }): Promise<any>;
    /**
      Set output container.
  
      See [Introducing the set\_output API](../../auto_examples/miscellaneous/plot_set_output.html#sphx-glr-auto-examples-miscellaneous-plot-set-output-py) for an example on how to use the API.
     */
    set_output(opts: {
        /**
          Configure output of `transform` and `fit\_transform`.
         */
        transform?: 'default' | 'pandas';
    }): Promise<any>;
    /**
      Transform dataset.
     */
    transform(opts: {
        /**
          Input data to be transformed. Use `dtype=np.float32` for maximum efficiency. Sparse matrices are also supported, use sparse `csr\_matrix` for maximum efficiency.
         */
        X?: ArrayLike | SparseMatrix[];
    }): Promise<SparseMatrix[]>;
    /**
      The child estimator template used to create the collection of fitted sub-estimators.
     */
    get estimator_(): Promise<any>;
    /**
      The collection of fitted sub-estimators.
     */
    get estimators_(): Promise<any>;
    /**
      Number of features seen during [fit](../../glossary.html#term-fit).
     */
    get n_features_in_(): Promise<number>;
    /**
      Names of features seen during [fit](../../glossary.html#term-fit). Defined only when `X` has feature names that are all strings.
     */
    get feature_names_in_(): Promise<NDArray>;
    /**
      The number of outputs when `fit` is performed.
     */
    get n_outputs_(): Promise<number>;
    /**
      One-hot encoder used to create the sparse embedding.
     */
    get one_hot_encoder_(): Promise<any>;
}
//# sourceMappingURL=RandomTreesEmbedding.d.ts.map