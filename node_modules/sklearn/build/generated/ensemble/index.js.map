{"version":3,"sources":["../../../src/generated/ensemble/AdaBoostClassifier.ts","../../../src/generated/ensemble/AdaBoostRegressor.ts","../../../src/generated/ensemble/BaggingClassifier.ts","../../../src/generated/ensemble/BaggingRegressor.ts","../../../src/generated/ensemble/ExtraTreesClassifier.ts","../../../src/generated/ensemble/ExtraTreesRegressor.ts","../../../src/generated/ensemble/GradientBoostingClassifier.ts","../../../src/generated/ensemble/GradientBoostingRegressor.ts","../../../src/generated/ensemble/HistGradientBoostingClassifier.ts","../../../src/generated/ensemble/HistGradientBoostingRegressor.ts","../../../src/generated/ensemble/IsolationForest.ts","../../../src/generated/ensemble/RandomForestClassifier.ts","../../../src/generated/ensemble/RandomForestRegressor.ts","../../../src/generated/ensemble/RandomTreesEmbedding.ts","../../../src/generated/ensemble/StackingClassifier.ts","../../../src/generated/ensemble/StackingRegressor.ts","../../../src/generated/ensemble/VotingClassifier.ts","../../../src/generated/ensemble/VotingRegressor.ts"],"sourcesContent":["/* eslint-disable */\n/* NOTE: This file is auto-generated. Do not edit it directly. */\n\nimport crypto from 'node:crypto'\n\nimport { PythonBridge, NDArray, ArrayLike, SparseMatrix } from '@/sklearn/types'\n\n/**\n  An AdaBoost classifier.\n\n  An AdaBoost \\[1\\] classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases.\n\n  This class implements the algorithm known as AdaBoost-SAMME \\[2\\].\n\n  Read more in the [User Guide](../ensemble.html#adaboost).\n\n  [Python Reference](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html)\n */\nexport class AdaBoostClassifier {\n  id: string\n  opts: any\n\n  _py: PythonBridge\n  _isInitialized: boolean = false\n  _isDisposed: boolean = false\n\n  constructor(opts?: {\n    /**\n      The base estimator from which the boosted ensemble is built. Support for sample weighting is required, as well as proper `classes\\_` and `n\\_classes\\_` attributes. If `undefined`, then the base estimator is [`DecisionTreeClassifier`](sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier \"sklearn.tree.DecisionTreeClassifier\") initialized with `max\\_depth=1`.\n     */\n    estimator?: any\n\n    /**\n      The maximum number of estimators at which boosting is terminated. In case of perfect fit, the learning procedure is stopped early. Values must be in the range `\\[1, inf)`.\n\n      @defaultValue `50`\n     */\n    n_estimators?: number\n\n    /**\n      Weight applied to each classifier at each boosting iteration. A higher learning rate increases the contribution of each classifier. There is a trade-off between the `learning\\_rate` and `n\\_estimators` parameters. Values must be in the range `(0.0, inf)`.\n\n      @defaultValue `1`\n     */\n    learning_rate?: number\n\n    /**\n      If ‘SAMME.R’ then use the SAMME.R real boosting algorithm. `estimator` must support calculation of class probabilities. If ‘SAMME’ then use the SAMME discrete boosting algorithm. The SAMME.R algorithm typically converges faster than SAMME, achieving a lower test error with fewer boosting iterations.\n\n      @defaultValue `'SAMME.R'`\n     */\n    algorithm?: 'SAMME' | 'SAMME.R'\n\n    /**\n      Controls the random seed given at each `estimator` at each boosting iteration. Thus, it is only used when `estimator` exposes a `random\\_state`. Pass an int for reproducible output across multiple function calls. See [Glossary](../../glossary.html#term-random_state).\n     */\n    random_state?: number\n\n    /**\n      The base estimator from which the boosted ensemble is built. Support for sample weighting is required, as well as proper `classes\\_` and `n\\_classes\\_` attributes. If `undefined`, then the base estimator is [`DecisionTreeClassifier`](sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier \"sklearn.tree.DecisionTreeClassifier\") initialized with `max\\_depth=1`.\n     */\n    base_estimator?: any\n  }) {\n    this.id = `AdaBoostClassifier${crypto.randomUUID().split('-')[0]}`\n    this.opts = opts || {}\n  }\n\n  get py(): PythonBridge {\n    return this._py\n  }\n\n  set py(pythonBridge: PythonBridge) {\n    this._py = pythonBridge\n  }\n\n  /**\n    Initializes the underlying Python resources.\n\n    This instance is not usable until the `Promise` returned by `init()` resolves.\n   */\n  async init(py: PythonBridge): Promise<void> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This AdaBoostClassifier instance has already been disposed'\n      )\n    }\n\n    if (this._isInitialized) {\n      return\n    }\n\n    if (!py) {\n      throw new Error(\n        'AdaBoostClassifier.init requires a PythonBridge instance'\n      )\n    }\n\n    this._py = py\n\n    await this._py.ex`\nimport numpy as np\nfrom sklearn.ensemble import AdaBoostClassifier\ntry: bridgeAdaBoostClassifier\nexcept NameError: bridgeAdaBoostClassifier = {}\n`\n\n    // set up constructor params\n    await this._py.ex`ctor_AdaBoostClassifier = {'estimator': ${\n      this.opts['estimator'] ?? undefined\n    }, 'n_estimators': ${\n      this.opts['n_estimators'] ?? undefined\n    }, 'learning_rate': ${\n      this.opts['learning_rate'] ?? undefined\n    }, 'algorithm': ${this.opts['algorithm'] ?? undefined}, 'random_state': ${\n      this.opts['random_state'] ?? undefined\n    }, 'base_estimator': ${this.opts['base_estimator'] ?? undefined}}\n\nctor_AdaBoostClassifier = {k: v for k, v in ctor_AdaBoostClassifier.items() if v is not None}`\n\n    await this._py\n      .ex`bridgeAdaBoostClassifier[${this.id}] = AdaBoostClassifier(**ctor_AdaBoostClassifier)`\n\n    this._isInitialized = true\n  }\n\n  /**\n    Disposes of the underlying Python resources.\n\n    Once `dispose()` is called, the instance is no longer usable.\n   */\n  async dispose() {\n    if (this._isDisposed) {\n      return\n    }\n\n    if (!this._isInitialized) {\n      return\n    }\n\n    await this._py.ex`del bridgeAdaBoostClassifier[${this.id}]`\n\n    this._isDisposed = true\n  }\n\n  /**\n    Compute the decision function of `X`.\n   */\n  async decision_function(opts: {\n    /**\n      The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n     */\n    X?: ArrayLike | SparseMatrix[]\n  }): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This AdaBoostClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'AdaBoostClassifier must call init() before decision_function()'\n      )\n    }\n\n    // set up method params\n    await this._py\n      .ex`pms_AdaBoostClassifier_decision_function = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None}\n\npms_AdaBoostClassifier_decision_function = {k: v for k, v in pms_AdaBoostClassifier_decision_function.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_AdaBoostClassifier_decision_function = bridgeAdaBoostClassifier[${this.id}].decision_function(**pms_AdaBoostClassifier_decision_function)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_AdaBoostClassifier_decision_function.tolist() if hasattr(res_AdaBoostClassifier_decision_function, 'tolist') else res_AdaBoostClassifier_decision_function`\n  }\n\n  /**\n    Build a boosted classifier/regressor from the training set (X, y).\n   */\n  async fit(opts: {\n    /**\n      The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n     */\n    X?: ArrayLike | SparseMatrix[]\n\n    /**\n      The target values.\n     */\n    y?: ArrayLike\n\n    /**\n      Sample weights. If `undefined`, the sample weights are initialized to 1 / n\\_samples.\n     */\n    sample_weight?: ArrayLike\n  }): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This AdaBoostClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('AdaBoostClassifier must call init() before fit()')\n    }\n\n    // set up method params\n    await this._py.ex`pms_AdaBoostClassifier_fit = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None, 'y': np.array(${\n      opts['y'] ?? undefined\n    }) if ${opts['y'] !== undefined} else None, 'sample_weight': np.array(${\n      opts['sample_weight'] ?? undefined\n    }) if ${opts['sample_weight'] !== undefined} else None}\n\npms_AdaBoostClassifier_fit = {k: v for k, v in pms_AdaBoostClassifier_fit.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_AdaBoostClassifier_fit = bridgeAdaBoostClassifier[${this.id}].fit(**pms_AdaBoostClassifier_fit)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_AdaBoostClassifier_fit.tolist() if hasattr(res_AdaBoostClassifier_fit, 'tolist') else res_AdaBoostClassifier_fit`\n  }\n\n  /**\n    Predict classes for X.\n\n    The predicted class of an input sample is computed as the weighted mean prediction of the classifiers in the ensemble.\n   */\n  async predict(opts: {\n    /**\n      The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n     */\n    X?: ArrayLike | SparseMatrix[]\n  }): Promise<NDArray> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This AdaBoostClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('AdaBoostClassifier must call init() before predict()')\n    }\n\n    // set up method params\n    await this._py.ex`pms_AdaBoostClassifier_predict = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None}\n\npms_AdaBoostClassifier_predict = {k: v for k, v in pms_AdaBoostClassifier_predict.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_AdaBoostClassifier_predict = bridgeAdaBoostClassifier[${this.id}].predict(**pms_AdaBoostClassifier_predict)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_AdaBoostClassifier_predict.tolist() if hasattr(res_AdaBoostClassifier_predict, 'tolist') else res_AdaBoostClassifier_predict`\n  }\n\n  /**\n    Predict class log-probabilities for X.\n\n    The predicted class log-probabilities of an input sample is computed as the weighted mean predicted class log-probabilities of the classifiers in the ensemble.\n   */\n  async predict_log_proba(opts: {\n    /**\n      The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n     */\n    X?: ArrayLike | SparseMatrix[]\n  }): Promise<NDArray[]> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This AdaBoostClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'AdaBoostClassifier must call init() before predict_log_proba()'\n      )\n    }\n\n    // set up method params\n    await this._py\n      .ex`pms_AdaBoostClassifier_predict_log_proba = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None}\n\npms_AdaBoostClassifier_predict_log_proba = {k: v for k, v in pms_AdaBoostClassifier_predict_log_proba.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_AdaBoostClassifier_predict_log_proba = bridgeAdaBoostClassifier[${this.id}].predict_log_proba(**pms_AdaBoostClassifier_predict_log_proba)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_AdaBoostClassifier_predict_log_proba.tolist() if hasattr(res_AdaBoostClassifier_predict_log_proba, 'tolist') else res_AdaBoostClassifier_predict_log_proba`\n  }\n\n  /**\n    Predict class probabilities for X.\n\n    The predicted class probabilities of an input sample is computed as the weighted mean predicted class probabilities of the classifiers in the ensemble.\n   */\n  async predict_proba(opts: {\n    /**\n      The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n     */\n    X?: ArrayLike | SparseMatrix[]\n  }): Promise<NDArray[]> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This AdaBoostClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'AdaBoostClassifier must call init() before predict_proba()'\n      )\n    }\n\n    // set up method params\n    await this._py.ex`pms_AdaBoostClassifier_predict_proba = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None}\n\npms_AdaBoostClassifier_predict_proba = {k: v for k, v in pms_AdaBoostClassifier_predict_proba.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_AdaBoostClassifier_predict_proba = bridgeAdaBoostClassifier[${this.id}].predict_proba(**pms_AdaBoostClassifier_predict_proba)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_AdaBoostClassifier_predict_proba.tolist() if hasattr(res_AdaBoostClassifier_predict_proba, 'tolist') else res_AdaBoostClassifier_predict_proba`\n  }\n\n  /**\n    Return the mean accuracy on the given test data and labels.\n\n    In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted.\n   */\n  async score(opts: {\n    /**\n      Test samples.\n     */\n    X?: ArrayLike[]\n\n    /**\n      True labels for `X`.\n     */\n    y?: ArrayLike\n\n    /**\n      Sample weights.\n     */\n    sample_weight?: ArrayLike\n  }): Promise<number> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This AdaBoostClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('AdaBoostClassifier must call init() before score()')\n    }\n\n    // set up method params\n    await this._py.ex`pms_AdaBoostClassifier_score = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None, 'y': np.array(${\n      opts['y'] ?? undefined\n    }) if ${opts['y'] !== undefined} else None, 'sample_weight': np.array(${\n      opts['sample_weight'] ?? undefined\n    }) if ${opts['sample_weight'] !== undefined} else None}\n\npms_AdaBoostClassifier_score = {k: v for k, v in pms_AdaBoostClassifier_score.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_AdaBoostClassifier_score = bridgeAdaBoostClassifier[${this.id}].score(**pms_AdaBoostClassifier_score)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_AdaBoostClassifier_score.tolist() if hasattr(res_AdaBoostClassifier_score, 'tolist') else res_AdaBoostClassifier_score`\n  }\n\n  /**\n    Compute decision function of `X` for each boosting iteration.\n\n    This method allows monitoring (i.e. determine error on testing set) after each boosting iteration.\n   */\n  async staged_decision_function(opts: {\n    /**\n      The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n     */\n    X?: ArrayLike | SparseMatrix[]\n  }): Promise<any[]> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This AdaBoostClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'AdaBoostClassifier must call init() before staged_decision_function()'\n      )\n    }\n\n    // set up method params\n    await this._py\n      .ex`pms_AdaBoostClassifier_staged_decision_function = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None}\n\npms_AdaBoostClassifier_staged_decision_function = {k: v for k, v in pms_AdaBoostClassifier_staged_decision_function.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_AdaBoostClassifier_staged_decision_function = bridgeAdaBoostClassifier[${this.id}].staged_decision_function(**pms_AdaBoostClassifier_staged_decision_function)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_AdaBoostClassifier_staged_decision_function.tolist() if hasattr(res_AdaBoostClassifier_staged_decision_function, 'tolist') else res_AdaBoostClassifier_staged_decision_function`\n  }\n\n  /**\n    Return staged predictions for X.\n\n    The predicted class of an input sample is computed as the weighted mean prediction of the classifiers in the ensemble.\n\n    This generator method yields the ensemble prediction after each iteration of boosting and therefore allows monitoring, such as to determine the prediction on a test set after each boost.\n   */\n  async staged_predict(opts: {\n    /**\n      The input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n     */\n    X?: ArrayLike[]\n  }): Promise<any[]> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This AdaBoostClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'AdaBoostClassifier must call init() before staged_predict()'\n      )\n    }\n\n    // set up method params\n    await this._py.ex`pms_AdaBoostClassifier_staged_predict = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None}\n\npms_AdaBoostClassifier_staged_predict = {k: v for k, v in pms_AdaBoostClassifier_staged_predict.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_AdaBoostClassifier_staged_predict = bridgeAdaBoostClassifier[${this.id}].staged_predict(**pms_AdaBoostClassifier_staged_predict)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_AdaBoostClassifier_staged_predict.tolist() if hasattr(res_AdaBoostClassifier_staged_predict, 'tolist') else res_AdaBoostClassifier_staged_predict`\n  }\n\n  /**\n    Predict class probabilities for X.\n\n    The predicted class probabilities of an input sample is computed as the weighted mean predicted class probabilities of the classifiers in the ensemble.\n\n    This generator method yields the ensemble predicted class probabilities after each iteration of boosting and therefore allows monitoring, such as to determine the predicted class probabilities on a test set after each boost.\n   */\n  async staged_predict_proba(opts: {\n    /**\n      The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n     */\n    X?: ArrayLike | SparseMatrix[]\n  }): Promise<any[]> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This AdaBoostClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'AdaBoostClassifier must call init() before staged_predict_proba()'\n      )\n    }\n\n    // set up method params\n    await this._py\n      .ex`pms_AdaBoostClassifier_staged_predict_proba = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None}\n\npms_AdaBoostClassifier_staged_predict_proba = {k: v for k, v in pms_AdaBoostClassifier_staged_predict_proba.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_AdaBoostClassifier_staged_predict_proba = bridgeAdaBoostClassifier[${this.id}].staged_predict_proba(**pms_AdaBoostClassifier_staged_predict_proba)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_AdaBoostClassifier_staged_predict_proba.tolist() if hasattr(res_AdaBoostClassifier_staged_predict_proba, 'tolist') else res_AdaBoostClassifier_staged_predict_proba`\n  }\n\n  /**\n    Return staged scores for X, y.\n\n    This generator method yields the ensemble score after each iteration of boosting and therefore allows monitoring, such as to determine the score on a test set after each boost.\n   */\n  async staged_score(opts: {\n    /**\n      The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n     */\n    X?: ArrayLike | SparseMatrix[]\n\n    /**\n      Labels for X.\n     */\n    y?: ArrayLike\n\n    /**\n      Sample weights.\n     */\n    sample_weight?: ArrayLike\n  }): Promise<number> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This AdaBoostClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'AdaBoostClassifier must call init() before staged_score()'\n      )\n    }\n\n    // set up method params\n    await this._py.ex`pms_AdaBoostClassifier_staged_score = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None, 'y': np.array(${\n      opts['y'] ?? undefined\n    }) if ${opts['y'] !== undefined} else None, 'sample_weight': np.array(${\n      opts['sample_weight'] ?? undefined\n    }) if ${opts['sample_weight'] !== undefined} else None}\n\npms_AdaBoostClassifier_staged_score = {k: v for k, v in pms_AdaBoostClassifier_staged_score.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_AdaBoostClassifier_staged_score = bridgeAdaBoostClassifier[${this.id}].staged_score(**pms_AdaBoostClassifier_staged_score)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_AdaBoostClassifier_staged_score.tolist() if hasattr(res_AdaBoostClassifier_staged_score, 'tolist') else res_AdaBoostClassifier_staged_score`\n  }\n\n  /**\n    The base estimator from which the ensemble is grown.\n   */\n  get estimator_(): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This AdaBoostClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'AdaBoostClassifier must call init() before accessing estimator_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_AdaBoostClassifier_estimator_ = bridgeAdaBoostClassifier[${this.id}].estimator_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_AdaBoostClassifier_estimator_.tolist() if hasattr(attr_AdaBoostClassifier_estimator_, 'tolist') else attr_AdaBoostClassifier_estimator_`\n    })()\n  }\n\n  /**\n    The collection of fitted sub-estimators.\n   */\n  get estimators_(): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This AdaBoostClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'AdaBoostClassifier must call init() before accessing estimators_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_AdaBoostClassifier_estimators_ = bridgeAdaBoostClassifier[${this.id}].estimators_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_AdaBoostClassifier_estimators_.tolist() if hasattr(attr_AdaBoostClassifier_estimators_, 'tolist') else attr_AdaBoostClassifier_estimators_`\n    })()\n  }\n\n  /**\n    The classes labels.\n   */\n  get classes_(): Promise<NDArray> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This AdaBoostClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'AdaBoostClassifier must call init() before accessing classes_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_AdaBoostClassifier_classes_ = bridgeAdaBoostClassifier[${this.id}].classes_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_AdaBoostClassifier_classes_.tolist() if hasattr(attr_AdaBoostClassifier_classes_, 'tolist') else attr_AdaBoostClassifier_classes_`\n    })()\n  }\n\n  /**\n    The number of classes.\n   */\n  get n_classes_(): Promise<number> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This AdaBoostClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'AdaBoostClassifier must call init() before accessing n_classes_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_AdaBoostClassifier_n_classes_ = bridgeAdaBoostClassifier[${this.id}].n_classes_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_AdaBoostClassifier_n_classes_.tolist() if hasattr(attr_AdaBoostClassifier_n_classes_, 'tolist') else attr_AdaBoostClassifier_n_classes_`\n    })()\n  }\n\n  /**\n    Weights for each estimator in the boosted ensemble.\n   */\n  get estimator_weights_(): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This AdaBoostClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'AdaBoostClassifier must call init() before accessing estimator_weights_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_AdaBoostClassifier_estimator_weights_ = bridgeAdaBoostClassifier[${this.id}].estimator_weights_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_AdaBoostClassifier_estimator_weights_.tolist() if hasattr(attr_AdaBoostClassifier_estimator_weights_, 'tolist') else attr_AdaBoostClassifier_estimator_weights_`\n    })()\n  }\n\n  /**\n    Classification error for each estimator in the boosted ensemble.\n   */\n  get estimator_errors_(): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This AdaBoostClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'AdaBoostClassifier must call init() before accessing estimator_errors_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_AdaBoostClassifier_estimator_errors_ = bridgeAdaBoostClassifier[${this.id}].estimator_errors_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_AdaBoostClassifier_estimator_errors_.tolist() if hasattr(attr_AdaBoostClassifier_estimator_errors_, 'tolist') else attr_AdaBoostClassifier_estimator_errors_`\n    })()\n  }\n\n  /**\n    Number of features seen during [fit](../../glossary.html#term-fit).\n   */\n  get n_features_in_(): Promise<number> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This AdaBoostClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'AdaBoostClassifier must call init() before accessing n_features_in_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_AdaBoostClassifier_n_features_in_ = bridgeAdaBoostClassifier[${this.id}].n_features_in_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_AdaBoostClassifier_n_features_in_.tolist() if hasattr(attr_AdaBoostClassifier_n_features_in_, 'tolist') else attr_AdaBoostClassifier_n_features_in_`\n    })()\n  }\n\n  /**\n    Names of features seen during [fit](../../glossary.html#term-fit). Defined only when `X` has feature names that are all strings.\n   */\n  get feature_names_in_(): Promise<NDArray> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This AdaBoostClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'AdaBoostClassifier must call init() before accessing feature_names_in_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_AdaBoostClassifier_feature_names_in_ = bridgeAdaBoostClassifier[${this.id}].feature_names_in_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_AdaBoostClassifier_feature_names_in_.tolist() if hasattr(attr_AdaBoostClassifier_feature_names_in_, 'tolist') else attr_AdaBoostClassifier_feature_names_in_`\n    })()\n  }\n}\n","/* eslint-disable */\n/* NOTE: This file is auto-generated. Do not edit it directly. */\n\nimport crypto from 'node:crypto'\n\nimport { PythonBridge, NDArray, ArrayLike, SparseMatrix } from '@/sklearn/types'\n\n/**\n  An AdaBoost regressor.\n\n  An AdaBoost \\[1\\] regressor is a meta-estimator that begins by fitting a regressor on the original dataset and then fits additional copies of the regressor on the same dataset but where the weights of instances are adjusted according to the error of the current prediction. As such, subsequent regressors focus more on difficult cases.\n\n  This class implements the algorithm known as AdaBoost.R2 \\[2\\].\n\n  Read more in the [User Guide](../ensemble.html#adaboost).\n\n  [Python Reference](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html)\n */\nexport class AdaBoostRegressor {\n  id: string\n  opts: any\n\n  _py: PythonBridge\n  _isInitialized: boolean = false\n  _isDisposed: boolean = false\n\n  constructor(opts?: {\n    /**\n      The base estimator from which the boosted ensemble is built. If `undefined`, then the base estimator is [`DecisionTreeRegressor`](sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor \"sklearn.tree.DecisionTreeRegressor\") initialized with `max\\_depth=3`.\n     */\n    estimator?: any\n\n    /**\n      The maximum number of estimators at which boosting is terminated. In case of perfect fit, the learning procedure is stopped early. Values must be in the range `\\[1, inf)`.\n\n      @defaultValue `50`\n     */\n    n_estimators?: number\n\n    /**\n      Weight applied to each regressor at each boosting iteration. A higher learning rate increases the contribution of each regressor. There is a trade-off between the `learning\\_rate` and `n\\_estimators` parameters. Values must be in the range `(0.0, inf)`.\n\n      @defaultValue `1`\n     */\n    learning_rate?: number\n\n    /**\n      The loss function to use when updating the weights after each boosting iteration.\n\n      @defaultValue `'linear'`\n     */\n    loss?: 'linear' | 'square' | 'exponential'\n\n    /**\n      Controls the random seed given at each `estimator` at each boosting iteration. Thus, it is only used when `estimator` exposes a `random\\_state`. In addition, it controls the bootstrap of the weights used to train the `estimator` at each boosting iteration. Pass an int for reproducible output across multiple function calls. See [Glossary](../../glossary.html#term-random_state).\n     */\n    random_state?: number\n\n    /**\n      The base estimator from which the boosted ensemble is built. If `undefined`, then the base estimator is [`DecisionTreeRegressor`](sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor \"sklearn.tree.DecisionTreeRegressor\") initialized with `max\\_depth=3`.\n     */\n    base_estimator?: any\n  }) {\n    this.id = `AdaBoostRegressor${crypto.randomUUID().split('-')[0]}`\n    this.opts = opts || {}\n  }\n\n  get py(): PythonBridge {\n    return this._py\n  }\n\n  set py(pythonBridge: PythonBridge) {\n    this._py = pythonBridge\n  }\n\n  /**\n    Initializes the underlying Python resources.\n\n    This instance is not usable until the `Promise` returned by `init()` resolves.\n   */\n  async init(py: PythonBridge): Promise<void> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This AdaBoostRegressor instance has already been disposed'\n      )\n    }\n\n    if (this._isInitialized) {\n      return\n    }\n\n    if (!py) {\n      throw new Error('AdaBoostRegressor.init requires a PythonBridge instance')\n    }\n\n    this._py = py\n\n    await this._py.ex`\nimport numpy as np\nfrom sklearn.ensemble import AdaBoostRegressor\ntry: bridgeAdaBoostRegressor\nexcept NameError: bridgeAdaBoostRegressor = {}\n`\n\n    // set up constructor params\n    await this._py.ex`ctor_AdaBoostRegressor = {'estimator': ${\n      this.opts['estimator'] ?? undefined\n    }, 'n_estimators': ${\n      this.opts['n_estimators'] ?? undefined\n    }, 'learning_rate': ${this.opts['learning_rate'] ?? undefined}, 'loss': ${\n      this.opts['loss'] ?? undefined\n    }, 'random_state': ${\n      this.opts['random_state'] ?? undefined\n    }, 'base_estimator': ${this.opts['base_estimator'] ?? undefined}}\n\nctor_AdaBoostRegressor = {k: v for k, v in ctor_AdaBoostRegressor.items() if v is not None}`\n\n    await this._py\n      .ex`bridgeAdaBoostRegressor[${this.id}] = AdaBoostRegressor(**ctor_AdaBoostRegressor)`\n\n    this._isInitialized = true\n  }\n\n  /**\n    Disposes of the underlying Python resources.\n\n    Once `dispose()` is called, the instance is no longer usable.\n   */\n  async dispose() {\n    if (this._isDisposed) {\n      return\n    }\n\n    if (!this._isInitialized) {\n      return\n    }\n\n    await this._py.ex`del bridgeAdaBoostRegressor[${this.id}]`\n\n    this._isDisposed = true\n  }\n\n  /**\n    Build a boosted classifier/regressor from the training set (X, y).\n   */\n  async fit(opts: {\n    /**\n      The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n     */\n    X?: ArrayLike | SparseMatrix[]\n\n    /**\n      The target values.\n     */\n    y?: ArrayLike\n\n    /**\n      Sample weights. If `undefined`, the sample weights are initialized to 1 / n\\_samples.\n     */\n    sample_weight?: ArrayLike\n  }): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This AdaBoostRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('AdaBoostRegressor must call init() before fit()')\n    }\n\n    // set up method params\n    await this._py.ex`pms_AdaBoostRegressor_fit = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None, 'y': np.array(${\n      opts['y'] ?? undefined\n    }) if ${opts['y'] !== undefined} else None, 'sample_weight': np.array(${\n      opts['sample_weight'] ?? undefined\n    }) if ${opts['sample_weight'] !== undefined} else None}\n\npms_AdaBoostRegressor_fit = {k: v for k, v in pms_AdaBoostRegressor_fit.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_AdaBoostRegressor_fit = bridgeAdaBoostRegressor[${this.id}].fit(**pms_AdaBoostRegressor_fit)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_AdaBoostRegressor_fit.tolist() if hasattr(res_AdaBoostRegressor_fit, 'tolist') else res_AdaBoostRegressor_fit`\n  }\n\n  /**\n    Predict regression value for X.\n\n    The predicted regression value of an input sample is computed as the weighted median prediction of the regressors in the ensemble.\n   */\n  async predict(opts: {\n    /**\n      The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n     */\n    X?: ArrayLike | SparseMatrix[]\n  }): Promise<NDArray> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This AdaBoostRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('AdaBoostRegressor must call init() before predict()')\n    }\n\n    // set up method params\n    await this._py.ex`pms_AdaBoostRegressor_predict = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None}\n\npms_AdaBoostRegressor_predict = {k: v for k, v in pms_AdaBoostRegressor_predict.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_AdaBoostRegressor_predict = bridgeAdaBoostRegressor[${this.id}].predict(**pms_AdaBoostRegressor_predict)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_AdaBoostRegressor_predict.tolist() if hasattr(res_AdaBoostRegressor_predict, 'tolist') else res_AdaBoostRegressor_predict`\n  }\n\n  /**\n    Return the coefficient of determination of the prediction.\n\n    The coefficient of determination \\\\(R^2\\\\) is defined as \\\\((1 - \\\\frac{u}{v})\\\\), where \\\\(u\\\\) is the residual sum of squares `((y\\_true \\- y\\_pred)\\*\\* 2).sum()` and \\\\(v\\\\) is the total sum of squares `((y\\_true \\- y\\_true.mean()) \\*\\* 2).sum()`. The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of `y`, disregarding the input features, would get a \\\\(R^2\\\\) score of 0.0.\n   */\n  async score(opts: {\n    /**\n      Test samples. For some estimators this may be a precomputed kernel matrix or a list of generic objects instead with shape `(n\\_samples, n\\_samples\\_fitted)`, where `n\\_samples\\_fitted` is the number of samples used in the fitting for the estimator.\n     */\n    X?: ArrayLike[]\n\n    /**\n      True values for `X`.\n     */\n    y?: ArrayLike\n\n    /**\n      Sample weights.\n     */\n    sample_weight?: ArrayLike\n  }): Promise<number> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This AdaBoostRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('AdaBoostRegressor must call init() before score()')\n    }\n\n    // set up method params\n    await this._py.ex`pms_AdaBoostRegressor_score = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None, 'y': np.array(${\n      opts['y'] ?? undefined\n    }) if ${opts['y'] !== undefined} else None, 'sample_weight': np.array(${\n      opts['sample_weight'] ?? undefined\n    }) if ${opts['sample_weight'] !== undefined} else None}\n\npms_AdaBoostRegressor_score = {k: v for k, v in pms_AdaBoostRegressor_score.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_AdaBoostRegressor_score = bridgeAdaBoostRegressor[${this.id}].score(**pms_AdaBoostRegressor_score)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_AdaBoostRegressor_score.tolist() if hasattr(res_AdaBoostRegressor_score, 'tolist') else res_AdaBoostRegressor_score`\n  }\n\n  /**\n    Return staged predictions for X.\n\n    The predicted regression value of an input sample is computed as the weighted median prediction of the regressors in the ensemble.\n\n    This generator method yields the ensemble prediction after each iteration of boosting and therefore allows monitoring, such as to determine the prediction on a test set after each boost.\n   */\n  async staged_predict(opts: {\n    /**\n      The training input samples.\n     */\n    X?: ArrayLike | SparseMatrix[]\n  }): Promise<any[]> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This AdaBoostRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'AdaBoostRegressor must call init() before staged_predict()'\n      )\n    }\n\n    // set up method params\n    await this._py.ex`pms_AdaBoostRegressor_staged_predict = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None}\n\npms_AdaBoostRegressor_staged_predict = {k: v for k, v in pms_AdaBoostRegressor_staged_predict.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_AdaBoostRegressor_staged_predict = bridgeAdaBoostRegressor[${this.id}].staged_predict(**pms_AdaBoostRegressor_staged_predict)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_AdaBoostRegressor_staged_predict.tolist() if hasattr(res_AdaBoostRegressor_staged_predict, 'tolist') else res_AdaBoostRegressor_staged_predict`\n  }\n\n  /**\n    Return staged scores for X, y.\n\n    This generator method yields the ensemble score after each iteration of boosting and therefore allows monitoring, such as to determine the score on a test set after each boost.\n   */\n  async staged_score(opts: {\n    /**\n      The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n     */\n    X?: ArrayLike | SparseMatrix[]\n\n    /**\n      Labels for X.\n     */\n    y?: ArrayLike\n\n    /**\n      Sample weights.\n     */\n    sample_weight?: ArrayLike\n  }): Promise<number> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This AdaBoostRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'AdaBoostRegressor must call init() before staged_score()'\n      )\n    }\n\n    // set up method params\n    await this._py.ex`pms_AdaBoostRegressor_staged_score = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None, 'y': np.array(${\n      opts['y'] ?? undefined\n    }) if ${opts['y'] !== undefined} else None, 'sample_weight': np.array(${\n      opts['sample_weight'] ?? undefined\n    }) if ${opts['sample_weight'] !== undefined} else None}\n\npms_AdaBoostRegressor_staged_score = {k: v for k, v in pms_AdaBoostRegressor_staged_score.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_AdaBoostRegressor_staged_score = bridgeAdaBoostRegressor[${this.id}].staged_score(**pms_AdaBoostRegressor_staged_score)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_AdaBoostRegressor_staged_score.tolist() if hasattr(res_AdaBoostRegressor_staged_score, 'tolist') else res_AdaBoostRegressor_staged_score`\n  }\n\n  /**\n    The base estimator from which the ensemble is grown.\n   */\n  get estimator_(): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This AdaBoostRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'AdaBoostRegressor must call init() before accessing estimator_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_AdaBoostRegressor_estimator_ = bridgeAdaBoostRegressor[${this.id}].estimator_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_AdaBoostRegressor_estimator_.tolist() if hasattr(attr_AdaBoostRegressor_estimator_, 'tolist') else attr_AdaBoostRegressor_estimator_`\n    })()\n  }\n\n  /**\n    The collection of fitted sub-estimators.\n   */\n  get estimators_(): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This AdaBoostRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'AdaBoostRegressor must call init() before accessing estimators_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_AdaBoostRegressor_estimators_ = bridgeAdaBoostRegressor[${this.id}].estimators_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_AdaBoostRegressor_estimators_.tolist() if hasattr(attr_AdaBoostRegressor_estimators_, 'tolist') else attr_AdaBoostRegressor_estimators_`\n    })()\n  }\n\n  /**\n    Weights for each estimator in the boosted ensemble.\n   */\n  get estimator_weights_(): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This AdaBoostRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'AdaBoostRegressor must call init() before accessing estimator_weights_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_AdaBoostRegressor_estimator_weights_ = bridgeAdaBoostRegressor[${this.id}].estimator_weights_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_AdaBoostRegressor_estimator_weights_.tolist() if hasattr(attr_AdaBoostRegressor_estimator_weights_, 'tolist') else attr_AdaBoostRegressor_estimator_weights_`\n    })()\n  }\n\n  /**\n    Regression error for each estimator in the boosted ensemble.\n   */\n  get estimator_errors_(): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This AdaBoostRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'AdaBoostRegressor must call init() before accessing estimator_errors_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_AdaBoostRegressor_estimator_errors_ = bridgeAdaBoostRegressor[${this.id}].estimator_errors_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_AdaBoostRegressor_estimator_errors_.tolist() if hasattr(attr_AdaBoostRegressor_estimator_errors_, 'tolist') else attr_AdaBoostRegressor_estimator_errors_`\n    })()\n  }\n\n  /**\n    Number of features seen during [fit](../../glossary.html#term-fit).\n   */\n  get n_features_in_(): Promise<number> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This AdaBoostRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'AdaBoostRegressor must call init() before accessing n_features_in_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_AdaBoostRegressor_n_features_in_ = bridgeAdaBoostRegressor[${this.id}].n_features_in_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_AdaBoostRegressor_n_features_in_.tolist() if hasattr(attr_AdaBoostRegressor_n_features_in_, 'tolist') else attr_AdaBoostRegressor_n_features_in_`\n    })()\n  }\n\n  /**\n    Names of features seen during [fit](../../glossary.html#term-fit). Defined only when `X` has feature names that are all strings.\n   */\n  get feature_names_in_(): Promise<NDArray> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This AdaBoostRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'AdaBoostRegressor must call init() before accessing feature_names_in_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_AdaBoostRegressor_feature_names_in_ = bridgeAdaBoostRegressor[${this.id}].feature_names_in_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_AdaBoostRegressor_feature_names_in_.tolist() if hasattr(attr_AdaBoostRegressor_feature_names_in_, 'tolist') else attr_AdaBoostRegressor_feature_names_in_`\n    })()\n  }\n}\n","/* eslint-disable */\n/* NOTE: This file is auto-generated. Do not edit it directly. */\n\nimport crypto from 'node:crypto'\n\nimport { PythonBridge, NDArray, ArrayLike, SparseMatrix } from '@/sklearn/types'\n\n/**\n  A Bagging classifier.\n\n  A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it.\n\n  This algorithm encompasses several works from the literature. When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting [\\[1\\]](#rb1846455d0e5-1). If samples are drawn with replacement, then the method is known as Bagging [\\[2\\]](#rb1846455d0e5-2). When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces [\\[3\\]](#rb1846455d0e5-3). Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches [\\[4\\]](#rb1846455d0e5-4).\n\n  Read more in the [User Guide](../ensemble.html#bagging).\n\n  [Python Reference](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html)\n */\nexport class BaggingClassifier {\n  id: string\n  opts: any\n\n  _py: PythonBridge\n  _isInitialized: boolean = false\n  _isDisposed: boolean = false\n\n  constructor(opts?: {\n    /**\n      The base estimator to fit on random subsets of the dataset. If `undefined`, then the base estimator is a [`DecisionTreeClassifier`](sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier \"sklearn.tree.DecisionTreeClassifier\").\n     */\n    estimator?: any\n\n    /**\n      The number of base estimators in the ensemble.\n\n      @defaultValue `10`\n     */\n    n_estimators?: number\n\n    /**\n      The number of samples to draw from X to train each base estimator (with replacement by default, see `bootstrap` for more details).\n\n      @defaultValue `1`\n     */\n    max_samples?: number\n\n    /**\n      The number of features to draw from X to train each base estimator ( without replacement by default, see `bootstrap\\_features` for more details).\n\n      @defaultValue `1`\n     */\n    max_features?: number\n\n    /**\n      Whether samples are drawn with replacement. If `false`, sampling without replacement is performed.\n\n      @defaultValue `true`\n     */\n    bootstrap?: boolean\n\n    /**\n      Whether features are drawn with replacement.\n\n      @defaultValue `false`\n     */\n    bootstrap_features?: boolean\n\n    /**\n      Whether to use out-of-bag samples to estimate the generalization error. Only available if bootstrap=`true`.\n\n      @defaultValue `false`\n     */\n    oob_score?: boolean\n\n    /**\n      When set to `true`, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new ensemble. See [the Glossary](../../glossary.html#term-warm_start).\n\n      @defaultValue `false`\n     */\n    warm_start?: boolean\n\n    /**\n      The number of jobs to run in parallel for both [`fit`](#sklearn.ensemble.BaggingClassifier.fit \"sklearn.ensemble.BaggingClassifier.fit\") and [`predict`](#sklearn.ensemble.BaggingClassifier.predict \"sklearn.ensemble.BaggingClassifier.predict\"). `undefined` means 1 unless in a [`joblib.parallel\\_backend`](https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend \"(in joblib v1.3.0.dev0)\") context. `\\-1` means using all processors. See [Glossary](../../glossary.html#term-n_jobs) for more details.\n     */\n    n_jobs?: number\n\n    /**\n      Controls the random resampling of the original dataset (sample wise and feature wise). If the base estimator accepts a `random\\_state` attribute, a different seed is generated for each instance in the ensemble. Pass an int for reproducible output across multiple function calls. See [Glossary](../../glossary.html#term-random_state).\n     */\n    random_state?: number\n\n    /**\n      Controls the verbosity when fitting and predicting.\n\n      @defaultValue `0`\n     */\n    verbose?: number\n\n    /**\n      Use `estimator` instead.\n\n      @defaultValue `'deprecated'`\n     */\n    base_estimator?: any\n  }) {\n    this.id = `BaggingClassifier${crypto.randomUUID().split('-')[0]}`\n    this.opts = opts || {}\n  }\n\n  get py(): PythonBridge {\n    return this._py\n  }\n\n  set py(pythonBridge: PythonBridge) {\n    this._py = pythonBridge\n  }\n\n  /**\n    Initializes the underlying Python resources.\n\n    This instance is not usable until the `Promise` returned by `init()` resolves.\n   */\n  async init(py: PythonBridge): Promise<void> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This BaggingClassifier instance has already been disposed'\n      )\n    }\n\n    if (this._isInitialized) {\n      return\n    }\n\n    if (!py) {\n      throw new Error('BaggingClassifier.init requires a PythonBridge instance')\n    }\n\n    this._py = py\n\n    await this._py.ex`\nimport numpy as np\nfrom sklearn.ensemble import BaggingClassifier\ntry: bridgeBaggingClassifier\nexcept NameError: bridgeBaggingClassifier = {}\n`\n\n    // set up constructor params\n    await this._py.ex`ctor_BaggingClassifier = {'estimator': ${\n      this.opts['estimator'] ?? undefined\n    }, 'n_estimators': ${\n      this.opts['n_estimators'] ?? undefined\n    }, 'max_samples': ${\n      this.opts['max_samples'] ?? undefined\n    }, 'max_features': ${\n      this.opts['max_features'] ?? undefined\n    }, 'bootstrap': ${\n      this.opts['bootstrap'] ?? undefined\n    }, 'bootstrap_features': ${\n      this.opts['bootstrap_features'] ?? undefined\n    }, 'oob_score': ${this.opts['oob_score'] ?? undefined}, 'warm_start': ${\n      this.opts['warm_start'] ?? undefined\n    }, 'n_jobs': ${this.opts['n_jobs'] ?? undefined}, 'random_state': ${\n      this.opts['random_state'] ?? undefined\n    }, 'verbose': ${this.opts['verbose'] ?? undefined}, 'base_estimator': ${\n      this.opts['base_estimator'] ?? undefined\n    }}\n\nctor_BaggingClassifier = {k: v for k, v in ctor_BaggingClassifier.items() if v is not None}`\n\n    await this._py\n      .ex`bridgeBaggingClassifier[${this.id}] = BaggingClassifier(**ctor_BaggingClassifier)`\n\n    this._isInitialized = true\n  }\n\n  /**\n    Disposes of the underlying Python resources.\n\n    Once `dispose()` is called, the instance is no longer usable.\n   */\n  async dispose() {\n    if (this._isDisposed) {\n      return\n    }\n\n    if (!this._isInitialized) {\n      return\n    }\n\n    await this._py.ex`del bridgeBaggingClassifier[${this.id}]`\n\n    this._isDisposed = true\n  }\n\n  /**\n    Average of the decision functions of the base classifiers.\n   */\n  async decision_function(opts: {\n    /**\n      The training input samples. Sparse matrices are accepted only if they are supported by the base estimator.\n     */\n    X?: ArrayLike | SparseMatrix[]\n  }): Promise<NDArray[]> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This BaggingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'BaggingClassifier must call init() before decision_function()'\n      )\n    }\n\n    // set up method params\n    await this._py\n      .ex`pms_BaggingClassifier_decision_function = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None}\n\npms_BaggingClassifier_decision_function = {k: v for k, v in pms_BaggingClassifier_decision_function.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_BaggingClassifier_decision_function = bridgeBaggingClassifier[${this.id}].decision_function(**pms_BaggingClassifier_decision_function)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_BaggingClassifier_decision_function.tolist() if hasattr(res_BaggingClassifier_decision_function, 'tolist') else res_BaggingClassifier_decision_function`\n  }\n\n  /**\n    Build a Bagging ensemble of estimators from the training set (X, y).\n   */\n  async fit(opts: {\n    /**\n      The training input samples. Sparse matrices are accepted only if they are supported by the base estimator.\n     */\n    X?: ArrayLike | SparseMatrix[]\n\n    /**\n      The target values (class labels in classification, real numbers in regression).\n     */\n    y?: ArrayLike\n\n    /**\n      Sample weights. If `undefined`, then samples are equally weighted. Note that this is supported only if the base estimator supports sample weighting.\n     */\n    sample_weight?: ArrayLike\n  }): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This BaggingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('BaggingClassifier must call init() before fit()')\n    }\n\n    // set up method params\n    await this._py.ex`pms_BaggingClassifier_fit = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None, 'y': np.array(${\n      opts['y'] ?? undefined\n    }) if ${opts['y'] !== undefined} else None, 'sample_weight': np.array(${\n      opts['sample_weight'] ?? undefined\n    }) if ${opts['sample_weight'] !== undefined} else None}\n\npms_BaggingClassifier_fit = {k: v for k, v in pms_BaggingClassifier_fit.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_BaggingClassifier_fit = bridgeBaggingClassifier[${this.id}].fit(**pms_BaggingClassifier_fit)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_BaggingClassifier_fit.tolist() if hasattr(res_BaggingClassifier_fit, 'tolist') else res_BaggingClassifier_fit`\n  }\n\n  /**\n    Predict class for X.\n\n    The predicted class of an input sample is computed as the class with the highest mean predicted probability. If base estimators do not implement a `predict\\_proba` method, then it resorts to voting.\n   */\n  async predict(opts: {\n    /**\n      The training input samples. Sparse matrices are accepted only if they are supported by the base estimator.\n     */\n    X?: ArrayLike | SparseMatrix[]\n  }): Promise<NDArray> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This BaggingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('BaggingClassifier must call init() before predict()')\n    }\n\n    // set up method params\n    await this._py.ex`pms_BaggingClassifier_predict = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None}\n\npms_BaggingClassifier_predict = {k: v for k, v in pms_BaggingClassifier_predict.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_BaggingClassifier_predict = bridgeBaggingClassifier[${this.id}].predict(**pms_BaggingClassifier_predict)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_BaggingClassifier_predict.tolist() if hasattr(res_BaggingClassifier_predict, 'tolist') else res_BaggingClassifier_predict`\n  }\n\n  /**\n    Predict class log-probabilities for X.\n\n    The predicted class log-probabilities of an input sample is computed as the log of the mean predicted class probabilities of the base estimators in the ensemble.\n   */\n  async predict_log_proba(opts: {\n    /**\n      The training input samples. Sparse matrices are accepted only if they are supported by the base estimator.\n     */\n    X?: ArrayLike | SparseMatrix[]\n  }): Promise<NDArray[]> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This BaggingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'BaggingClassifier must call init() before predict_log_proba()'\n      )\n    }\n\n    // set up method params\n    await this._py\n      .ex`pms_BaggingClassifier_predict_log_proba = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None}\n\npms_BaggingClassifier_predict_log_proba = {k: v for k, v in pms_BaggingClassifier_predict_log_proba.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_BaggingClassifier_predict_log_proba = bridgeBaggingClassifier[${this.id}].predict_log_proba(**pms_BaggingClassifier_predict_log_proba)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_BaggingClassifier_predict_log_proba.tolist() if hasattr(res_BaggingClassifier_predict_log_proba, 'tolist') else res_BaggingClassifier_predict_log_proba`\n  }\n\n  /**\n    Predict class probabilities for X.\n\n    The predicted class probabilities of an input sample is computed as the mean predicted class probabilities of the base estimators in the ensemble. If base estimators do not implement a `predict\\_proba` method, then it resorts to voting and the predicted class probabilities of an input sample represents the proportion of estimators predicting each class.\n   */\n  async predict_proba(opts: {\n    /**\n      The training input samples. Sparse matrices are accepted only if they are supported by the base estimator.\n     */\n    X?: ArrayLike | SparseMatrix[]\n  }): Promise<NDArray[]> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This BaggingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'BaggingClassifier must call init() before predict_proba()'\n      )\n    }\n\n    // set up method params\n    await this._py.ex`pms_BaggingClassifier_predict_proba = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None}\n\npms_BaggingClassifier_predict_proba = {k: v for k, v in pms_BaggingClassifier_predict_proba.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_BaggingClassifier_predict_proba = bridgeBaggingClassifier[${this.id}].predict_proba(**pms_BaggingClassifier_predict_proba)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_BaggingClassifier_predict_proba.tolist() if hasattr(res_BaggingClassifier_predict_proba, 'tolist') else res_BaggingClassifier_predict_proba`\n  }\n\n  /**\n    Return the mean accuracy on the given test data and labels.\n\n    In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted.\n   */\n  async score(opts: {\n    /**\n      Test samples.\n     */\n    X?: ArrayLike[]\n\n    /**\n      True labels for `X`.\n     */\n    y?: ArrayLike\n\n    /**\n      Sample weights.\n     */\n    sample_weight?: ArrayLike\n  }): Promise<number> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This BaggingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('BaggingClassifier must call init() before score()')\n    }\n\n    // set up method params\n    await this._py.ex`pms_BaggingClassifier_score = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None, 'y': np.array(${\n      opts['y'] ?? undefined\n    }) if ${opts['y'] !== undefined} else None, 'sample_weight': np.array(${\n      opts['sample_weight'] ?? undefined\n    }) if ${opts['sample_weight'] !== undefined} else None}\n\npms_BaggingClassifier_score = {k: v for k, v in pms_BaggingClassifier_score.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_BaggingClassifier_score = bridgeBaggingClassifier[${this.id}].score(**pms_BaggingClassifier_score)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_BaggingClassifier_score.tolist() if hasattr(res_BaggingClassifier_score, 'tolist') else res_BaggingClassifier_score`\n  }\n\n  /**\n    The base estimator from which the ensemble is grown.\n   */\n  get estimator_(): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This BaggingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'BaggingClassifier must call init() before accessing estimator_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_BaggingClassifier_estimator_ = bridgeBaggingClassifier[${this.id}].estimator_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_BaggingClassifier_estimator_.tolist() if hasattr(attr_BaggingClassifier_estimator_, 'tolist') else attr_BaggingClassifier_estimator_`\n    })()\n  }\n\n  /**\n    Number of features seen during [fit](../../glossary.html#term-fit).\n   */\n  get n_features_in_(): Promise<number> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This BaggingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'BaggingClassifier must call init() before accessing n_features_in_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_BaggingClassifier_n_features_in_ = bridgeBaggingClassifier[${this.id}].n_features_in_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_BaggingClassifier_n_features_in_.tolist() if hasattr(attr_BaggingClassifier_n_features_in_, 'tolist') else attr_BaggingClassifier_n_features_in_`\n    })()\n  }\n\n  /**\n    Names of features seen during [fit](../../glossary.html#term-fit). Defined only when `X` has feature names that are all strings.\n   */\n  get feature_names_in_(): Promise<NDArray> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This BaggingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'BaggingClassifier must call init() before accessing feature_names_in_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_BaggingClassifier_feature_names_in_ = bridgeBaggingClassifier[${this.id}].feature_names_in_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_BaggingClassifier_feature_names_in_.tolist() if hasattr(attr_BaggingClassifier_feature_names_in_, 'tolist') else attr_BaggingClassifier_feature_names_in_`\n    })()\n  }\n\n  /**\n    The collection of fitted base estimators.\n   */\n  get estimators_(): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This BaggingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'BaggingClassifier must call init() before accessing estimators_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_BaggingClassifier_estimators_ = bridgeBaggingClassifier[${this.id}].estimators_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_BaggingClassifier_estimators_.tolist() if hasattr(attr_BaggingClassifier_estimators_, 'tolist') else attr_BaggingClassifier_estimators_`\n    })()\n  }\n\n  /**\n    The subset of drawn features for each base estimator.\n   */\n  get estimators_features_(): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This BaggingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'BaggingClassifier must call init() before accessing estimators_features_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_BaggingClassifier_estimators_features_ = bridgeBaggingClassifier[${this.id}].estimators_features_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_BaggingClassifier_estimators_features_.tolist() if hasattr(attr_BaggingClassifier_estimators_features_, 'tolist') else attr_BaggingClassifier_estimators_features_`\n    })()\n  }\n\n  /**\n    The classes labels.\n   */\n  get classes_(): Promise<NDArray> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This BaggingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'BaggingClassifier must call init() before accessing classes_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_BaggingClassifier_classes_ = bridgeBaggingClassifier[${this.id}].classes_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_BaggingClassifier_classes_.tolist() if hasattr(attr_BaggingClassifier_classes_, 'tolist') else attr_BaggingClassifier_classes_`\n    })()\n  }\n\n  /**\n    The number of classes.\n   */\n  get n_classes_(): Promise<number | any[]> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This BaggingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'BaggingClassifier must call init() before accessing n_classes_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_BaggingClassifier_n_classes_ = bridgeBaggingClassifier[${this.id}].n_classes_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_BaggingClassifier_n_classes_.tolist() if hasattr(attr_BaggingClassifier_n_classes_, 'tolist') else attr_BaggingClassifier_n_classes_`\n    })()\n  }\n\n  /**\n    Score of the training dataset obtained using an out-of-bag estimate. This attribute exists only when `oob\\_score` is `true`.\n   */\n  get oob_score_(): Promise<number> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This BaggingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'BaggingClassifier must call init() before accessing oob_score_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_BaggingClassifier_oob_score_ = bridgeBaggingClassifier[${this.id}].oob_score_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_BaggingClassifier_oob_score_.tolist() if hasattr(attr_BaggingClassifier_oob_score_, 'tolist') else attr_BaggingClassifier_oob_score_`\n    })()\n  }\n\n  /**\n    Decision function computed with out-of-bag estimate on the training set. If n\\_estimators is small it might be possible that a data point was never left out during the bootstrap. In this case, `oob\\_decision\\_function\\_` might contain NaN. This attribute exists only when `oob\\_score` is `true`.\n   */\n  get oob_decision_function_(): Promise<NDArray[]> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This BaggingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'BaggingClassifier must call init() before accessing oob_decision_function_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_BaggingClassifier_oob_decision_function_ = bridgeBaggingClassifier[${this.id}].oob_decision_function_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_BaggingClassifier_oob_decision_function_.tolist() if hasattr(attr_BaggingClassifier_oob_decision_function_, 'tolist') else attr_BaggingClassifier_oob_decision_function_`\n    })()\n  }\n}\n","/* eslint-disable */\n/* NOTE: This file is auto-generated. Do not edit it directly. */\n\nimport crypto from 'node:crypto'\n\nimport { PythonBridge, NDArray, ArrayLike, SparseMatrix } from '@/sklearn/types'\n\n/**\n  A Bagging regressor.\n\n  A Bagging regressor is an ensemble meta-estimator that fits base regressors each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it.\n\n  This algorithm encompasses several works from the literature. When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting [\\[1\\]](#r4d113ba76fc0-1). If samples are drawn with replacement, then the method is known as Bagging [\\[2\\]](#r4d113ba76fc0-2). When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces [\\[3\\]](#r4d113ba76fc0-3). Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches [\\[4\\]](#r4d113ba76fc0-4).\n\n  Read more in the [User Guide](../ensemble.html#bagging).\n\n  [Python Reference](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingRegressor.html)\n */\nexport class BaggingRegressor {\n  id: string\n  opts: any\n\n  _py: PythonBridge\n  _isInitialized: boolean = false\n  _isDisposed: boolean = false\n\n  constructor(opts?: {\n    /**\n      The base estimator to fit on random subsets of the dataset. If `undefined`, then the base estimator is a [`DecisionTreeRegressor`](sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor \"sklearn.tree.DecisionTreeRegressor\").\n     */\n    estimator?: any\n\n    /**\n      The number of base estimators in the ensemble.\n\n      @defaultValue `10`\n     */\n    n_estimators?: number\n\n    /**\n      The number of samples to draw from X to train each base estimator (with replacement by default, see `bootstrap` for more details).\n\n      @defaultValue `1`\n     */\n    max_samples?: number\n\n    /**\n      The number of features to draw from X to train each base estimator ( without replacement by default, see `bootstrap\\_features` for more details).\n\n      @defaultValue `1`\n     */\n    max_features?: number\n\n    /**\n      Whether samples are drawn with replacement. If `false`, sampling without replacement is performed.\n\n      @defaultValue `true`\n     */\n    bootstrap?: boolean\n\n    /**\n      Whether features are drawn with replacement.\n\n      @defaultValue `false`\n     */\n    bootstrap_features?: boolean\n\n    /**\n      Whether to use out-of-bag samples to estimate the generalization error. Only available if bootstrap=`true`.\n\n      @defaultValue `false`\n     */\n    oob_score?: boolean\n\n    /**\n      When set to `true`, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new ensemble. See [the Glossary](../../glossary.html#term-warm_start).\n\n      @defaultValue `false`\n     */\n    warm_start?: boolean\n\n    /**\n      The number of jobs to run in parallel for both [`fit`](#sklearn.ensemble.BaggingRegressor.fit \"sklearn.ensemble.BaggingRegressor.fit\") and [`predict`](#sklearn.ensemble.BaggingRegressor.predict \"sklearn.ensemble.BaggingRegressor.predict\"). `undefined` means 1 unless in a [`joblib.parallel\\_backend`](https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend \"(in joblib v1.3.0.dev0)\") context. `\\-1` means using all processors. See [Glossary](../../glossary.html#term-n_jobs) for more details.\n     */\n    n_jobs?: number\n\n    /**\n      Controls the random resampling of the original dataset (sample wise and feature wise). If the base estimator accepts a `random\\_state` attribute, a different seed is generated for each instance in the ensemble. Pass an int for reproducible output across multiple function calls. See [Glossary](../../glossary.html#term-random_state).\n     */\n    random_state?: number\n\n    /**\n      Controls the verbosity when fitting and predicting.\n\n      @defaultValue `0`\n     */\n    verbose?: number\n\n    /**\n      Use `estimator` instead.\n\n      @defaultValue `'deprecated'`\n     */\n    base_estimator?: any\n  }) {\n    this.id = `BaggingRegressor${crypto.randomUUID().split('-')[0]}`\n    this.opts = opts || {}\n  }\n\n  get py(): PythonBridge {\n    return this._py\n  }\n\n  set py(pythonBridge: PythonBridge) {\n    this._py = pythonBridge\n  }\n\n  /**\n    Initializes the underlying Python resources.\n\n    This instance is not usable until the `Promise` returned by `init()` resolves.\n   */\n  async init(py: PythonBridge): Promise<void> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This BaggingRegressor instance has already been disposed'\n      )\n    }\n\n    if (this._isInitialized) {\n      return\n    }\n\n    if (!py) {\n      throw new Error('BaggingRegressor.init requires a PythonBridge instance')\n    }\n\n    this._py = py\n\n    await this._py.ex`\nimport numpy as np\nfrom sklearn.ensemble import BaggingRegressor\ntry: bridgeBaggingRegressor\nexcept NameError: bridgeBaggingRegressor = {}\n`\n\n    // set up constructor params\n    await this._py.ex`ctor_BaggingRegressor = {'estimator': ${\n      this.opts['estimator'] ?? undefined\n    }, 'n_estimators': ${\n      this.opts['n_estimators'] ?? undefined\n    }, 'max_samples': ${\n      this.opts['max_samples'] ?? undefined\n    }, 'max_features': ${\n      this.opts['max_features'] ?? undefined\n    }, 'bootstrap': ${\n      this.opts['bootstrap'] ?? undefined\n    }, 'bootstrap_features': ${\n      this.opts['bootstrap_features'] ?? undefined\n    }, 'oob_score': ${this.opts['oob_score'] ?? undefined}, 'warm_start': ${\n      this.opts['warm_start'] ?? undefined\n    }, 'n_jobs': ${this.opts['n_jobs'] ?? undefined}, 'random_state': ${\n      this.opts['random_state'] ?? undefined\n    }, 'verbose': ${this.opts['verbose'] ?? undefined}, 'base_estimator': ${\n      this.opts['base_estimator'] ?? undefined\n    }}\n\nctor_BaggingRegressor = {k: v for k, v in ctor_BaggingRegressor.items() if v is not None}`\n\n    await this._py\n      .ex`bridgeBaggingRegressor[${this.id}] = BaggingRegressor(**ctor_BaggingRegressor)`\n\n    this._isInitialized = true\n  }\n\n  /**\n    Disposes of the underlying Python resources.\n\n    Once `dispose()` is called, the instance is no longer usable.\n   */\n  async dispose() {\n    if (this._isDisposed) {\n      return\n    }\n\n    if (!this._isInitialized) {\n      return\n    }\n\n    await this._py.ex`del bridgeBaggingRegressor[${this.id}]`\n\n    this._isDisposed = true\n  }\n\n  /**\n    Build a Bagging ensemble of estimators from the training set (X, y).\n   */\n  async fit(opts: {\n    /**\n      The training input samples. Sparse matrices are accepted only if they are supported by the base estimator.\n     */\n    X?: ArrayLike | SparseMatrix[]\n\n    /**\n      The target values (class labels in classification, real numbers in regression).\n     */\n    y?: ArrayLike\n\n    /**\n      Sample weights. If `undefined`, then samples are equally weighted. Note that this is supported only if the base estimator supports sample weighting.\n     */\n    sample_weight?: ArrayLike\n  }): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This BaggingRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('BaggingRegressor must call init() before fit()')\n    }\n\n    // set up method params\n    await this._py.ex`pms_BaggingRegressor_fit = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None, 'y': np.array(${\n      opts['y'] ?? undefined\n    }) if ${opts['y'] !== undefined} else None, 'sample_weight': np.array(${\n      opts['sample_weight'] ?? undefined\n    }) if ${opts['sample_weight'] !== undefined} else None}\n\npms_BaggingRegressor_fit = {k: v for k, v in pms_BaggingRegressor_fit.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_BaggingRegressor_fit = bridgeBaggingRegressor[${this.id}].fit(**pms_BaggingRegressor_fit)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_BaggingRegressor_fit.tolist() if hasattr(res_BaggingRegressor_fit, 'tolist') else res_BaggingRegressor_fit`\n  }\n\n  /**\n    Predict regression target for X.\n\n    The predicted regression target of an input sample is computed as the mean predicted regression targets of the estimators in the ensemble.\n   */\n  async predict(opts: {\n    /**\n      The training input samples. Sparse matrices are accepted only if they are supported by the base estimator.\n     */\n    X?: ArrayLike | SparseMatrix[]\n  }): Promise<NDArray> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This BaggingRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('BaggingRegressor must call init() before predict()')\n    }\n\n    // set up method params\n    await this._py.ex`pms_BaggingRegressor_predict = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None}\n\npms_BaggingRegressor_predict = {k: v for k, v in pms_BaggingRegressor_predict.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_BaggingRegressor_predict = bridgeBaggingRegressor[${this.id}].predict(**pms_BaggingRegressor_predict)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_BaggingRegressor_predict.tolist() if hasattr(res_BaggingRegressor_predict, 'tolist') else res_BaggingRegressor_predict`\n  }\n\n  /**\n    Return the coefficient of determination of the prediction.\n\n    The coefficient of determination \\\\(R^2\\\\) is defined as \\\\((1 - \\\\frac{u}{v})\\\\), where \\\\(u\\\\) is the residual sum of squares `((y\\_true \\- y\\_pred)\\*\\* 2).sum()` and \\\\(v\\\\) is the total sum of squares `((y\\_true \\- y\\_true.mean()) \\*\\* 2).sum()`. The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of `y`, disregarding the input features, would get a \\\\(R^2\\\\) score of 0.0.\n   */\n  async score(opts: {\n    /**\n      Test samples. For some estimators this may be a precomputed kernel matrix or a list of generic objects instead with shape `(n\\_samples, n\\_samples\\_fitted)`, where `n\\_samples\\_fitted` is the number of samples used in the fitting for the estimator.\n     */\n    X?: ArrayLike[]\n\n    /**\n      True values for `X`.\n     */\n    y?: ArrayLike\n\n    /**\n      Sample weights.\n     */\n    sample_weight?: ArrayLike\n  }): Promise<number> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This BaggingRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('BaggingRegressor must call init() before score()')\n    }\n\n    // set up method params\n    await this._py.ex`pms_BaggingRegressor_score = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None, 'y': np.array(${\n      opts['y'] ?? undefined\n    }) if ${opts['y'] !== undefined} else None, 'sample_weight': np.array(${\n      opts['sample_weight'] ?? undefined\n    }) if ${opts['sample_weight'] !== undefined} else None}\n\npms_BaggingRegressor_score = {k: v for k, v in pms_BaggingRegressor_score.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_BaggingRegressor_score = bridgeBaggingRegressor[${this.id}].score(**pms_BaggingRegressor_score)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_BaggingRegressor_score.tolist() if hasattr(res_BaggingRegressor_score, 'tolist') else res_BaggingRegressor_score`\n  }\n\n  /**\n    The base estimator from which the ensemble is grown.\n   */\n  get estimator_(): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This BaggingRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'BaggingRegressor must call init() before accessing estimator_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_BaggingRegressor_estimator_ = bridgeBaggingRegressor[${this.id}].estimator_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_BaggingRegressor_estimator_.tolist() if hasattr(attr_BaggingRegressor_estimator_, 'tolist') else attr_BaggingRegressor_estimator_`\n    })()\n  }\n\n  /**\n    Number of features seen during [fit](../../glossary.html#term-fit).\n   */\n  get n_features_in_(): Promise<number> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This BaggingRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'BaggingRegressor must call init() before accessing n_features_in_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_BaggingRegressor_n_features_in_ = bridgeBaggingRegressor[${this.id}].n_features_in_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_BaggingRegressor_n_features_in_.tolist() if hasattr(attr_BaggingRegressor_n_features_in_, 'tolist') else attr_BaggingRegressor_n_features_in_`\n    })()\n  }\n\n  /**\n    Names of features seen during [fit](../../glossary.html#term-fit). Defined only when `X` has feature names that are all strings.\n   */\n  get feature_names_in_(): Promise<NDArray> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This BaggingRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'BaggingRegressor must call init() before accessing feature_names_in_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_BaggingRegressor_feature_names_in_ = bridgeBaggingRegressor[${this.id}].feature_names_in_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_BaggingRegressor_feature_names_in_.tolist() if hasattr(attr_BaggingRegressor_feature_names_in_, 'tolist') else attr_BaggingRegressor_feature_names_in_`\n    })()\n  }\n\n  /**\n    The collection of fitted sub-estimators.\n   */\n  get estimators_(): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This BaggingRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'BaggingRegressor must call init() before accessing estimators_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_BaggingRegressor_estimators_ = bridgeBaggingRegressor[${this.id}].estimators_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_BaggingRegressor_estimators_.tolist() if hasattr(attr_BaggingRegressor_estimators_, 'tolist') else attr_BaggingRegressor_estimators_`\n    })()\n  }\n\n  /**\n    The subset of drawn features for each base estimator.\n   */\n  get estimators_features_(): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This BaggingRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'BaggingRegressor must call init() before accessing estimators_features_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_BaggingRegressor_estimators_features_ = bridgeBaggingRegressor[${this.id}].estimators_features_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_BaggingRegressor_estimators_features_.tolist() if hasattr(attr_BaggingRegressor_estimators_features_, 'tolist') else attr_BaggingRegressor_estimators_features_`\n    })()\n  }\n\n  /**\n    Score of the training dataset obtained using an out-of-bag estimate. This attribute exists only when `oob\\_score` is `true`.\n   */\n  get oob_score_(): Promise<number> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This BaggingRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'BaggingRegressor must call init() before accessing oob_score_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_BaggingRegressor_oob_score_ = bridgeBaggingRegressor[${this.id}].oob_score_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_BaggingRegressor_oob_score_.tolist() if hasattr(attr_BaggingRegressor_oob_score_, 'tolist') else attr_BaggingRegressor_oob_score_`\n    })()\n  }\n\n  /**\n    Prediction computed with out-of-bag estimate on the training set. If n\\_estimators is small it might be possible that a data point was never left out during the bootstrap. In this case, `oob\\_prediction\\_` might contain NaN. This attribute exists only when `oob\\_score` is `true`.\n   */\n  get oob_prediction_(): Promise<NDArray> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This BaggingRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'BaggingRegressor must call init() before accessing oob_prediction_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_BaggingRegressor_oob_prediction_ = bridgeBaggingRegressor[${this.id}].oob_prediction_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_BaggingRegressor_oob_prediction_.tolist() if hasattr(attr_BaggingRegressor_oob_prediction_, 'tolist') else attr_BaggingRegressor_oob_prediction_`\n    })()\n  }\n}\n","/* eslint-disable */\n/* NOTE: This file is auto-generated. Do not edit it directly. */\n\nimport crypto from 'node:crypto'\n\nimport { PythonBridge, NDArray, ArrayLike, SparseMatrix } from '@/sklearn/types'\n\n/**\n  An extra-trees classifier.\n\n  This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.\n\n  Read more in the [User Guide](../ensemble.html#forest).\n\n  [Python Reference](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html)\n */\nexport class ExtraTreesClassifier {\n  id: string\n  opts: any\n\n  _py: PythonBridge\n  _isInitialized: boolean = false\n  _isDisposed: boolean = false\n\n  constructor(opts?: {\n    /**\n      The number of trees in the forest.\n\n      @defaultValue `100`\n     */\n    n_estimators?: number\n\n    /**\n      The function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “log\\_loss” and “entropy” both for the Shannon information gain, see [Mathematical formulation](../tree.html#tree-mathematical-formulation). Note: This parameter is tree-specific.\n\n      @defaultValue `'gini'`\n     */\n    criterion?: 'gini' | 'entropy' | 'log_loss'\n\n    /**\n      The maximum depth of the tree. If `undefined`, then nodes are expanded until all leaves are pure or until all leaves contain less than min\\_samples\\_split samples.\n     */\n    max_depth?: number\n\n    /**\n      The minimum number of samples required to split an internal node:\n\n      @defaultValue `2`\n     */\n    min_samples_split?: number\n\n    /**\n      The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least `min\\_samples\\_leaf` training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.\n\n      @defaultValue `1`\n     */\n    min_samples_leaf?: number\n\n    /**\n      The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample\\_weight is not provided.\n\n      @defaultValue `0`\n     */\n    min_weight_fraction_leaf?: number\n\n    /**\n      The number of features to consider when looking for the best split:\n\n      @defaultValue `'sqrt'`\n     */\n    max_features?: 'sqrt' | 'log2' | number | number\n\n    /**\n      Grow trees with `max\\_leaf\\_nodes` in best-first fashion. Best nodes are defined as relative reduction in impurity. If `undefined` then unlimited number of leaf nodes.\n     */\n    max_leaf_nodes?: number\n\n    /**\n      A node will be split if this split induces a decrease of the impurity greater than or equal to this value.\n\n      The weighted impurity decrease equation is the following:\n\n      @defaultValue `0`\n     */\n    min_impurity_decrease?: number\n\n    /**\n      Whether bootstrap samples are used when building trees. If `false`, the whole dataset is used to build each tree.\n\n      @defaultValue `false`\n     */\n    bootstrap?: boolean\n\n    /**\n      Whether to use out-of-bag samples to estimate the generalization score. Only available if bootstrap=`true`.\n\n      @defaultValue `false`\n     */\n    oob_score?: boolean\n\n    /**\n      The number of jobs to run in parallel. [`fit`](#sklearn.ensemble.ExtraTreesClassifier.fit \"sklearn.ensemble.ExtraTreesClassifier.fit\"), [`predict`](#sklearn.ensemble.ExtraTreesClassifier.predict \"sklearn.ensemble.ExtraTreesClassifier.predict\"), [`decision\\_path`](#sklearn.ensemble.ExtraTreesClassifier.decision_path \"sklearn.ensemble.ExtraTreesClassifier.decision_path\") and [`apply`](#sklearn.ensemble.ExtraTreesClassifier.apply \"sklearn.ensemble.ExtraTreesClassifier.apply\") are all parallelized over the trees. `undefined` means 1 unless in a [`joblib.parallel\\_backend`](https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend \"(in joblib v1.3.0.dev0)\") context. `\\-1` means using all processors. See [Glossary](../../glossary.html#term-n_jobs) for more details.\n     */\n    n_jobs?: number\n\n    /**\n      Controls 3 sources of randomness:\n     */\n    random_state?: number\n\n    /**\n      Controls the verbosity when fitting and predicting.\n\n      @defaultValue `0`\n     */\n    verbose?: number\n\n    /**\n      When set to `true`, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new forest. See [Glossary](../../glossary.html#term-warm_start) and [Fitting additional weak-learners](../ensemble.html#gradient-boosting-warm-start) for details.\n\n      @defaultValue `false`\n     */\n    warm_start?: boolean\n\n    /**\n      Weights associated with classes in the form `{class\\_label: weight}`. If not given, all classes are supposed to have weight one. For multi-output problems, a list of dicts can be provided in the same order as the columns of y.\n\n      Note that for multioutput (including multilabel) weights should be defined for each class of every column in its own dict. For example, for four-class multilabel classification weights should be \\[{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}\\] instead of \\[{1:1}, {2:5}, {3:1}, {4:1}\\].\n\n      The “balanced” mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as `n\\_samples / (n\\_classes \\* np.bincount(y))`\n\n      The “balanced\\_subsample” mode is the same as “balanced” except that weights are computed based on the bootstrap sample for every tree grown.\n\n      For multi-output, the weights of each column of y will be multiplied.\n\n      Note that these weights will be multiplied with sample\\_weight (passed through the fit method) if sample\\_weight is specified.\n     */\n    class_weight?: 'balanced' | 'balanced_subsample' | any\n\n    /**\n      Complexity parameter used for Minimal Cost-Complexity Pruning. The subtree with the largest cost complexity that is smaller than `ccp\\_alpha` will be chosen. By default, no pruning is performed. See [Minimal Cost-Complexity Pruning](../tree.html#minimal-cost-complexity-pruning) for details.\n\n      @defaultValue `0`\n     */\n    ccp_alpha?: any\n\n    /**\n      If bootstrap is `true`, the number of samples to draw from X to train each base estimator.\n     */\n    max_samples?: number\n  }) {\n    this.id = `ExtraTreesClassifier${crypto.randomUUID().split('-')[0]}`\n    this.opts = opts || {}\n  }\n\n  get py(): PythonBridge {\n    return this._py\n  }\n\n  set py(pythonBridge: PythonBridge) {\n    this._py = pythonBridge\n  }\n\n  /**\n    Initializes the underlying Python resources.\n\n    This instance is not usable until the `Promise` returned by `init()` resolves.\n   */\n  async init(py: PythonBridge): Promise<void> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This ExtraTreesClassifier instance has already been disposed'\n      )\n    }\n\n    if (this._isInitialized) {\n      return\n    }\n\n    if (!py) {\n      throw new Error(\n        'ExtraTreesClassifier.init requires a PythonBridge instance'\n      )\n    }\n\n    this._py = py\n\n    await this._py.ex`\nimport numpy as np\nfrom sklearn.ensemble import ExtraTreesClassifier\ntry: bridgeExtraTreesClassifier\nexcept NameError: bridgeExtraTreesClassifier = {}\n`\n\n    // set up constructor params\n    await this._py.ex`ctor_ExtraTreesClassifier = {'n_estimators': ${\n      this.opts['n_estimators'] ?? undefined\n    }, 'criterion': ${this.opts['criterion'] ?? undefined}, 'max_depth': ${\n      this.opts['max_depth'] ?? undefined\n    }, 'min_samples_split': ${\n      this.opts['min_samples_split'] ?? undefined\n    }, 'min_samples_leaf': ${\n      this.opts['min_samples_leaf'] ?? undefined\n    }, 'min_weight_fraction_leaf': ${\n      this.opts['min_weight_fraction_leaf'] ?? undefined\n    }, 'max_features': ${\n      this.opts['max_features'] ?? undefined\n    }, 'max_leaf_nodes': ${\n      this.opts['max_leaf_nodes'] ?? undefined\n    }, 'min_impurity_decrease': ${\n      this.opts['min_impurity_decrease'] ?? undefined\n    }, 'bootstrap': ${this.opts['bootstrap'] ?? undefined}, 'oob_score': ${\n      this.opts['oob_score'] ?? undefined\n    }, 'n_jobs': ${this.opts['n_jobs'] ?? undefined}, 'random_state': ${\n      this.opts['random_state'] ?? undefined\n    }, 'verbose': ${this.opts['verbose'] ?? undefined}, 'warm_start': ${\n      this.opts['warm_start'] ?? undefined\n    }, 'class_weight': ${\n      this.opts['class_weight'] ?? undefined\n    }, 'ccp_alpha': ${this.opts['ccp_alpha'] ?? undefined}, 'max_samples': ${\n      this.opts['max_samples'] ?? undefined\n    }}\n\nctor_ExtraTreesClassifier = {k: v for k, v in ctor_ExtraTreesClassifier.items() if v is not None}`\n\n    await this._py\n      .ex`bridgeExtraTreesClassifier[${this.id}] = ExtraTreesClassifier(**ctor_ExtraTreesClassifier)`\n\n    this._isInitialized = true\n  }\n\n  /**\n    Disposes of the underlying Python resources.\n\n    Once `dispose()` is called, the instance is no longer usable.\n   */\n  async dispose() {\n    if (this._isDisposed) {\n      return\n    }\n\n    if (!this._isInitialized) {\n      return\n    }\n\n    await this._py.ex`del bridgeExtraTreesClassifier[${this.id}]`\n\n    this._isDisposed = true\n  }\n\n  /**\n    Apply trees in the forest to X, return leaf indices.\n   */\n  async apply(opts: {\n    /**\n      The input samples. Internally, its dtype will be converted to `dtype=np.float32`. If a sparse matrix is provided, it will be converted into a sparse `csr\\_matrix`.\n     */\n    X?: ArrayLike | SparseMatrix[]\n  }): Promise<NDArray[]> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This ExtraTreesClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('ExtraTreesClassifier must call init() before apply()')\n    }\n\n    // set up method params\n    await this._py.ex`pms_ExtraTreesClassifier_apply = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None}\n\npms_ExtraTreesClassifier_apply = {k: v for k, v in pms_ExtraTreesClassifier_apply.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_ExtraTreesClassifier_apply = bridgeExtraTreesClassifier[${this.id}].apply(**pms_ExtraTreesClassifier_apply)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_ExtraTreesClassifier_apply.tolist() if hasattr(res_ExtraTreesClassifier_apply, 'tolist') else res_ExtraTreesClassifier_apply`\n  }\n\n  /**\n    Return the decision path in the forest.\n   */\n  async decision_path(opts: {\n    /**\n      The input samples. Internally, its dtype will be converted to `dtype=np.float32`. If a sparse matrix is provided, it will be converted into a sparse `csr\\_matrix`.\n     */\n    X?: ArrayLike | SparseMatrix[]\n  }): Promise<SparseMatrix[]> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This ExtraTreesClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'ExtraTreesClassifier must call init() before decision_path()'\n      )\n    }\n\n    // set up method params\n    await this._py.ex`pms_ExtraTreesClassifier_decision_path = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None}\n\npms_ExtraTreesClassifier_decision_path = {k: v for k, v in pms_ExtraTreesClassifier_decision_path.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_ExtraTreesClassifier_decision_path = bridgeExtraTreesClassifier[${this.id}].decision_path(**pms_ExtraTreesClassifier_decision_path)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_ExtraTreesClassifier_decision_path.tolist() if hasattr(res_ExtraTreesClassifier_decision_path, 'tolist') else res_ExtraTreesClassifier_decision_path`\n  }\n\n  /**\n    Build a forest of trees from the training set (X, y).\n   */\n  async fit(opts: {\n    /**\n      The training input samples. Internally, its dtype will be converted to `dtype=np.float32`. If a sparse matrix is provided, it will be converted into a sparse `csc\\_matrix`.\n     */\n    X?: ArrayLike | SparseMatrix[]\n\n    /**\n      The target values (class labels in classification, real numbers in regression).\n     */\n    y?: ArrayLike\n\n    /**\n      Sample weights. If `undefined`, then samples are equally weighted. Splits that would create child nodes with net zero or negative weight are ignored while searching for a split in each node. In the case of classification, splits are also ignored if they would result in any single class carrying a negative weight in either child node.\n     */\n    sample_weight?: ArrayLike\n  }): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This ExtraTreesClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('ExtraTreesClassifier must call init() before fit()')\n    }\n\n    // set up method params\n    await this._py.ex`pms_ExtraTreesClassifier_fit = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None, 'y': np.array(${\n      opts['y'] ?? undefined\n    }) if ${opts['y'] !== undefined} else None, 'sample_weight': np.array(${\n      opts['sample_weight'] ?? undefined\n    }) if ${opts['sample_weight'] !== undefined} else None}\n\npms_ExtraTreesClassifier_fit = {k: v for k, v in pms_ExtraTreesClassifier_fit.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_ExtraTreesClassifier_fit = bridgeExtraTreesClassifier[${this.id}].fit(**pms_ExtraTreesClassifier_fit)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_ExtraTreesClassifier_fit.tolist() if hasattr(res_ExtraTreesClassifier_fit, 'tolist') else res_ExtraTreesClassifier_fit`\n  }\n\n  /**\n    Predict class for X.\n\n    The predicted class of an input sample is a vote by the trees in the forest, weighted by their probability estimates. That is, the predicted class is the one with highest mean probability estimate across the trees.\n   */\n  async predict(opts: {\n    /**\n      The input samples. Internally, its dtype will be converted to `dtype=np.float32`. If a sparse matrix is provided, it will be converted into a sparse `csr\\_matrix`.\n     */\n    X?: ArrayLike | SparseMatrix[]\n  }): Promise<NDArray> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This ExtraTreesClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('ExtraTreesClassifier must call init() before predict()')\n    }\n\n    // set up method params\n    await this._py.ex`pms_ExtraTreesClassifier_predict = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None}\n\npms_ExtraTreesClassifier_predict = {k: v for k, v in pms_ExtraTreesClassifier_predict.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_ExtraTreesClassifier_predict = bridgeExtraTreesClassifier[${this.id}].predict(**pms_ExtraTreesClassifier_predict)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_ExtraTreesClassifier_predict.tolist() if hasattr(res_ExtraTreesClassifier_predict, 'tolist') else res_ExtraTreesClassifier_predict`\n  }\n\n  /**\n    Predict class log-probabilities for X.\n\n    The predicted class log-probabilities of an input sample is computed as the log of the mean predicted class probabilities of the trees in the forest.\n   */\n  async predict_log_proba(opts: {\n    /**\n      The input samples. Internally, its dtype will be converted to `dtype=np.float32`. If a sparse matrix is provided, it will be converted into a sparse `csr\\_matrix`.\n     */\n    X?: ArrayLike | SparseMatrix[]\n  }): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This ExtraTreesClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'ExtraTreesClassifier must call init() before predict_log_proba()'\n      )\n    }\n\n    // set up method params\n    await this._py\n      .ex`pms_ExtraTreesClassifier_predict_log_proba = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None}\n\npms_ExtraTreesClassifier_predict_log_proba = {k: v for k, v in pms_ExtraTreesClassifier_predict_log_proba.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_ExtraTreesClassifier_predict_log_proba = bridgeExtraTreesClassifier[${this.id}].predict_log_proba(**pms_ExtraTreesClassifier_predict_log_proba)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_ExtraTreesClassifier_predict_log_proba.tolist() if hasattr(res_ExtraTreesClassifier_predict_log_proba, 'tolist') else res_ExtraTreesClassifier_predict_log_proba`\n  }\n\n  /**\n    Predict class probabilities for X.\n\n    The predicted class probabilities of an input sample are computed as the mean predicted class probabilities of the trees in the forest. The class probability of a single tree is the fraction of samples of the same class in a leaf.\n   */\n  async predict_proba(opts: {\n    /**\n      The input samples. Internally, its dtype will be converted to `dtype=np.float32`. If a sparse matrix is provided, it will be converted into a sparse `csr\\_matrix`.\n     */\n    X?: ArrayLike | SparseMatrix[]\n  }): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This ExtraTreesClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'ExtraTreesClassifier must call init() before predict_proba()'\n      )\n    }\n\n    // set up method params\n    await this._py.ex`pms_ExtraTreesClassifier_predict_proba = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None}\n\npms_ExtraTreesClassifier_predict_proba = {k: v for k, v in pms_ExtraTreesClassifier_predict_proba.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_ExtraTreesClassifier_predict_proba = bridgeExtraTreesClassifier[${this.id}].predict_proba(**pms_ExtraTreesClassifier_predict_proba)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_ExtraTreesClassifier_predict_proba.tolist() if hasattr(res_ExtraTreesClassifier_predict_proba, 'tolist') else res_ExtraTreesClassifier_predict_proba`\n  }\n\n  /**\n    Return the mean accuracy on the given test data and labels.\n\n    In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted.\n   */\n  async score(opts: {\n    /**\n      Test samples.\n     */\n    X?: ArrayLike[]\n\n    /**\n      True labels for `X`.\n     */\n    y?: ArrayLike\n\n    /**\n      Sample weights.\n     */\n    sample_weight?: ArrayLike\n  }): Promise<number> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This ExtraTreesClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('ExtraTreesClassifier must call init() before score()')\n    }\n\n    // set up method params\n    await this._py.ex`pms_ExtraTreesClassifier_score = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None, 'y': np.array(${\n      opts['y'] ?? undefined\n    }) if ${opts['y'] !== undefined} else None, 'sample_weight': np.array(${\n      opts['sample_weight'] ?? undefined\n    }) if ${opts['sample_weight'] !== undefined} else None}\n\npms_ExtraTreesClassifier_score = {k: v for k, v in pms_ExtraTreesClassifier_score.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_ExtraTreesClassifier_score = bridgeExtraTreesClassifier[${this.id}].score(**pms_ExtraTreesClassifier_score)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_ExtraTreesClassifier_score.tolist() if hasattr(res_ExtraTreesClassifier_score, 'tolist') else res_ExtraTreesClassifier_score`\n  }\n\n  /**\n    The child estimator template used to create the collection of fitted sub-estimators.\n   */\n  get estimator_(): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This ExtraTreesClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'ExtraTreesClassifier must call init() before accessing estimator_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_ExtraTreesClassifier_estimator_ = bridgeExtraTreesClassifier[${this.id}].estimator_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_ExtraTreesClassifier_estimator_.tolist() if hasattr(attr_ExtraTreesClassifier_estimator_, 'tolist') else attr_ExtraTreesClassifier_estimator_`\n    })()\n  }\n\n  /**\n    The collection of fitted sub-estimators.\n   */\n  get estimators_(): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This ExtraTreesClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'ExtraTreesClassifier must call init() before accessing estimators_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_ExtraTreesClassifier_estimators_ = bridgeExtraTreesClassifier[${this.id}].estimators_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_ExtraTreesClassifier_estimators_.tolist() if hasattr(attr_ExtraTreesClassifier_estimators_, 'tolist') else attr_ExtraTreesClassifier_estimators_`\n    })()\n  }\n\n  /**\n    The classes labels (single output problem), or a list of arrays of class labels (multi-output problem).\n   */\n  get classes_(): Promise<NDArray> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This ExtraTreesClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'ExtraTreesClassifier must call init() before accessing classes_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_ExtraTreesClassifier_classes_ = bridgeExtraTreesClassifier[${this.id}].classes_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_ExtraTreesClassifier_classes_.tolist() if hasattr(attr_ExtraTreesClassifier_classes_, 'tolist') else attr_ExtraTreesClassifier_classes_`\n    })()\n  }\n\n  /**\n    The number of classes (single output problem), or a list containing the number of classes for each output (multi-output problem).\n   */\n  get n_classes_(): Promise<number | any[]> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This ExtraTreesClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'ExtraTreesClassifier must call init() before accessing n_classes_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_ExtraTreesClassifier_n_classes_ = bridgeExtraTreesClassifier[${this.id}].n_classes_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_ExtraTreesClassifier_n_classes_.tolist() if hasattr(attr_ExtraTreesClassifier_n_classes_, 'tolist') else attr_ExtraTreesClassifier_n_classes_`\n    })()\n  }\n\n  /**\n    Number of features seen during [fit](../../glossary.html#term-fit).\n   */\n  get n_features_in_(): Promise<number> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This ExtraTreesClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'ExtraTreesClassifier must call init() before accessing n_features_in_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_ExtraTreesClassifier_n_features_in_ = bridgeExtraTreesClassifier[${this.id}].n_features_in_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_ExtraTreesClassifier_n_features_in_.tolist() if hasattr(attr_ExtraTreesClassifier_n_features_in_, 'tolist') else attr_ExtraTreesClassifier_n_features_in_`\n    })()\n  }\n\n  /**\n    Names of features seen during [fit](../../glossary.html#term-fit). Defined only when `X` has feature names that are all strings.\n   */\n  get feature_names_in_(): Promise<NDArray> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This ExtraTreesClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'ExtraTreesClassifier must call init() before accessing feature_names_in_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_ExtraTreesClassifier_feature_names_in_ = bridgeExtraTreesClassifier[${this.id}].feature_names_in_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_ExtraTreesClassifier_feature_names_in_.tolist() if hasattr(attr_ExtraTreesClassifier_feature_names_in_, 'tolist') else attr_ExtraTreesClassifier_feature_names_in_`\n    })()\n  }\n\n  /**\n    The number of outputs when `fit` is performed.\n   */\n  get n_outputs_(): Promise<number> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This ExtraTreesClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'ExtraTreesClassifier must call init() before accessing n_outputs_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_ExtraTreesClassifier_n_outputs_ = bridgeExtraTreesClassifier[${this.id}].n_outputs_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_ExtraTreesClassifier_n_outputs_.tolist() if hasattr(attr_ExtraTreesClassifier_n_outputs_, 'tolist') else attr_ExtraTreesClassifier_n_outputs_`\n    })()\n  }\n\n  /**\n    Score of the training dataset obtained using an out-of-bag estimate. This attribute exists only when `oob\\_score` is `true`.\n   */\n  get oob_score_(): Promise<number> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This ExtraTreesClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'ExtraTreesClassifier must call init() before accessing oob_score_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_ExtraTreesClassifier_oob_score_ = bridgeExtraTreesClassifier[${this.id}].oob_score_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_ExtraTreesClassifier_oob_score_.tolist() if hasattr(attr_ExtraTreesClassifier_oob_score_, 'tolist') else attr_ExtraTreesClassifier_oob_score_`\n    })()\n  }\n\n  /**\n    Decision function computed with out-of-bag estimate on the training set. If n\\_estimators is small it might be possible that a data point was never left out during the bootstrap. In this case, `oob\\_decision\\_function\\_` might contain NaN. This attribute exists only when `oob\\_score` is `true`.\n   */\n  get oob_decision_function_(): Promise<NDArray[]> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This ExtraTreesClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'ExtraTreesClassifier must call init() before accessing oob_decision_function_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_ExtraTreesClassifier_oob_decision_function_ = bridgeExtraTreesClassifier[${this.id}].oob_decision_function_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_ExtraTreesClassifier_oob_decision_function_.tolist() if hasattr(attr_ExtraTreesClassifier_oob_decision_function_, 'tolist') else attr_ExtraTreesClassifier_oob_decision_function_`\n    })()\n  }\n}\n","/* eslint-disable */\n/* NOTE: This file is auto-generated. Do not edit it directly. */\n\nimport crypto from 'node:crypto'\n\nimport { PythonBridge, NDArray, ArrayLike, SparseMatrix } from '@/sklearn/types'\n\n/**\n  An extra-trees regressor.\n\n  This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.\n\n  Read more in the [User Guide](../ensemble.html#forest).\n\n  [Python Reference](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesRegressor.html)\n */\nexport class ExtraTreesRegressor {\n  id: string\n  opts: any\n\n  _py: PythonBridge\n  _isInitialized: boolean = false\n  _isDisposed: boolean = false\n\n  constructor(opts?: {\n    /**\n      The number of trees in the forest.\n\n      @defaultValue `100`\n     */\n    n_estimators?: number\n\n    /**\n      The function to measure the quality of a split. Supported criteria are “squared\\_error” for the mean squared error, which is equal to variance reduction as feature selection criterion and minimizes the L2 loss using the mean of each terminal node, “friedman\\_mse”, which uses mean squared error with Friedman’s improvement score for potential splits, “absolute\\_error” for the mean absolute error, which minimizes the L1 loss using the median of each terminal node, and “poisson” which uses reduction in Poisson deviance to find splits. Training using “absolute\\_error” is significantly slower than when using “squared\\_error”.\n\n      @defaultValue `'squared_error'`\n     */\n    criterion?: 'squared_error' | 'absolute_error' | 'friedman_mse' | 'poisson'\n\n    /**\n      The maximum depth of the tree. If `undefined`, then nodes are expanded until all leaves are pure or until all leaves contain less than min\\_samples\\_split samples.\n     */\n    max_depth?: number\n\n    /**\n      The minimum number of samples required to split an internal node:\n\n      @defaultValue `2`\n     */\n    min_samples_split?: number\n\n    /**\n      The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least `min\\_samples\\_leaf` training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.\n\n      @defaultValue `1`\n     */\n    min_samples_leaf?: number\n\n    /**\n      The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample\\_weight is not provided.\n\n      @defaultValue `0`\n     */\n    min_weight_fraction_leaf?: number\n\n    /**\n      The number of features to consider when looking for the best split:\n\n      @defaultValue `1`\n     */\n    max_features?: 'sqrt' | 'log2' | number | number\n\n    /**\n      Grow trees with `max\\_leaf\\_nodes` in best-first fashion. Best nodes are defined as relative reduction in impurity. If `undefined` then unlimited number of leaf nodes.\n     */\n    max_leaf_nodes?: number\n\n    /**\n      A node will be split if this split induces a decrease of the impurity greater than or equal to this value.\n\n      The weighted impurity decrease equation is the following:\n\n      @defaultValue `0`\n     */\n    min_impurity_decrease?: number\n\n    /**\n      Whether bootstrap samples are used when building trees. If `false`, the whole dataset is used to build each tree.\n\n      @defaultValue `false`\n     */\n    bootstrap?: boolean\n\n    /**\n      Whether to use out-of-bag samples to estimate the generalization score. Only available if bootstrap=`true`.\n\n      @defaultValue `false`\n     */\n    oob_score?: boolean\n\n    /**\n      The number of jobs to run in parallel. [`fit`](#sklearn.ensemble.ExtraTreesRegressor.fit \"sklearn.ensemble.ExtraTreesRegressor.fit\"), [`predict`](#sklearn.ensemble.ExtraTreesRegressor.predict \"sklearn.ensemble.ExtraTreesRegressor.predict\"), [`decision\\_path`](#sklearn.ensemble.ExtraTreesRegressor.decision_path \"sklearn.ensemble.ExtraTreesRegressor.decision_path\") and [`apply`](#sklearn.ensemble.ExtraTreesRegressor.apply \"sklearn.ensemble.ExtraTreesRegressor.apply\") are all parallelized over the trees. `undefined` means 1 unless in a [`joblib.parallel\\_backend`](https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend \"(in joblib v1.3.0.dev0)\") context. `\\-1` means using all processors. See [Glossary](../../glossary.html#term-n_jobs) for more details.\n     */\n    n_jobs?: number\n\n    /**\n      Controls 3 sources of randomness:\n     */\n    random_state?: number\n\n    /**\n      Controls the verbosity when fitting and predicting.\n\n      @defaultValue `0`\n     */\n    verbose?: number\n\n    /**\n      When set to `true`, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new forest. See [Glossary](../../glossary.html#term-warm_start) and [Fitting additional weak-learners](../ensemble.html#gradient-boosting-warm-start) for details.\n\n      @defaultValue `false`\n     */\n    warm_start?: boolean\n\n    /**\n      Complexity parameter used for Minimal Cost-Complexity Pruning. The subtree with the largest cost complexity that is smaller than `ccp\\_alpha` will be chosen. By default, no pruning is performed. See [Minimal Cost-Complexity Pruning](../tree.html#minimal-cost-complexity-pruning) for details.\n\n      @defaultValue `0`\n     */\n    ccp_alpha?: any\n\n    /**\n      If bootstrap is `true`, the number of samples to draw from X to train each base estimator.\n     */\n    max_samples?: number\n  }) {\n    this.id = `ExtraTreesRegressor${crypto.randomUUID().split('-')[0]}`\n    this.opts = opts || {}\n  }\n\n  get py(): PythonBridge {\n    return this._py\n  }\n\n  set py(pythonBridge: PythonBridge) {\n    this._py = pythonBridge\n  }\n\n  /**\n    Initializes the underlying Python resources.\n\n    This instance is not usable until the `Promise` returned by `init()` resolves.\n   */\n  async init(py: PythonBridge): Promise<void> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This ExtraTreesRegressor instance has already been disposed'\n      )\n    }\n\n    if (this._isInitialized) {\n      return\n    }\n\n    if (!py) {\n      throw new Error(\n        'ExtraTreesRegressor.init requires a PythonBridge instance'\n      )\n    }\n\n    this._py = py\n\n    await this._py.ex`\nimport numpy as np\nfrom sklearn.ensemble import ExtraTreesRegressor\ntry: bridgeExtraTreesRegressor\nexcept NameError: bridgeExtraTreesRegressor = {}\n`\n\n    // set up constructor params\n    await this._py.ex`ctor_ExtraTreesRegressor = {'n_estimators': ${\n      this.opts['n_estimators'] ?? undefined\n    }, 'criterion': ${this.opts['criterion'] ?? undefined}, 'max_depth': ${\n      this.opts['max_depth'] ?? undefined\n    }, 'min_samples_split': ${\n      this.opts['min_samples_split'] ?? undefined\n    }, 'min_samples_leaf': ${\n      this.opts['min_samples_leaf'] ?? undefined\n    }, 'min_weight_fraction_leaf': ${\n      this.opts['min_weight_fraction_leaf'] ?? undefined\n    }, 'max_features': ${\n      this.opts['max_features'] ?? undefined\n    }, 'max_leaf_nodes': ${\n      this.opts['max_leaf_nodes'] ?? undefined\n    }, 'min_impurity_decrease': ${\n      this.opts['min_impurity_decrease'] ?? undefined\n    }, 'bootstrap': ${this.opts['bootstrap'] ?? undefined}, 'oob_score': ${\n      this.opts['oob_score'] ?? undefined\n    }, 'n_jobs': ${this.opts['n_jobs'] ?? undefined}, 'random_state': ${\n      this.opts['random_state'] ?? undefined\n    }, 'verbose': ${this.opts['verbose'] ?? undefined}, 'warm_start': ${\n      this.opts['warm_start'] ?? undefined\n    }, 'ccp_alpha': ${this.opts['ccp_alpha'] ?? undefined}, 'max_samples': ${\n      this.opts['max_samples'] ?? undefined\n    }}\n\nctor_ExtraTreesRegressor = {k: v for k, v in ctor_ExtraTreesRegressor.items() if v is not None}`\n\n    await this._py\n      .ex`bridgeExtraTreesRegressor[${this.id}] = ExtraTreesRegressor(**ctor_ExtraTreesRegressor)`\n\n    this._isInitialized = true\n  }\n\n  /**\n    Disposes of the underlying Python resources.\n\n    Once `dispose()` is called, the instance is no longer usable.\n   */\n  async dispose() {\n    if (this._isDisposed) {\n      return\n    }\n\n    if (!this._isInitialized) {\n      return\n    }\n\n    await this._py.ex`del bridgeExtraTreesRegressor[${this.id}]`\n\n    this._isDisposed = true\n  }\n\n  /**\n    Apply trees in the forest to X, return leaf indices.\n   */\n  async apply(opts: {\n    /**\n      The input samples. Internally, its dtype will be converted to `dtype=np.float32`. If a sparse matrix is provided, it will be converted into a sparse `csr\\_matrix`.\n     */\n    X?: ArrayLike | SparseMatrix[]\n  }): Promise<NDArray[]> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This ExtraTreesRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('ExtraTreesRegressor must call init() before apply()')\n    }\n\n    // set up method params\n    await this._py.ex`pms_ExtraTreesRegressor_apply = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None}\n\npms_ExtraTreesRegressor_apply = {k: v for k, v in pms_ExtraTreesRegressor_apply.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_ExtraTreesRegressor_apply = bridgeExtraTreesRegressor[${this.id}].apply(**pms_ExtraTreesRegressor_apply)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_ExtraTreesRegressor_apply.tolist() if hasattr(res_ExtraTreesRegressor_apply, 'tolist') else res_ExtraTreesRegressor_apply`\n  }\n\n  /**\n    Return the decision path in the forest.\n   */\n  async decision_path(opts: {\n    /**\n      The input samples. Internally, its dtype will be converted to `dtype=np.float32`. If a sparse matrix is provided, it will be converted into a sparse `csr\\_matrix`.\n     */\n    X?: ArrayLike | SparseMatrix[]\n  }): Promise<SparseMatrix[]> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This ExtraTreesRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'ExtraTreesRegressor must call init() before decision_path()'\n      )\n    }\n\n    // set up method params\n    await this._py.ex`pms_ExtraTreesRegressor_decision_path = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None}\n\npms_ExtraTreesRegressor_decision_path = {k: v for k, v in pms_ExtraTreesRegressor_decision_path.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_ExtraTreesRegressor_decision_path = bridgeExtraTreesRegressor[${this.id}].decision_path(**pms_ExtraTreesRegressor_decision_path)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_ExtraTreesRegressor_decision_path.tolist() if hasattr(res_ExtraTreesRegressor_decision_path, 'tolist') else res_ExtraTreesRegressor_decision_path`\n  }\n\n  /**\n    Build a forest of trees from the training set (X, y).\n   */\n  async fit(opts: {\n    /**\n      The training input samples. Internally, its dtype will be converted to `dtype=np.float32`. If a sparse matrix is provided, it will be converted into a sparse `csc\\_matrix`.\n     */\n    X?: ArrayLike | SparseMatrix[]\n\n    /**\n      The target values (class labels in classification, real numbers in regression).\n     */\n    y?: ArrayLike\n\n    /**\n      Sample weights. If `undefined`, then samples are equally weighted. Splits that would create child nodes with net zero or negative weight are ignored while searching for a split in each node. In the case of classification, splits are also ignored if they would result in any single class carrying a negative weight in either child node.\n     */\n    sample_weight?: ArrayLike\n  }): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This ExtraTreesRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('ExtraTreesRegressor must call init() before fit()')\n    }\n\n    // set up method params\n    await this._py.ex`pms_ExtraTreesRegressor_fit = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None, 'y': np.array(${\n      opts['y'] ?? undefined\n    }) if ${opts['y'] !== undefined} else None, 'sample_weight': np.array(${\n      opts['sample_weight'] ?? undefined\n    }) if ${opts['sample_weight'] !== undefined} else None}\n\npms_ExtraTreesRegressor_fit = {k: v for k, v in pms_ExtraTreesRegressor_fit.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_ExtraTreesRegressor_fit = bridgeExtraTreesRegressor[${this.id}].fit(**pms_ExtraTreesRegressor_fit)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_ExtraTreesRegressor_fit.tolist() if hasattr(res_ExtraTreesRegressor_fit, 'tolist') else res_ExtraTreesRegressor_fit`\n  }\n\n  /**\n    Predict regression target for X.\n\n    The predicted regression target of an input sample is computed as the mean predicted regression targets of the trees in the forest.\n   */\n  async predict(opts: {\n    /**\n      The input samples. Internally, its dtype will be converted to `dtype=np.float32`. If a sparse matrix is provided, it will be converted into a sparse `csr\\_matrix`.\n     */\n    X?: ArrayLike | SparseMatrix[]\n  }): Promise<NDArray> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This ExtraTreesRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('ExtraTreesRegressor must call init() before predict()')\n    }\n\n    // set up method params\n    await this._py.ex`pms_ExtraTreesRegressor_predict = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None}\n\npms_ExtraTreesRegressor_predict = {k: v for k, v in pms_ExtraTreesRegressor_predict.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_ExtraTreesRegressor_predict = bridgeExtraTreesRegressor[${this.id}].predict(**pms_ExtraTreesRegressor_predict)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_ExtraTreesRegressor_predict.tolist() if hasattr(res_ExtraTreesRegressor_predict, 'tolist') else res_ExtraTreesRegressor_predict`\n  }\n\n  /**\n    Return the coefficient of determination of the prediction.\n\n    The coefficient of determination \\\\(R^2\\\\) is defined as \\\\((1 - \\\\frac{u}{v})\\\\), where \\\\(u\\\\) is the residual sum of squares `((y\\_true \\- y\\_pred)\\*\\* 2).sum()` and \\\\(v\\\\) is the total sum of squares `((y\\_true \\- y\\_true.mean()) \\*\\* 2).sum()`. The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of `y`, disregarding the input features, would get a \\\\(R^2\\\\) score of 0.0.\n   */\n  async score(opts: {\n    /**\n      Test samples. For some estimators this may be a precomputed kernel matrix or a list of generic objects instead with shape `(n\\_samples, n\\_samples\\_fitted)`, where `n\\_samples\\_fitted` is the number of samples used in the fitting for the estimator.\n     */\n    X?: ArrayLike[]\n\n    /**\n      True values for `X`.\n     */\n    y?: ArrayLike\n\n    /**\n      Sample weights.\n     */\n    sample_weight?: ArrayLike\n  }): Promise<number> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This ExtraTreesRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('ExtraTreesRegressor must call init() before score()')\n    }\n\n    // set up method params\n    await this._py.ex`pms_ExtraTreesRegressor_score = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None, 'y': np.array(${\n      opts['y'] ?? undefined\n    }) if ${opts['y'] !== undefined} else None, 'sample_weight': np.array(${\n      opts['sample_weight'] ?? undefined\n    }) if ${opts['sample_weight'] !== undefined} else None}\n\npms_ExtraTreesRegressor_score = {k: v for k, v in pms_ExtraTreesRegressor_score.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_ExtraTreesRegressor_score = bridgeExtraTreesRegressor[${this.id}].score(**pms_ExtraTreesRegressor_score)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_ExtraTreesRegressor_score.tolist() if hasattr(res_ExtraTreesRegressor_score, 'tolist') else res_ExtraTreesRegressor_score`\n  }\n\n  /**\n    The child estimator template used to create the collection of fitted sub-estimators.\n   */\n  get estimator_(): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This ExtraTreesRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'ExtraTreesRegressor must call init() before accessing estimator_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_ExtraTreesRegressor_estimator_ = bridgeExtraTreesRegressor[${this.id}].estimator_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_ExtraTreesRegressor_estimator_.tolist() if hasattr(attr_ExtraTreesRegressor_estimator_, 'tolist') else attr_ExtraTreesRegressor_estimator_`\n    })()\n  }\n\n  /**\n    The collection of fitted sub-estimators.\n   */\n  get estimators_(): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This ExtraTreesRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'ExtraTreesRegressor must call init() before accessing estimators_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_ExtraTreesRegressor_estimators_ = bridgeExtraTreesRegressor[${this.id}].estimators_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_ExtraTreesRegressor_estimators_.tolist() if hasattr(attr_ExtraTreesRegressor_estimators_, 'tolist') else attr_ExtraTreesRegressor_estimators_`\n    })()\n  }\n\n  /**\n    Number of features seen during [fit](../../glossary.html#term-fit).\n   */\n  get n_features_in_(): Promise<number> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This ExtraTreesRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'ExtraTreesRegressor must call init() before accessing n_features_in_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_ExtraTreesRegressor_n_features_in_ = bridgeExtraTreesRegressor[${this.id}].n_features_in_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_ExtraTreesRegressor_n_features_in_.tolist() if hasattr(attr_ExtraTreesRegressor_n_features_in_, 'tolist') else attr_ExtraTreesRegressor_n_features_in_`\n    })()\n  }\n\n  /**\n    Names of features seen during [fit](../../glossary.html#term-fit). Defined only when `X` has feature names that are all strings.\n   */\n  get feature_names_in_(): Promise<NDArray> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This ExtraTreesRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'ExtraTreesRegressor must call init() before accessing feature_names_in_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_ExtraTreesRegressor_feature_names_in_ = bridgeExtraTreesRegressor[${this.id}].feature_names_in_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_ExtraTreesRegressor_feature_names_in_.tolist() if hasattr(attr_ExtraTreesRegressor_feature_names_in_, 'tolist') else attr_ExtraTreesRegressor_feature_names_in_`\n    })()\n  }\n\n  /**\n    The number of outputs.\n   */\n  get n_outputs_(): Promise<number> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This ExtraTreesRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'ExtraTreesRegressor must call init() before accessing n_outputs_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_ExtraTreesRegressor_n_outputs_ = bridgeExtraTreesRegressor[${this.id}].n_outputs_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_ExtraTreesRegressor_n_outputs_.tolist() if hasattr(attr_ExtraTreesRegressor_n_outputs_, 'tolist') else attr_ExtraTreesRegressor_n_outputs_`\n    })()\n  }\n\n  /**\n    Score of the training dataset obtained using an out-of-bag estimate. This attribute exists only when `oob\\_score` is `true`.\n   */\n  get oob_score_(): Promise<number> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This ExtraTreesRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'ExtraTreesRegressor must call init() before accessing oob_score_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_ExtraTreesRegressor_oob_score_ = bridgeExtraTreesRegressor[${this.id}].oob_score_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_ExtraTreesRegressor_oob_score_.tolist() if hasattr(attr_ExtraTreesRegressor_oob_score_, 'tolist') else attr_ExtraTreesRegressor_oob_score_`\n    })()\n  }\n\n  /**\n    Prediction computed with out-of-bag estimate on the training set. This attribute exists only when `oob\\_score` is `true`.\n   */\n  get oob_prediction_(): Promise<NDArray> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This ExtraTreesRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'ExtraTreesRegressor must call init() before accessing oob_prediction_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_ExtraTreesRegressor_oob_prediction_ = bridgeExtraTreesRegressor[${this.id}].oob_prediction_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_ExtraTreesRegressor_oob_prediction_.tolist() if hasattr(attr_ExtraTreesRegressor_oob_prediction_, 'tolist') else attr_ExtraTreesRegressor_oob_prediction_`\n    })()\n  }\n}\n","/* eslint-disable */\n/* NOTE: This file is auto-generated. Do not edit it directly. */\n\nimport crypto from 'node:crypto'\n\nimport { PythonBridge, NDArray, ArrayLike, SparseMatrix } from '@/sklearn/types'\n\n/**\n  Gradient Boosting for classification.\n\n  This algorithm builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage `n\\_classes\\_` regression trees are fit on the negative gradient of the loss function, e.g. binary or multiclass log loss. Binary classification is a special case where only a single regression tree is induced.\n\n  [`sklearn.ensemble.HistGradientBoostingClassifier`](sklearn.ensemble.HistGradientBoostingClassifier.html#sklearn.ensemble.HistGradientBoostingClassifier \"sklearn.ensemble.HistGradientBoostingClassifier\") is a much faster variant of this algorithm for intermediate datasets (`n\\_samples >= 10\\_000`).\n\n  Read more in the [User Guide](../ensemble.html#gradient-boosting).\n\n  [Python Reference](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)\n */\nexport class GradientBoostingClassifier {\n  id: string\n  opts: any\n\n  _py: PythonBridge\n  _isInitialized: boolean = false\n  _isDisposed: boolean = false\n\n  constructor(opts?: {\n    /**\n      The loss function to be optimized. ‘log\\_loss’ refers to binomial and multinomial deviance, the same as used in logistic regression. It is a good choice for classification with probabilistic outputs. For loss ‘exponential’, gradient boosting recovers the AdaBoost algorithm.\n\n      @defaultValue `'log_loss'`\n     */\n    loss?: 'log_loss' | 'deviance' | 'exponential'\n\n    /**\n      Learning rate shrinks the contribution of each tree by `learning\\_rate`. There is a trade-off between learning\\_rate and n\\_estimators. Values must be in the range `\\[0.0, inf)`.\n\n      @defaultValue `0.1`\n     */\n    learning_rate?: number\n\n    /**\n      The number of boosting stages to perform. Gradient boosting is fairly robust to over-fitting so a large number usually results in better performance. Values must be in the range `\\[1, inf)`.\n\n      @defaultValue `100`\n     */\n    n_estimators?: number\n\n    /**\n      The fraction of samples to be used for fitting the individual base learners. If smaller than 1.0 this results in Stochastic Gradient Boosting. `subsample` interacts with the parameter `n\\_estimators`. Choosing `subsample < 1.0` leads to a reduction of variance and an increase in bias. Values must be in the range `(0.0, 1.0\\]`.\n\n      @defaultValue `1`\n     */\n    subsample?: number\n\n    /**\n      The function to measure the quality of a split. Supported criteria are ‘friedman\\_mse’ for the mean squared error with improvement score by Friedman, ‘squared\\_error’ for mean squared error. The default value of ‘friedman\\_mse’ is generally the best as it can provide a better approximation in some cases.\n\n      @defaultValue `'friedman_mse'`\n     */\n    criterion?: 'friedman_mse' | 'squared_error'\n\n    /**\n      The minimum number of samples required to split an internal node:\n\n      @defaultValue `2`\n     */\n    min_samples_split?: number\n\n    /**\n      The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least `min\\_samples\\_leaf` training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.\n\n      @defaultValue `1`\n     */\n    min_samples_leaf?: number\n\n    /**\n      The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample\\_weight is not provided. Values must be in the range `\\[0.0, 0.5\\]`.\n\n      @defaultValue `0`\n     */\n    min_weight_fraction_leaf?: number\n\n    /**\n      Maximum depth of the individual regression estimators. The maximum depth limits the number of nodes in the tree. Tune this parameter for best performance; the best value depends on the interaction of the input variables. If `undefined`, then nodes are expanded until all leaves are pure or until all leaves contain less than min\\_samples\\_split samples. If int, values must be in the range `\\[1, inf)`.\n\n      @defaultValue `3`\n     */\n    max_depth?: number\n\n    /**\n      A node will be split if this split induces a decrease of the impurity greater than or equal to this value. Values must be in the range `\\[0.0, inf)`.\n\n      The weighted impurity decrease equation is the following:\n\n      @defaultValue `0`\n     */\n    min_impurity_decrease?: number\n\n    /**\n      An estimator object that is used to compute the initial predictions. `init` has to provide [`fit`](#sklearn.ensemble.GradientBoostingClassifier.fit \"sklearn.ensemble.GradientBoostingClassifier.fit\") and [`predict\\_proba`](#sklearn.ensemble.GradientBoostingClassifier.predict_proba \"sklearn.ensemble.GradientBoostingClassifier.predict_proba\"). If ‘zero’, the initial raw predictions are set to zero. By default, a `DummyEstimator` predicting the classes priors is used.\n     */\n    init?: 'zero'\n\n    /**\n      Controls the random seed given to each Tree estimator at each boosting iteration. In addition, it controls the random permutation of the features at each split (see Notes for more details). It also controls the random splitting of the training data to obtain a validation set if `n\\_iter\\_no\\_change` is not `undefined`. Pass an int for reproducible output across multiple function calls. See [Glossary](../../glossary.html#term-random_state).\n     */\n    random_state?: number\n\n    /**\n      The number of features to consider when looking for the best split:\n     */\n    max_features?: 'auto' | 'sqrt' | 'log2' | number | number\n\n    /**\n      Enable verbose output. If 1 then it prints progress and performance once in a while (the more trees the lower the frequency). If greater than 1 then it prints progress and performance for every tree. Values must be in the range `\\[0, inf)`.\n\n      @defaultValue `0`\n     */\n    verbose?: number\n\n    /**\n      Grow trees with `max\\_leaf\\_nodes` in best-first fashion. Best nodes are defined as relative reduction in impurity. Values must be in the range `\\[2, inf)`. If `undefined`, then unlimited number of leaf nodes.\n     */\n    max_leaf_nodes?: number\n\n    /**\n      When set to `true`, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just erase the previous solution. See [the Glossary](../../glossary.html#term-warm_start).\n\n      @defaultValue `false`\n     */\n    warm_start?: boolean\n\n    /**\n      The proportion of training data to set aside as validation set for early stopping. Values must be in the range `(0.0, 1.0)`. Only used if `n\\_iter\\_no\\_change` is set to an integer.\n\n      @defaultValue `0.1`\n     */\n    validation_fraction?: number\n\n    /**\n      `n\\_iter\\_no\\_change` is used to decide if early stopping will be used to terminate training when validation score is not improving. By default it is set to `undefined` to disable early stopping. If set to a number, it will set aside `validation\\_fraction` size of the training data as validation and terminate training when validation score is not improving in all of the previous `n\\_iter\\_no\\_change` numbers of iterations. The split is stratified. Values must be in the range `\\[1, inf)`.\n     */\n    n_iter_no_change?: number\n\n    /**\n      Tolerance for the early stopping. When the loss is not improving by at least tol for `n\\_iter\\_no\\_change` iterations (if set to a number), the training stops. Values must be in the range `\\[0.0, inf)`.\n\n      @defaultValue `0.0001`\n     */\n    tol?: number\n\n    /**\n      Complexity parameter used for Minimal Cost-Complexity Pruning. The subtree with the largest cost complexity that is smaller than `ccp\\_alpha` will be chosen. By default, no pruning is performed. Values must be in the range `\\[0.0, inf)`. See [Minimal Cost-Complexity Pruning](../tree.html#minimal-cost-complexity-pruning) for details.\n\n      @defaultValue `0`\n     */\n    ccp_alpha?: any\n  }) {\n    this.id = `GradientBoostingClassifier${crypto.randomUUID().split('-')[0]}`\n    this.opts = opts || {}\n  }\n\n  get py(): PythonBridge {\n    return this._py\n  }\n\n  set py(pythonBridge: PythonBridge) {\n    this._py = pythonBridge\n  }\n\n  /**\n    Initializes the underlying Python resources.\n\n    This instance is not usable until the `Promise` returned by `init()` resolves.\n   */\n  async init(py: PythonBridge): Promise<void> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This GradientBoostingClassifier instance has already been disposed'\n      )\n    }\n\n    if (this._isInitialized) {\n      return\n    }\n\n    if (!py) {\n      throw new Error(\n        'GradientBoostingClassifier.init requires a PythonBridge instance'\n      )\n    }\n\n    this._py = py\n\n    await this._py.ex`\nimport numpy as np\nfrom sklearn.ensemble import GradientBoostingClassifier\ntry: bridgeGradientBoostingClassifier\nexcept NameError: bridgeGradientBoostingClassifier = {}\n`\n\n    // set up constructor params\n    await this._py.ex`ctor_GradientBoostingClassifier = {'loss': ${\n      this.opts['loss'] ?? undefined\n    }, 'learning_rate': ${\n      this.opts['learning_rate'] ?? undefined\n    }, 'n_estimators': ${\n      this.opts['n_estimators'] ?? undefined\n    }, 'subsample': ${this.opts['subsample'] ?? undefined}, 'criterion': ${\n      this.opts['criterion'] ?? undefined\n    }, 'min_samples_split': ${\n      this.opts['min_samples_split'] ?? undefined\n    }, 'min_samples_leaf': ${\n      this.opts['min_samples_leaf'] ?? undefined\n    }, 'min_weight_fraction_leaf': ${\n      this.opts['min_weight_fraction_leaf'] ?? undefined\n    }, 'max_depth': ${\n      this.opts['max_depth'] ?? undefined\n    }, 'min_impurity_decrease': ${\n      this.opts['min_impurity_decrease'] ?? undefined\n    }, 'init': ${this.opts['init'] ?? undefined}, 'random_state': ${\n      this.opts['random_state'] ?? undefined\n    }, 'max_features': ${this.opts['max_features'] ?? undefined}, 'verbose': ${\n      this.opts['verbose'] ?? undefined\n    }, 'max_leaf_nodes': ${\n      this.opts['max_leaf_nodes'] ?? undefined\n    }, 'warm_start': ${\n      this.opts['warm_start'] ?? undefined\n    }, 'validation_fraction': ${\n      this.opts['validation_fraction'] ?? undefined\n    }, 'n_iter_no_change': ${\n      this.opts['n_iter_no_change'] ?? undefined\n    }, 'tol': ${this.opts['tol'] ?? undefined}, 'ccp_alpha': ${\n      this.opts['ccp_alpha'] ?? undefined\n    }}\n\nctor_GradientBoostingClassifier = {k: v for k, v in ctor_GradientBoostingClassifier.items() if v is not None}`\n\n    await this._py\n      .ex`bridgeGradientBoostingClassifier[${this.id}] = GradientBoostingClassifier(**ctor_GradientBoostingClassifier)`\n\n    this._isInitialized = true\n  }\n\n  /**\n    Disposes of the underlying Python resources.\n\n    Once `dispose()` is called, the instance is no longer usable.\n   */\n  async dispose() {\n    if (this._isDisposed) {\n      return\n    }\n\n    if (!this._isInitialized) {\n      return\n    }\n\n    await this._py.ex`del bridgeGradientBoostingClassifier[${this.id}]`\n\n    this._isDisposed = true\n  }\n\n  /**\n    Apply trees in the ensemble to X, return leaf indices.\n   */\n  async apply(opts: {\n    /**\n      The input samples. Internally, its dtype will be converted to `dtype=np.float32`. If a sparse matrix is provided, it will be converted to a sparse `csr\\_matrix`.\n     */\n    X?: ArrayLike | SparseMatrix[]\n  }): Promise<ArrayLike[][]> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This GradientBoostingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'GradientBoostingClassifier must call init() before apply()'\n      )\n    }\n\n    // set up method params\n    await this._py.ex`pms_GradientBoostingClassifier_apply = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None}\n\npms_GradientBoostingClassifier_apply = {k: v for k, v in pms_GradientBoostingClassifier_apply.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_GradientBoostingClassifier_apply = bridgeGradientBoostingClassifier[${this.id}].apply(**pms_GradientBoostingClassifier_apply)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_GradientBoostingClassifier_apply.tolist() if hasattr(res_GradientBoostingClassifier_apply, 'tolist') else res_GradientBoostingClassifier_apply`\n  }\n\n  /**\n    Compute the decision function of `X`.\n   */\n  async decision_function(opts: {\n    /**\n      The input samples. Internally, it will be converted to `dtype=np.float32` and if a sparse matrix is provided to a sparse `csr\\_matrix`.\n     */\n    X?: ArrayLike | SparseMatrix[]\n  }): Promise<NDArray[]> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This GradientBoostingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'GradientBoostingClassifier must call init() before decision_function()'\n      )\n    }\n\n    // set up method params\n    await this._py\n      .ex`pms_GradientBoostingClassifier_decision_function = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None}\n\npms_GradientBoostingClassifier_decision_function = {k: v for k, v in pms_GradientBoostingClassifier_decision_function.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_GradientBoostingClassifier_decision_function = bridgeGradientBoostingClassifier[${this.id}].decision_function(**pms_GradientBoostingClassifier_decision_function)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_GradientBoostingClassifier_decision_function.tolist() if hasattr(res_GradientBoostingClassifier_decision_function, 'tolist') else res_GradientBoostingClassifier_decision_function`\n  }\n\n  /**\n    Fit the gradient boosting model.\n   */\n  async fit(opts: {\n    /**\n      The input samples. Internally, it will be converted to `dtype=np.float32` and if a sparse matrix is provided to a sparse `csr\\_matrix`.\n     */\n    X?: ArrayLike | SparseMatrix[]\n\n    /**\n      Target values (strings or integers in classification, real numbers in regression) For classification, labels must correspond to classes.\n     */\n    y?: ArrayLike\n\n    /**\n      Sample weights. If `undefined`, then samples are equally weighted. Splits that would create child nodes with net zero or negative weight are ignored while searching for a split in each node. In the case of classification, splits are also ignored if they would result in any single class carrying a negative weight in either child node.\n     */\n    sample_weight?: ArrayLike\n\n    /**\n      The monitor is called after each iteration with the current iteration, a reference to the estimator and the local variables of `\\_fit\\_stages` as keyword arguments `callable(i, self, locals())`. If the callable returns `true` the fitting procedure is stopped. The monitor can be used for various things such as computing held-out estimates, early stopping, model introspect, and snapshoting.\n     */\n    monitor?: any\n  }): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This GradientBoostingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'GradientBoostingClassifier must call init() before fit()'\n      )\n    }\n\n    // set up method params\n    await this._py.ex`pms_GradientBoostingClassifier_fit = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None, 'y': np.array(${\n      opts['y'] ?? undefined\n    }) if ${opts['y'] !== undefined} else None, 'sample_weight': np.array(${\n      opts['sample_weight'] ?? undefined\n    }) if ${opts['sample_weight'] !== undefined} else None, 'monitor': ${\n      opts['monitor'] ?? undefined\n    }}\n\npms_GradientBoostingClassifier_fit = {k: v for k, v in pms_GradientBoostingClassifier_fit.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_GradientBoostingClassifier_fit = bridgeGradientBoostingClassifier[${this.id}].fit(**pms_GradientBoostingClassifier_fit)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_GradientBoostingClassifier_fit.tolist() if hasattr(res_GradientBoostingClassifier_fit, 'tolist') else res_GradientBoostingClassifier_fit`\n  }\n\n  /**\n    Predict class for X.\n   */\n  async predict(opts: {\n    /**\n      The input samples. Internally, it will be converted to `dtype=np.float32` and if a sparse matrix is provided to a sparse `csr\\_matrix`.\n     */\n    X?: ArrayLike | SparseMatrix[]\n  }): Promise<NDArray> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This GradientBoostingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'GradientBoostingClassifier must call init() before predict()'\n      )\n    }\n\n    // set up method params\n    await this._py.ex`pms_GradientBoostingClassifier_predict = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None}\n\npms_GradientBoostingClassifier_predict = {k: v for k, v in pms_GradientBoostingClassifier_predict.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_GradientBoostingClassifier_predict = bridgeGradientBoostingClassifier[${this.id}].predict(**pms_GradientBoostingClassifier_predict)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_GradientBoostingClassifier_predict.tolist() if hasattr(res_GradientBoostingClassifier_predict, 'tolist') else res_GradientBoostingClassifier_predict`\n  }\n\n  /**\n    Predict class log-probabilities for X.\n   */\n  async predict_log_proba(opts: {\n    /**\n      The input samples. Internally, it will be converted to `dtype=np.float32` and if a sparse matrix is provided to a sparse `csr\\_matrix`.\n     */\n    X?: ArrayLike | SparseMatrix[]\n  }): Promise<NDArray[]> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This GradientBoostingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'GradientBoostingClassifier must call init() before predict_log_proba()'\n      )\n    }\n\n    // set up method params\n    await this._py\n      .ex`pms_GradientBoostingClassifier_predict_log_proba = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None}\n\npms_GradientBoostingClassifier_predict_log_proba = {k: v for k, v in pms_GradientBoostingClassifier_predict_log_proba.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_GradientBoostingClassifier_predict_log_proba = bridgeGradientBoostingClassifier[${this.id}].predict_log_proba(**pms_GradientBoostingClassifier_predict_log_proba)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_GradientBoostingClassifier_predict_log_proba.tolist() if hasattr(res_GradientBoostingClassifier_predict_log_proba, 'tolist') else res_GradientBoostingClassifier_predict_log_proba`\n  }\n\n  /**\n    Predict class probabilities for X.\n   */\n  async predict_proba(opts: {\n    /**\n      The input samples. Internally, it will be converted to `dtype=np.float32` and if a sparse matrix is provided to a sparse `csr\\_matrix`.\n     */\n    X?: ArrayLike | SparseMatrix[]\n  }): Promise<NDArray[]> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This GradientBoostingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'GradientBoostingClassifier must call init() before predict_proba()'\n      )\n    }\n\n    // set up method params\n    await this._py\n      .ex`pms_GradientBoostingClassifier_predict_proba = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None}\n\npms_GradientBoostingClassifier_predict_proba = {k: v for k, v in pms_GradientBoostingClassifier_predict_proba.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_GradientBoostingClassifier_predict_proba = bridgeGradientBoostingClassifier[${this.id}].predict_proba(**pms_GradientBoostingClassifier_predict_proba)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_GradientBoostingClassifier_predict_proba.tolist() if hasattr(res_GradientBoostingClassifier_predict_proba, 'tolist') else res_GradientBoostingClassifier_predict_proba`\n  }\n\n  /**\n    Return the mean accuracy on the given test data and labels.\n\n    In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted.\n   */\n  async score(opts: {\n    /**\n      Test samples.\n     */\n    X?: ArrayLike[]\n\n    /**\n      True labels for `X`.\n     */\n    y?: ArrayLike\n\n    /**\n      Sample weights.\n     */\n    sample_weight?: ArrayLike\n  }): Promise<number> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This GradientBoostingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'GradientBoostingClassifier must call init() before score()'\n      )\n    }\n\n    // set up method params\n    await this._py.ex`pms_GradientBoostingClassifier_score = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None, 'y': np.array(${\n      opts['y'] ?? undefined\n    }) if ${opts['y'] !== undefined} else None, 'sample_weight': np.array(${\n      opts['sample_weight'] ?? undefined\n    }) if ${opts['sample_weight'] !== undefined} else None}\n\npms_GradientBoostingClassifier_score = {k: v for k, v in pms_GradientBoostingClassifier_score.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_GradientBoostingClassifier_score = bridgeGradientBoostingClassifier[${this.id}].score(**pms_GradientBoostingClassifier_score)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_GradientBoostingClassifier_score.tolist() if hasattr(res_GradientBoostingClassifier_score, 'tolist') else res_GradientBoostingClassifier_score`\n  }\n\n  /**\n    Compute decision function of `X` for each iteration.\n\n    This method allows monitoring (i.e. determine error on testing set) after each stage.\n   */\n  async staged_decision_function(opts: {\n    /**\n      The input samples. Internally, it will be converted to `dtype=np.float32` and if a sparse matrix is provided to a sparse `csr\\_matrix`.\n     */\n    X?: ArrayLike | SparseMatrix[]\n  }): Promise<any[]> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This GradientBoostingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'GradientBoostingClassifier must call init() before staged_decision_function()'\n      )\n    }\n\n    // set up method params\n    await this._py\n      .ex`pms_GradientBoostingClassifier_staged_decision_function = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None}\n\npms_GradientBoostingClassifier_staged_decision_function = {k: v for k, v in pms_GradientBoostingClassifier_staged_decision_function.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_GradientBoostingClassifier_staged_decision_function = bridgeGradientBoostingClassifier[${this.id}].staged_decision_function(**pms_GradientBoostingClassifier_staged_decision_function)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_GradientBoostingClassifier_staged_decision_function.tolist() if hasattr(res_GradientBoostingClassifier_staged_decision_function, 'tolist') else res_GradientBoostingClassifier_staged_decision_function`\n  }\n\n  /**\n    Predict class at each stage for X.\n\n    This method allows monitoring (i.e. determine error on testing set) after each stage.\n   */\n  async staged_predict(opts: {\n    /**\n      The input samples. Internally, it will be converted to `dtype=np.float32` and if a sparse matrix is provided to a sparse `csr\\_matrix`.\n     */\n    X?: ArrayLike | SparseMatrix[]\n  }): Promise<any[]> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This GradientBoostingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'GradientBoostingClassifier must call init() before staged_predict()'\n      )\n    }\n\n    // set up method params\n    await this._py\n      .ex`pms_GradientBoostingClassifier_staged_predict = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None}\n\npms_GradientBoostingClassifier_staged_predict = {k: v for k, v in pms_GradientBoostingClassifier_staged_predict.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_GradientBoostingClassifier_staged_predict = bridgeGradientBoostingClassifier[${this.id}].staged_predict(**pms_GradientBoostingClassifier_staged_predict)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_GradientBoostingClassifier_staged_predict.tolist() if hasattr(res_GradientBoostingClassifier_staged_predict, 'tolist') else res_GradientBoostingClassifier_staged_predict`\n  }\n\n  /**\n    Predict class probabilities at each stage for X.\n\n    This method allows monitoring (i.e. determine error on testing set) after each stage.\n   */\n  async staged_predict_proba(opts: {\n    /**\n      The input samples. Internally, it will be converted to `dtype=np.float32` and if a sparse matrix is provided to a sparse `csr\\_matrix`.\n     */\n    X?: ArrayLike | SparseMatrix[]\n  }): Promise<any[]> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This GradientBoostingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'GradientBoostingClassifier must call init() before staged_predict_proba()'\n      )\n    }\n\n    // set up method params\n    await this._py\n      .ex`pms_GradientBoostingClassifier_staged_predict_proba = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None}\n\npms_GradientBoostingClassifier_staged_predict_proba = {k: v for k, v in pms_GradientBoostingClassifier_staged_predict_proba.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_GradientBoostingClassifier_staged_predict_proba = bridgeGradientBoostingClassifier[${this.id}].staged_predict_proba(**pms_GradientBoostingClassifier_staged_predict_proba)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_GradientBoostingClassifier_staged_predict_proba.tolist() if hasattr(res_GradientBoostingClassifier_staged_predict_proba, 'tolist') else res_GradientBoostingClassifier_staged_predict_proba`\n  }\n\n  /**\n    The number of estimators as selected by early stopping (if `n\\_iter\\_no\\_change` is specified). Otherwise it is set to `n\\_estimators`.\n   */\n  get n_estimators_(): Promise<number> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This GradientBoostingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'GradientBoostingClassifier must call init() before accessing n_estimators_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_GradientBoostingClassifier_n_estimators_ = bridgeGradientBoostingClassifier[${this.id}].n_estimators_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_GradientBoostingClassifier_n_estimators_.tolist() if hasattr(attr_GradientBoostingClassifier_n_estimators_, 'tolist') else attr_GradientBoostingClassifier_n_estimators_`\n    })()\n  }\n\n  /**\n    The improvement in loss (= deviance) on the out-of-bag samples relative to the previous iteration. `oob\\_improvement\\_\\[0\\]` is the improvement in loss of the first stage over the `init` estimator. Only available if `subsample < 1.0`\n   */\n  get oob_improvement_(): Promise<NDArray> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This GradientBoostingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'GradientBoostingClassifier must call init() before accessing oob_improvement_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_GradientBoostingClassifier_oob_improvement_ = bridgeGradientBoostingClassifier[${this.id}].oob_improvement_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_GradientBoostingClassifier_oob_improvement_.tolist() if hasattr(attr_GradientBoostingClassifier_oob_improvement_, 'tolist') else attr_GradientBoostingClassifier_oob_improvement_`\n    })()\n  }\n\n  /**\n    The i-th score `train\\_score\\_\\[i\\]` is the deviance (= loss) of the model at iteration `i` on the in-bag sample. If `subsample \\== 1` this is the deviance on the training data.\n   */\n  get train_score_(): Promise<NDArray> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This GradientBoostingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'GradientBoostingClassifier must call init() before accessing train_score_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_GradientBoostingClassifier_train_score_ = bridgeGradientBoostingClassifier[${this.id}].train_score_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_GradientBoostingClassifier_train_score_.tolist() if hasattr(attr_GradientBoostingClassifier_train_score_, 'tolist') else attr_GradientBoostingClassifier_train_score_`\n    })()\n  }\n\n  /**\n    The concrete `LossFunction` object.\n   */\n  get loss_(): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This GradientBoostingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'GradientBoostingClassifier must call init() before accessing loss_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_GradientBoostingClassifier_loss_ = bridgeGradientBoostingClassifier[${this.id}].loss_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_GradientBoostingClassifier_loss_.tolist() if hasattr(attr_GradientBoostingClassifier_loss_, 'tolist') else attr_GradientBoostingClassifier_loss_`\n    })()\n  }\n\n  /**\n    The estimator that provides the initial predictions. Set via the `init` argument or `loss.init\\_estimator`.\n   */\n  get init_(): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This GradientBoostingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'GradientBoostingClassifier must call init() before accessing init_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_GradientBoostingClassifier_init_ = bridgeGradientBoostingClassifier[${this.id}].init_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_GradientBoostingClassifier_init_.tolist() if hasattr(attr_GradientBoostingClassifier_init_, 'tolist') else attr_GradientBoostingClassifier_init_`\n    })()\n  }\n\n  /**\n    The collection of fitted sub-estimators. `loss\\_.K` is 1 for binary classification, otherwise n\\_classes.\n   */\n  get estimators_(): Promise<any[]> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This GradientBoostingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'GradientBoostingClassifier must call init() before accessing estimators_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_GradientBoostingClassifier_estimators_ = bridgeGradientBoostingClassifier[${this.id}].estimators_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_GradientBoostingClassifier_estimators_.tolist() if hasattr(attr_GradientBoostingClassifier_estimators_, 'tolist') else attr_GradientBoostingClassifier_estimators_`\n    })()\n  }\n\n  /**\n    The classes labels.\n   */\n  get classes_(): Promise<NDArray> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This GradientBoostingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'GradientBoostingClassifier must call init() before accessing classes_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_GradientBoostingClassifier_classes_ = bridgeGradientBoostingClassifier[${this.id}].classes_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_GradientBoostingClassifier_classes_.tolist() if hasattr(attr_GradientBoostingClassifier_classes_, 'tolist') else attr_GradientBoostingClassifier_classes_`\n    })()\n  }\n\n  /**\n    Number of features seen during [fit](../../glossary.html#term-fit).\n   */\n  get n_features_in_(): Promise<number> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This GradientBoostingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'GradientBoostingClassifier must call init() before accessing n_features_in_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_GradientBoostingClassifier_n_features_in_ = bridgeGradientBoostingClassifier[${this.id}].n_features_in_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_GradientBoostingClassifier_n_features_in_.tolist() if hasattr(attr_GradientBoostingClassifier_n_features_in_, 'tolist') else attr_GradientBoostingClassifier_n_features_in_`\n    })()\n  }\n\n  /**\n    Names of features seen during [fit](../../glossary.html#term-fit). Defined only when `X` has feature names that are all strings.\n   */\n  get feature_names_in_(): Promise<NDArray> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This GradientBoostingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'GradientBoostingClassifier must call init() before accessing feature_names_in_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_GradientBoostingClassifier_feature_names_in_ = bridgeGradientBoostingClassifier[${this.id}].feature_names_in_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_GradientBoostingClassifier_feature_names_in_.tolist() if hasattr(attr_GradientBoostingClassifier_feature_names_in_, 'tolist') else attr_GradientBoostingClassifier_feature_names_in_`\n    })()\n  }\n\n  /**\n    The number of classes.\n   */\n  get n_classes_(): Promise<number> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This GradientBoostingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'GradientBoostingClassifier must call init() before accessing n_classes_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_GradientBoostingClassifier_n_classes_ = bridgeGradientBoostingClassifier[${this.id}].n_classes_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_GradientBoostingClassifier_n_classes_.tolist() if hasattr(attr_GradientBoostingClassifier_n_classes_, 'tolist') else attr_GradientBoostingClassifier_n_classes_`\n    })()\n  }\n\n  /**\n    The inferred value of max\\_features.\n   */\n  get max_features_(): Promise<number> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This GradientBoostingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'GradientBoostingClassifier must call init() before accessing max_features_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_GradientBoostingClassifier_max_features_ = bridgeGradientBoostingClassifier[${this.id}].max_features_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_GradientBoostingClassifier_max_features_.tolist() if hasattr(attr_GradientBoostingClassifier_max_features_, 'tolist') else attr_GradientBoostingClassifier_max_features_`\n    })()\n  }\n}\n","/* eslint-disable */\n/* NOTE: This file is auto-generated. Do not edit it directly. */\n\nimport crypto from 'node:crypto'\n\nimport { PythonBridge, NDArray, ArrayLike, SparseMatrix } from '@/sklearn/types'\n\n/**\n  Gradient Boosting for regression.\n\n  This estimator builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage a regression tree is fit on the negative gradient of the given loss function.\n\n  [`sklearn.ensemble.HistGradientBoostingRegressor`](sklearn.ensemble.HistGradientBoostingRegressor.html#sklearn.ensemble.HistGradientBoostingRegressor \"sklearn.ensemble.HistGradientBoostingRegressor\") is a much faster variant of this algorithm for intermediate datasets (`n\\_samples >= 10\\_000`).\n\n  Read more in the [User Guide](../ensemble.html#gradient-boosting).\n\n  [Python Reference](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html)\n */\nexport class GradientBoostingRegressor {\n  id: string\n  opts: any\n\n  _py: PythonBridge\n  _isInitialized: boolean = false\n  _isDisposed: boolean = false\n\n  constructor(opts?: {\n    /**\n      Loss function to be optimized. ‘squared\\_error’ refers to the squared error for regression. ‘absolute\\_error’ refers to the absolute error of regression and is a robust loss function. ‘huber’ is a combination of the two. ‘quantile’ allows quantile regression (use `alpha` to specify the quantile).\n\n      @defaultValue `'squared_error'`\n     */\n    loss?: 'squared_error' | 'absolute_error' | 'huber' | 'quantile'\n\n    /**\n      Learning rate shrinks the contribution of each tree by `learning\\_rate`. There is a trade-off between learning\\_rate and n\\_estimators. Values must be in the range `\\[0.0, inf)`.\n\n      @defaultValue `0.1`\n     */\n    learning_rate?: number\n\n    /**\n      The number of boosting stages to perform. Gradient boosting is fairly robust to over-fitting so a large number usually results in better performance. Values must be in the range `\\[1, inf)`.\n\n      @defaultValue `100`\n     */\n    n_estimators?: number\n\n    /**\n      The fraction of samples to be used for fitting the individual base learners. If smaller than 1.0 this results in Stochastic Gradient Boosting. `subsample` interacts with the parameter `n\\_estimators`. Choosing `subsample < 1.0` leads to a reduction of variance and an increase in bias. Values must be in the range `(0.0, 1.0\\]`.\n\n      @defaultValue `1`\n     */\n    subsample?: number\n\n    /**\n      The function to measure the quality of a split. Supported criteria are “friedman\\_mse” for the mean squared error with improvement score by Friedman, “squared\\_error” for mean squared error. The default value of “friedman\\_mse” is generally the best as it can provide a better approximation in some cases.\n\n      @defaultValue `'friedman_mse'`\n     */\n    criterion?: 'friedman_mse' | 'squared_error'\n\n    /**\n      The minimum number of samples required to split an internal node:\n\n      @defaultValue `2`\n     */\n    min_samples_split?: number\n\n    /**\n      The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least `min\\_samples\\_leaf` training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.\n\n      @defaultValue `1`\n     */\n    min_samples_leaf?: number\n\n    /**\n      The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample\\_weight is not provided. Values must be in the range `\\[0.0, 0.5\\]`.\n\n      @defaultValue `0`\n     */\n    min_weight_fraction_leaf?: number\n\n    /**\n      Maximum depth of the individual regression estimators. The maximum depth limits the number of nodes in the tree. Tune this parameter for best performance; the best value depends on the interaction of the input variables. If `undefined`, then nodes are expanded until all leaves are pure or until all leaves contain less than min\\_samples\\_split samples. If int, values must be in the range `\\[1, inf)`.\n\n      @defaultValue `3`\n     */\n    max_depth?: number\n\n    /**\n      A node will be split if this split induces a decrease of the impurity greater than or equal to this value. Values must be in the range `\\[0.0, inf)`.\n\n      The weighted impurity decrease equation is the following:\n\n      @defaultValue `0`\n     */\n    min_impurity_decrease?: number\n\n    /**\n      An estimator object that is used to compute the initial predictions. `init` has to provide [fit](../../glossary.html#term-fit) and [predict](../../glossary.html#term-predict). If ‘zero’, the initial raw predictions are set to zero. By default a `DummyEstimator` is used, predicting either the average target value (for loss=’squared\\_error’), or a quantile for the other losses.\n     */\n    init?: 'zero'\n\n    /**\n      Controls the random seed given to each Tree estimator at each boosting iteration. In addition, it controls the random permutation of the features at each split (see Notes for more details). It also controls the random splitting of the training data to obtain a validation set if `n\\_iter\\_no\\_change` is not `undefined`. Pass an int for reproducible output across multiple function calls. See [Glossary](../../glossary.html#term-random_state).\n     */\n    random_state?: number\n\n    /**\n      The number of features to consider when looking for the best split:\n     */\n    max_features?: 'auto' | 'sqrt' | 'log2' | number | number\n\n    /**\n      The alpha-quantile of the huber loss function and the quantile loss function. Only if `loss='huber'` or `loss='quantile'`. Values must be in the range `(0.0, 1.0)`.\n\n      @defaultValue `0.9`\n     */\n    alpha?: number\n\n    /**\n      Enable verbose output. If 1 then it prints progress and performance once in a while (the more trees the lower the frequency). If greater than 1 then it prints progress and performance for every tree. Values must be in the range `\\[0, inf)`.\n\n      @defaultValue `0`\n     */\n    verbose?: number\n\n    /**\n      Grow trees with `max\\_leaf\\_nodes` in best-first fashion. Best nodes are defined as relative reduction in impurity. Values must be in the range `\\[2, inf)`. If `undefined`, then unlimited number of leaf nodes.\n     */\n    max_leaf_nodes?: number\n\n    /**\n      When set to `true`, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just erase the previous solution. See [the Glossary](../../glossary.html#term-warm_start).\n\n      @defaultValue `false`\n     */\n    warm_start?: boolean\n\n    /**\n      The proportion of training data to set aside as validation set for early stopping. Values must be in the range `(0.0, 1.0)`. Only used if `n\\_iter\\_no\\_change` is set to an integer.\n\n      @defaultValue `0.1`\n     */\n    validation_fraction?: number\n\n    /**\n      `n\\_iter\\_no\\_change` is used to decide if early stopping will be used to terminate training when validation score is not improving. By default it is set to `undefined` to disable early stopping. If set to a number, it will set aside `validation\\_fraction` size of the training data as validation and terminate training when validation score is not improving in all of the previous `n\\_iter\\_no\\_change` numbers of iterations. Values must be in the range `\\[1, inf)`.\n     */\n    n_iter_no_change?: number\n\n    /**\n      Tolerance for the early stopping. When the loss is not improving by at least tol for `n\\_iter\\_no\\_change` iterations (if set to a number), the training stops. Values must be in the range `\\[0.0, inf)`.\n\n      @defaultValue `0.0001`\n     */\n    tol?: number\n\n    /**\n      Complexity parameter used for Minimal Cost-Complexity Pruning. The subtree with the largest cost complexity that is smaller than `ccp\\_alpha` will be chosen. By default, no pruning is performed. Values must be in the range `\\[0.0, inf)`. See [Minimal Cost-Complexity Pruning](../tree.html#minimal-cost-complexity-pruning) for details.\n\n      @defaultValue `0`\n     */\n    ccp_alpha?: any\n  }) {\n    this.id = `GradientBoostingRegressor${crypto.randomUUID().split('-')[0]}`\n    this.opts = opts || {}\n  }\n\n  get py(): PythonBridge {\n    return this._py\n  }\n\n  set py(pythonBridge: PythonBridge) {\n    this._py = pythonBridge\n  }\n\n  /**\n    Initializes the underlying Python resources.\n\n    This instance is not usable until the `Promise` returned by `init()` resolves.\n   */\n  async init(py: PythonBridge): Promise<void> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This GradientBoostingRegressor instance has already been disposed'\n      )\n    }\n\n    if (this._isInitialized) {\n      return\n    }\n\n    if (!py) {\n      throw new Error(\n        'GradientBoostingRegressor.init requires a PythonBridge instance'\n      )\n    }\n\n    this._py = py\n\n    await this._py.ex`\nimport numpy as np\nfrom sklearn.ensemble import GradientBoostingRegressor\ntry: bridgeGradientBoostingRegressor\nexcept NameError: bridgeGradientBoostingRegressor = {}\n`\n\n    // set up constructor params\n    await this._py.ex`ctor_GradientBoostingRegressor = {'loss': ${\n      this.opts['loss'] ?? undefined\n    }, 'learning_rate': ${\n      this.opts['learning_rate'] ?? undefined\n    }, 'n_estimators': ${\n      this.opts['n_estimators'] ?? undefined\n    }, 'subsample': ${this.opts['subsample'] ?? undefined}, 'criterion': ${\n      this.opts['criterion'] ?? undefined\n    }, 'min_samples_split': ${\n      this.opts['min_samples_split'] ?? undefined\n    }, 'min_samples_leaf': ${\n      this.opts['min_samples_leaf'] ?? undefined\n    }, 'min_weight_fraction_leaf': ${\n      this.opts['min_weight_fraction_leaf'] ?? undefined\n    }, 'max_depth': ${\n      this.opts['max_depth'] ?? undefined\n    }, 'min_impurity_decrease': ${\n      this.opts['min_impurity_decrease'] ?? undefined\n    }, 'init': ${this.opts['init'] ?? undefined}, 'random_state': ${\n      this.opts['random_state'] ?? undefined\n    }, 'max_features': ${this.opts['max_features'] ?? undefined}, 'alpha': ${\n      this.opts['alpha'] ?? undefined\n    }, 'verbose': ${this.opts['verbose'] ?? undefined}, 'max_leaf_nodes': ${\n      this.opts['max_leaf_nodes'] ?? undefined\n    }, 'warm_start': ${\n      this.opts['warm_start'] ?? undefined\n    }, 'validation_fraction': ${\n      this.opts['validation_fraction'] ?? undefined\n    }, 'n_iter_no_change': ${\n      this.opts['n_iter_no_change'] ?? undefined\n    }, 'tol': ${this.opts['tol'] ?? undefined}, 'ccp_alpha': ${\n      this.opts['ccp_alpha'] ?? undefined\n    }}\n\nctor_GradientBoostingRegressor = {k: v for k, v in ctor_GradientBoostingRegressor.items() if v is not None}`\n\n    await this._py\n      .ex`bridgeGradientBoostingRegressor[${this.id}] = GradientBoostingRegressor(**ctor_GradientBoostingRegressor)`\n\n    this._isInitialized = true\n  }\n\n  /**\n    Disposes of the underlying Python resources.\n\n    Once `dispose()` is called, the instance is no longer usable.\n   */\n  async dispose() {\n    if (this._isDisposed) {\n      return\n    }\n\n    if (!this._isInitialized) {\n      return\n    }\n\n    await this._py.ex`del bridgeGradientBoostingRegressor[${this.id}]`\n\n    this._isDisposed = true\n  }\n\n  /**\n    Apply trees in the ensemble to X, return leaf indices.\n   */\n  async apply(opts: {\n    /**\n      The input samples. Internally, its dtype will be converted to `dtype=np.float32`. If a sparse matrix is provided, it will be converted to a sparse `csr\\_matrix`.\n     */\n    X?: ArrayLike | SparseMatrix[]\n  }): Promise<ArrayLike[]> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This GradientBoostingRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'GradientBoostingRegressor must call init() before apply()'\n      )\n    }\n\n    // set up method params\n    await this._py.ex`pms_GradientBoostingRegressor_apply = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None}\n\npms_GradientBoostingRegressor_apply = {k: v for k, v in pms_GradientBoostingRegressor_apply.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_GradientBoostingRegressor_apply = bridgeGradientBoostingRegressor[${this.id}].apply(**pms_GradientBoostingRegressor_apply)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_GradientBoostingRegressor_apply.tolist() if hasattr(res_GradientBoostingRegressor_apply, 'tolist') else res_GradientBoostingRegressor_apply`\n  }\n\n  /**\n    Fit the gradient boosting model.\n   */\n  async fit(opts: {\n    /**\n      The input samples. Internally, it will be converted to `dtype=np.float32` and if a sparse matrix is provided to a sparse `csr\\_matrix`.\n     */\n    X?: ArrayLike | SparseMatrix[]\n\n    /**\n      Target values (strings or integers in classification, real numbers in regression) For classification, labels must correspond to classes.\n     */\n    y?: ArrayLike\n\n    /**\n      Sample weights. If `undefined`, then samples are equally weighted. Splits that would create child nodes with net zero or negative weight are ignored while searching for a split in each node. In the case of classification, splits are also ignored if they would result in any single class carrying a negative weight in either child node.\n     */\n    sample_weight?: ArrayLike\n\n    /**\n      The monitor is called after each iteration with the current iteration, a reference to the estimator and the local variables of `\\_fit\\_stages` as keyword arguments `callable(i, self, locals())`. If the callable returns `true` the fitting procedure is stopped. The monitor can be used for various things such as computing held-out estimates, early stopping, model introspect, and snapshoting.\n     */\n    monitor?: any\n  }): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This GradientBoostingRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('GradientBoostingRegressor must call init() before fit()')\n    }\n\n    // set up method params\n    await this._py.ex`pms_GradientBoostingRegressor_fit = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None, 'y': np.array(${\n      opts['y'] ?? undefined\n    }) if ${opts['y'] !== undefined} else None, 'sample_weight': np.array(${\n      opts['sample_weight'] ?? undefined\n    }) if ${opts['sample_weight'] !== undefined} else None, 'monitor': ${\n      opts['monitor'] ?? undefined\n    }}\n\npms_GradientBoostingRegressor_fit = {k: v for k, v in pms_GradientBoostingRegressor_fit.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_GradientBoostingRegressor_fit = bridgeGradientBoostingRegressor[${this.id}].fit(**pms_GradientBoostingRegressor_fit)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_GradientBoostingRegressor_fit.tolist() if hasattr(res_GradientBoostingRegressor_fit, 'tolist') else res_GradientBoostingRegressor_fit`\n  }\n\n  /**\n    Predict regression target for X.\n   */\n  async predict(opts: {\n    /**\n      The input samples. Internally, it will be converted to `dtype=np.float32` and if a sparse matrix is provided to a sparse `csr\\_matrix`.\n     */\n    X?: ArrayLike | SparseMatrix[]\n  }): Promise<NDArray> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This GradientBoostingRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'GradientBoostingRegressor must call init() before predict()'\n      )\n    }\n\n    // set up method params\n    await this._py.ex`pms_GradientBoostingRegressor_predict = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None}\n\npms_GradientBoostingRegressor_predict = {k: v for k, v in pms_GradientBoostingRegressor_predict.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_GradientBoostingRegressor_predict = bridgeGradientBoostingRegressor[${this.id}].predict(**pms_GradientBoostingRegressor_predict)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_GradientBoostingRegressor_predict.tolist() if hasattr(res_GradientBoostingRegressor_predict, 'tolist') else res_GradientBoostingRegressor_predict`\n  }\n\n  /**\n    Return the coefficient of determination of the prediction.\n\n    The coefficient of determination \\\\(R^2\\\\) is defined as \\\\((1 - \\\\frac{u}{v})\\\\), where \\\\(u\\\\) is the residual sum of squares `((y\\_true \\- y\\_pred)\\*\\* 2).sum()` and \\\\(v\\\\) is the total sum of squares `((y\\_true \\- y\\_true.mean()) \\*\\* 2).sum()`. The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of `y`, disregarding the input features, would get a \\\\(R^2\\\\) score of 0.0.\n   */\n  async score(opts: {\n    /**\n      Test samples. For some estimators this may be a precomputed kernel matrix or a list of generic objects instead with shape `(n\\_samples, n\\_samples\\_fitted)`, where `n\\_samples\\_fitted` is the number of samples used in the fitting for the estimator.\n     */\n    X?: ArrayLike[]\n\n    /**\n      True values for `X`.\n     */\n    y?: ArrayLike\n\n    /**\n      Sample weights.\n     */\n    sample_weight?: ArrayLike\n  }): Promise<number> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This GradientBoostingRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'GradientBoostingRegressor must call init() before score()'\n      )\n    }\n\n    // set up method params\n    await this._py.ex`pms_GradientBoostingRegressor_score = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None, 'y': np.array(${\n      opts['y'] ?? undefined\n    }) if ${opts['y'] !== undefined} else None, 'sample_weight': np.array(${\n      opts['sample_weight'] ?? undefined\n    }) if ${opts['sample_weight'] !== undefined} else None}\n\npms_GradientBoostingRegressor_score = {k: v for k, v in pms_GradientBoostingRegressor_score.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_GradientBoostingRegressor_score = bridgeGradientBoostingRegressor[${this.id}].score(**pms_GradientBoostingRegressor_score)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_GradientBoostingRegressor_score.tolist() if hasattr(res_GradientBoostingRegressor_score, 'tolist') else res_GradientBoostingRegressor_score`\n  }\n\n  /**\n    Predict regression target at each stage for X.\n\n    This method allows monitoring (i.e. determine error on testing set) after each stage.\n   */\n  async staged_predict(opts: {\n    /**\n      The input samples. Internally, it will be converted to `dtype=np.float32` and if a sparse matrix is provided to a sparse `csr\\_matrix`.\n     */\n    X?: ArrayLike | SparseMatrix[]\n  }): Promise<any[]> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This GradientBoostingRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'GradientBoostingRegressor must call init() before staged_predict()'\n      )\n    }\n\n    // set up method params\n    await this._py\n      .ex`pms_GradientBoostingRegressor_staged_predict = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None}\n\npms_GradientBoostingRegressor_staged_predict = {k: v for k, v in pms_GradientBoostingRegressor_staged_predict.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_GradientBoostingRegressor_staged_predict = bridgeGradientBoostingRegressor[${this.id}].staged_predict(**pms_GradientBoostingRegressor_staged_predict)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_GradientBoostingRegressor_staged_predict.tolist() if hasattr(res_GradientBoostingRegressor_staged_predict, 'tolist') else res_GradientBoostingRegressor_staged_predict`\n  }\n\n  /**\n    The improvement in loss (= deviance) on the out-of-bag samples relative to the previous iteration. `oob\\_improvement\\_\\[0\\]` is the improvement in loss of the first stage over the `init` estimator. Only available if `subsample < 1.0`\n   */\n  get oob_improvement_(): Promise<NDArray> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This GradientBoostingRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'GradientBoostingRegressor must call init() before accessing oob_improvement_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_GradientBoostingRegressor_oob_improvement_ = bridgeGradientBoostingRegressor[${this.id}].oob_improvement_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_GradientBoostingRegressor_oob_improvement_.tolist() if hasattr(attr_GradientBoostingRegressor_oob_improvement_, 'tolist') else attr_GradientBoostingRegressor_oob_improvement_`\n    })()\n  }\n\n  /**\n    The i-th score `train\\_score\\_\\[i\\]` is the deviance (= loss) of the model at iteration `i` on the in-bag sample. If `subsample \\== 1` this is the deviance on the training data.\n   */\n  get train_score_(): Promise<NDArray> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This GradientBoostingRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'GradientBoostingRegressor must call init() before accessing train_score_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_GradientBoostingRegressor_train_score_ = bridgeGradientBoostingRegressor[${this.id}].train_score_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_GradientBoostingRegressor_train_score_.tolist() if hasattr(attr_GradientBoostingRegressor_train_score_, 'tolist') else attr_GradientBoostingRegressor_train_score_`\n    })()\n  }\n\n  /**\n    The concrete `LossFunction` object.\n   */\n  get loss_(): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This GradientBoostingRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'GradientBoostingRegressor must call init() before accessing loss_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_GradientBoostingRegressor_loss_ = bridgeGradientBoostingRegressor[${this.id}].loss_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_GradientBoostingRegressor_loss_.tolist() if hasattr(attr_GradientBoostingRegressor_loss_, 'tolist') else attr_GradientBoostingRegressor_loss_`\n    })()\n  }\n\n  /**\n    The estimator that provides the initial predictions. Set via the `init` argument or `loss.init\\_estimator`.\n   */\n  get init_(): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This GradientBoostingRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'GradientBoostingRegressor must call init() before accessing init_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_GradientBoostingRegressor_init_ = bridgeGradientBoostingRegressor[${this.id}].init_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_GradientBoostingRegressor_init_.tolist() if hasattr(attr_GradientBoostingRegressor_init_, 'tolist') else attr_GradientBoostingRegressor_init_`\n    })()\n  }\n\n  /**\n    The collection of fitted sub-estimators.\n   */\n  get estimators_(): Promise<any[]> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This GradientBoostingRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'GradientBoostingRegressor must call init() before accessing estimators_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_GradientBoostingRegressor_estimators_ = bridgeGradientBoostingRegressor[${this.id}].estimators_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_GradientBoostingRegressor_estimators_.tolist() if hasattr(attr_GradientBoostingRegressor_estimators_, 'tolist') else attr_GradientBoostingRegressor_estimators_`\n    })()\n  }\n\n  /**\n    The number of estimators as selected by early stopping (if `n\\_iter\\_no\\_change` is specified). Otherwise it is set to `n\\_estimators`.\n   */\n  get n_estimators_(): Promise<number> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This GradientBoostingRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'GradientBoostingRegressor must call init() before accessing n_estimators_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_GradientBoostingRegressor_n_estimators_ = bridgeGradientBoostingRegressor[${this.id}].n_estimators_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_GradientBoostingRegressor_n_estimators_.tolist() if hasattr(attr_GradientBoostingRegressor_n_estimators_, 'tolist') else attr_GradientBoostingRegressor_n_estimators_`\n    })()\n  }\n\n  /**\n    Number of features seen during [fit](../../glossary.html#term-fit).\n   */\n  get n_features_in_(): Promise<number> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This GradientBoostingRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'GradientBoostingRegressor must call init() before accessing n_features_in_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_GradientBoostingRegressor_n_features_in_ = bridgeGradientBoostingRegressor[${this.id}].n_features_in_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_GradientBoostingRegressor_n_features_in_.tolist() if hasattr(attr_GradientBoostingRegressor_n_features_in_, 'tolist') else attr_GradientBoostingRegressor_n_features_in_`\n    })()\n  }\n\n  /**\n    Names of features seen during [fit](../../glossary.html#term-fit). Defined only when `X` has feature names that are all strings.\n   */\n  get feature_names_in_(): Promise<NDArray> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This GradientBoostingRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'GradientBoostingRegressor must call init() before accessing feature_names_in_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_GradientBoostingRegressor_feature_names_in_ = bridgeGradientBoostingRegressor[${this.id}].feature_names_in_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_GradientBoostingRegressor_feature_names_in_.tolist() if hasattr(attr_GradientBoostingRegressor_feature_names_in_, 'tolist') else attr_GradientBoostingRegressor_feature_names_in_`\n    })()\n  }\n\n  /**\n    The inferred value of max\\_features.\n   */\n  get max_features_(): Promise<number> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This GradientBoostingRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'GradientBoostingRegressor must call init() before accessing max_features_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_GradientBoostingRegressor_max_features_ = bridgeGradientBoostingRegressor[${this.id}].max_features_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_GradientBoostingRegressor_max_features_.tolist() if hasattr(attr_GradientBoostingRegressor_max_features_, 'tolist') else attr_GradientBoostingRegressor_max_features_`\n    })()\n  }\n}\n","/* eslint-disable */\n/* NOTE: This file is auto-generated. Do not edit it directly. */\n\nimport crypto from 'node:crypto'\n\nimport { PythonBridge, NDArray, ArrayLike, SparseMatrix } from '@/sklearn/types'\n\n/**\n  Histogram-based Gradient Boosting Classification Tree.\n\n  This estimator is much faster than [`GradientBoostingClassifier`](sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier \"sklearn.ensemble.GradientBoostingClassifier\") for big datasets (n\\_samples >= 10 000).\n\n  This estimator has native support for missing values (NaNs). During training, the tree grower learns at each split point whether samples with missing values should go to the left or right child, based on the potential gain. When predicting, samples with missing values are assigned to the left or right child consequently. If no missing values were encountered for a given feature during training, then samples with missing values are mapped to whichever child has the most samples.\n\n  This implementation is inspired by [LightGBM](https://github.com/Microsoft/LightGBM).\n\n  Read more in the [User Guide](../ensemble.html#histogram-based-gradient-boosting).\n\n  [Python Reference](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html)\n */\nexport class HistGradientBoostingClassifier {\n  id: string\n  opts: any\n\n  _py: PythonBridge\n  _isInitialized: boolean = false\n  _isDisposed: boolean = false\n\n  constructor(opts?: {\n    /**\n      The loss function to use in the boosting process.\n\n      For binary classification problems, ‘log\\_loss’ is also known as logistic loss, binomial deviance or binary crossentropy. Internally, the model fits one tree per boosting iteration and uses the logistic sigmoid function (expit) as inverse link function to compute the predicted positive class probability.\n\n      For multiclass classification problems, ‘log\\_loss’ is also known as multinomial deviance or categorical crossentropy. Internally, the model fits one tree per boosting iteration and per class and uses the softmax function as inverse link function to compute the predicted probabilities of the classes.\n\n      @defaultValue `'log_loss'`\n     */\n    loss?:\n      | 'log_loss'\n      | 'auto'\n      | 'binary_crossentropy'\n      | 'categorical_crossentropy'\n\n    /**\n      The learning rate, also known as *shrinkage*. This is used as a multiplicative factor for the leaves values. Use `1` for no shrinkage.\n\n      @defaultValue `0.1`\n     */\n    learning_rate?: number\n\n    /**\n      The maximum number of iterations of the boosting process, i.e. the maximum number of trees for binary classification. For multiclass classification, `n\\_classes` trees per iteration are built.\n\n      @defaultValue `100`\n     */\n    max_iter?: number\n\n    /**\n      The maximum number of leaves for each tree. Must be strictly greater than 1. If `undefined`, there is no maximum limit.\n\n      @defaultValue `31`\n     */\n    max_leaf_nodes?: number\n\n    /**\n      The maximum depth of each tree. The depth of a tree is the number of edges to go from the root to the deepest leaf. Depth isn’t constrained by default.\n     */\n    max_depth?: number\n\n    /**\n      The minimum number of samples per leaf. For small datasets with less than a few hundred samples, it is recommended to lower this value since only very shallow trees would be built.\n\n      @defaultValue `20`\n     */\n    min_samples_leaf?: number\n\n    /**\n      The L2 regularization parameter. Use 0 for no regularization.\n\n      @defaultValue `0`\n     */\n    l2_regularization?: number\n\n    /**\n      The maximum number of bins to use for non-missing values. Before training, each feature of the input array `X` is binned into integer-valued bins, which allows for a much faster training stage. Features with a small number of unique values may use less than `max\\_bins` bins. In addition to the `max\\_bins` bins, one more bin is always reserved for missing values. Must be no larger than 255.\n\n      @defaultValue `255`\n     */\n    max_bins?: number\n\n    /**\n      Indicates the categorical features.\n     */\n    categorical_features?: number\n\n    /**\n      Monotonic constraint to enforce on each feature are specified using the following integer values:\n     */\n    monotonic_cst?: any[] | any\n\n    /**\n      Specify interaction constraints, the sets of features which can interact with each other in child node splits.\n\n      Each item specifies the set of feature indices that are allowed to interact with each other. If there are more features than specified in these constraints, they are treated as if they were specified as an additional set.\n\n      The strings “pairwise” and “no\\_interactions” are shorthands for allowing only pairwise or no interactions, respectively.\n\n      For instance, with 5 features in total, `interaction\\_cst=\\[{0, 1}\\]` is equivalent to `interaction\\_cst=\\[{0, 1}, {2, 3, 4}\\]`, and specifies that each branch of a tree will either only split on features 0 and 1 or only split on features 2, 3 and 4.\n     */\n    interaction_cst?: 'pairwise' | 'no_interaction'\n\n    /**\n      When set to `true`, reuse the solution of the previous call to fit and add more estimators to the ensemble. For results to be valid, the estimator should be re-trained on the same data only. See [the Glossary](../../glossary.html#term-warm_start).\n\n      @defaultValue `false`\n     */\n    warm_start?: boolean\n\n    /**\n      If ‘auto’, early stopping is enabled if the sample size is larger than 10000. If `true`, early stopping is enabled, otherwise early stopping is disabled.\n\n      @defaultValue `'auto'`\n     */\n    early_stopping?: 'auto' | boolean\n\n    /**\n      Scoring parameter to use for early stopping. It can be a single string (see [The scoring parameter: defining model evaluation rules](../model_evaluation.html#scoring-parameter)) or a callable (see [Defining your scoring strategy from metric functions](../model_evaluation.html#scoring)). If `undefined`, the estimator’s default scorer is used. If `scoring='loss'`, early stopping is checked w.r.t the loss value. Only used if early stopping is performed.\n\n      @defaultValue `'loss'`\n     */\n    scoring?: string\n\n    /**\n      Proportion (or absolute size) of training data to set aside as validation data for early stopping. If `undefined`, early stopping is done on the training data. Only used if early stopping is performed.\n\n      @defaultValue `0.1`\n     */\n    validation_fraction?: number\n\n    /**\n      Used to determine when to “early stop”. The fitting process is stopped when none of the last `n\\_iter\\_no\\_change` scores are better than the `n\\_iter\\_no\\_change \\- 1` -th-to-last one, up to some tolerance. Only used if early stopping is performed.\n\n      @defaultValue `10`\n     */\n    n_iter_no_change?: number\n\n    /**\n      The absolute tolerance to use when comparing scores. The higher the tolerance, the more likely we are to early stop: higher tolerance means that it will be harder for subsequent iterations to be considered an improvement upon the reference score.\n\n      @defaultValue `1e-7`\n     */\n    tol?: number\n\n    /**\n      The verbosity level. If not zero, print some information about the fitting process.\n\n      @defaultValue `0`\n     */\n    verbose?: number\n\n    /**\n      Pseudo-random number generator to control the subsampling in the binning process, and the train/validation data split if early stopping is enabled. Pass an int for reproducible output across multiple function calls. See [Glossary](../../glossary.html#term-random_state).\n     */\n    random_state?: number\n\n    /**\n      Weights associated with classes in the form `{class\\_label: weight}`. If not given, all classes are supposed to have weight one. The “balanced” mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as `n\\_samples / (n\\_classes \\* np.bincount(y))`. Note that these weights will be multiplied with sample\\_weight (passed through the fit method) if `sample\\_weight` is specified.\n     */\n    class_weight?: any | 'balanced'\n  }) {\n    this.id = `HistGradientBoostingClassifier${\n      crypto.randomUUID().split('-')[0]\n    }`\n    this.opts = opts || {}\n  }\n\n  get py(): PythonBridge {\n    return this._py\n  }\n\n  set py(pythonBridge: PythonBridge) {\n    this._py = pythonBridge\n  }\n\n  /**\n    Initializes the underlying Python resources.\n\n    This instance is not usable until the `Promise` returned by `init()` resolves.\n   */\n  async init(py: PythonBridge): Promise<void> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This HistGradientBoostingClassifier instance has already been disposed'\n      )\n    }\n\n    if (this._isInitialized) {\n      return\n    }\n\n    if (!py) {\n      throw new Error(\n        'HistGradientBoostingClassifier.init requires a PythonBridge instance'\n      )\n    }\n\n    this._py = py\n\n    await this._py.ex`\nimport numpy as np\nfrom sklearn.ensemble import HistGradientBoostingClassifier\ntry: bridgeHistGradientBoostingClassifier\nexcept NameError: bridgeHistGradientBoostingClassifier = {}\n`\n\n    // set up constructor params\n    await this._py.ex`ctor_HistGradientBoostingClassifier = {'loss': ${\n      this.opts['loss'] ?? undefined\n    }, 'learning_rate': ${\n      this.opts['learning_rate'] ?? undefined\n    }, 'max_iter': ${this.opts['max_iter'] ?? undefined}, 'max_leaf_nodes': ${\n      this.opts['max_leaf_nodes'] ?? undefined\n    }, 'max_depth': ${\n      this.opts['max_depth'] ?? undefined\n    }, 'min_samples_leaf': ${\n      this.opts['min_samples_leaf'] ?? undefined\n    }, 'l2_regularization': ${\n      this.opts['l2_regularization'] ?? undefined\n    }, 'max_bins': ${\n      this.opts['max_bins'] ?? undefined\n    }, 'categorical_features': np.array(${\n      this.opts['categorical_features'] ?? undefined\n    }) if ${\n      this.opts['categorical_features'] !== undefined\n    } else None, 'monotonic_cst': np.array(${\n      this.opts['monotonic_cst'] ?? undefined\n    }) if ${\n      this.opts['monotonic_cst'] !== undefined\n    } else None, 'interaction_cst': ${\n      this.opts['interaction_cst'] ?? undefined\n    }, 'warm_start': ${\n      this.opts['warm_start'] ?? undefined\n    }, 'early_stopping': ${\n      this.opts['early_stopping'] ?? undefined\n    }, 'scoring': ${\n      this.opts['scoring'] ?? undefined\n    }, 'validation_fraction': ${\n      this.opts['validation_fraction'] ?? undefined\n    }, 'n_iter_no_change': ${\n      this.opts['n_iter_no_change'] ?? undefined\n    }, 'tol': ${this.opts['tol'] ?? undefined}, 'verbose': ${\n      this.opts['verbose'] ?? undefined\n    }, 'random_state': ${\n      this.opts['random_state'] ?? undefined\n    }, 'class_weight': ${this.opts['class_weight'] ?? undefined}}\n\nctor_HistGradientBoostingClassifier = {k: v for k, v in ctor_HistGradientBoostingClassifier.items() if v is not None}`\n\n    await this._py\n      .ex`bridgeHistGradientBoostingClassifier[${this.id}] = HistGradientBoostingClassifier(**ctor_HistGradientBoostingClassifier)`\n\n    this._isInitialized = true\n  }\n\n  /**\n    Disposes of the underlying Python resources.\n\n    Once `dispose()` is called, the instance is no longer usable.\n   */\n  async dispose() {\n    if (this._isDisposed) {\n      return\n    }\n\n    if (!this._isInitialized) {\n      return\n    }\n\n    await this._py.ex`del bridgeHistGradientBoostingClassifier[${this.id}]`\n\n    this._isDisposed = true\n  }\n\n  /**\n    Compute the decision function of `X`.\n   */\n  async decision_function(opts: {\n    /**\n      The input samples.\n     */\n    X?: ArrayLike\n  }): Promise<NDArray> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This HistGradientBoostingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'HistGradientBoostingClassifier must call init() before decision_function()'\n      )\n    }\n\n    // set up method params\n    await this._py\n      .ex`pms_HistGradientBoostingClassifier_decision_function = {'X': ${\n      opts['X'] ?? undefined\n    }}\n\npms_HistGradientBoostingClassifier_decision_function = {k: v for k, v in pms_HistGradientBoostingClassifier_decision_function.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_HistGradientBoostingClassifier_decision_function = bridgeHistGradientBoostingClassifier[${this.id}].decision_function(**pms_HistGradientBoostingClassifier_decision_function)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_HistGradientBoostingClassifier_decision_function.tolist() if hasattr(res_HistGradientBoostingClassifier_decision_function, 'tolist') else res_HistGradientBoostingClassifier_decision_function`\n  }\n\n  /**\n    Fit the gradient boosting model.\n   */\n  async fit(opts: {\n    /**\n      The input samples.\n     */\n    X?: ArrayLike[]\n\n    /**\n      Target values.\n     */\n    y?: ArrayLike\n\n    /**\n      Weights of training data.\n     */\n    sample_weight?: any\n  }): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This HistGradientBoostingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'HistGradientBoostingClassifier must call init() before fit()'\n      )\n    }\n\n    // set up method params\n    await this._py.ex`pms_HistGradientBoostingClassifier_fit = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None, 'y': np.array(${\n      opts['y'] ?? undefined\n    }) if ${opts['y'] !== undefined} else None, 'sample_weight': np.array(${\n      opts['sample_weight'] ?? undefined\n    }) if ${opts['sample_weight'] !== undefined} else None}\n\npms_HistGradientBoostingClassifier_fit = {k: v for k, v in pms_HistGradientBoostingClassifier_fit.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_HistGradientBoostingClassifier_fit = bridgeHistGradientBoostingClassifier[${this.id}].fit(**pms_HistGradientBoostingClassifier_fit)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_HistGradientBoostingClassifier_fit.tolist() if hasattr(res_HistGradientBoostingClassifier_fit, 'tolist') else res_HistGradientBoostingClassifier_fit`\n  }\n\n  /**\n    Predict classes for X.\n   */\n  async predict(opts: {\n    /**\n      The input samples.\n     */\n    X?: ArrayLike\n  }): Promise<NDArray> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This HistGradientBoostingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'HistGradientBoostingClassifier must call init() before predict()'\n      )\n    }\n\n    // set up method params\n    await this._py.ex`pms_HistGradientBoostingClassifier_predict = {'X': ${\n      opts['X'] ?? undefined\n    }}\n\npms_HistGradientBoostingClassifier_predict = {k: v for k, v in pms_HistGradientBoostingClassifier_predict.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_HistGradientBoostingClassifier_predict = bridgeHistGradientBoostingClassifier[${this.id}].predict(**pms_HistGradientBoostingClassifier_predict)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_HistGradientBoostingClassifier_predict.tolist() if hasattr(res_HistGradientBoostingClassifier_predict, 'tolist') else res_HistGradientBoostingClassifier_predict`\n  }\n\n  /**\n    Predict class probabilities for X.\n   */\n  async predict_proba(opts: {\n    /**\n      The input samples.\n     */\n    X?: ArrayLike\n  }): Promise<NDArray> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This HistGradientBoostingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'HistGradientBoostingClassifier must call init() before predict_proba()'\n      )\n    }\n\n    // set up method params\n    await this._py\n      .ex`pms_HistGradientBoostingClassifier_predict_proba = {'X': ${\n      opts['X'] ?? undefined\n    }}\n\npms_HistGradientBoostingClassifier_predict_proba = {k: v for k, v in pms_HistGradientBoostingClassifier_predict_proba.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_HistGradientBoostingClassifier_predict_proba = bridgeHistGradientBoostingClassifier[${this.id}].predict_proba(**pms_HistGradientBoostingClassifier_predict_proba)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_HistGradientBoostingClassifier_predict_proba.tolist() if hasattr(res_HistGradientBoostingClassifier_predict_proba, 'tolist') else res_HistGradientBoostingClassifier_predict_proba`\n  }\n\n  /**\n    Return the mean accuracy on the given test data and labels.\n\n    In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted.\n   */\n  async score(opts: {\n    /**\n      Test samples.\n     */\n    X?: ArrayLike[]\n\n    /**\n      True labels for `X`.\n     */\n    y?: ArrayLike\n\n    /**\n      Sample weights.\n     */\n    sample_weight?: ArrayLike\n  }): Promise<number> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This HistGradientBoostingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'HistGradientBoostingClassifier must call init() before score()'\n      )\n    }\n\n    // set up method params\n    await this._py\n      .ex`pms_HistGradientBoostingClassifier_score = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None, 'y': np.array(${\n      opts['y'] ?? undefined\n    }) if ${opts['y'] !== undefined} else None, 'sample_weight': np.array(${\n      opts['sample_weight'] ?? undefined\n    }) if ${opts['sample_weight'] !== undefined} else None}\n\npms_HistGradientBoostingClassifier_score = {k: v for k, v in pms_HistGradientBoostingClassifier_score.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_HistGradientBoostingClassifier_score = bridgeHistGradientBoostingClassifier[${this.id}].score(**pms_HistGradientBoostingClassifier_score)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_HistGradientBoostingClassifier_score.tolist() if hasattr(res_HistGradientBoostingClassifier_score, 'tolist') else res_HistGradientBoostingClassifier_score`\n  }\n\n  /**\n    Compute decision function of `X` for each iteration.\n\n    This method allows monitoring (i.e. determine error on testing set) after each stage.\n   */\n  async staged_decision_function(opts: {\n    /**\n      The input samples.\n     */\n    X?: ArrayLike[]\n  }): Promise<any[]> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This HistGradientBoostingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'HistGradientBoostingClassifier must call init() before staged_decision_function()'\n      )\n    }\n\n    // set up method params\n    await this._py\n      .ex`pms_HistGradientBoostingClassifier_staged_decision_function = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None}\n\npms_HistGradientBoostingClassifier_staged_decision_function = {k: v for k, v in pms_HistGradientBoostingClassifier_staged_decision_function.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_HistGradientBoostingClassifier_staged_decision_function = bridgeHistGradientBoostingClassifier[${this.id}].staged_decision_function(**pms_HistGradientBoostingClassifier_staged_decision_function)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_HistGradientBoostingClassifier_staged_decision_function.tolist() if hasattr(res_HistGradientBoostingClassifier_staged_decision_function, 'tolist') else res_HistGradientBoostingClassifier_staged_decision_function`\n  }\n\n  /**\n    Predict classes at each iteration.\n\n    This method allows monitoring (i.e. determine error on testing set) after each stage.\n   */\n  async staged_predict(opts: {\n    /**\n      The input samples.\n     */\n    X?: ArrayLike[]\n  }): Promise<any[]> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This HistGradientBoostingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'HistGradientBoostingClassifier must call init() before staged_predict()'\n      )\n    }\n\n    // set up method params\n    await this._py\n      .ex`pms_HistGradientBoostingClassifier_staged_predict = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None}\n\npms_HistGradientBoostingClassifier_staged_predict = {k: v for k, v in pms_HistGradientBoostingClassifier_staged_predict.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_HistGradientBoostingClassifier_staged_predict = bridgeHistGradientBoostingClassifier[${this.id}].staged_predict(**pms_HistGradientBoostingClassifier_staged_predict)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_HistGradientBoostingClassifier_staged_predict.tolist() if hasattr(res_HistGradientBoostingClassifier_staged_predict, 'tolist') else res_HistGradientBoostingClassifier_staged_predict`\n  }\n\n  /**\n    Predict class probabilities at each iteration.\n\n    This method allows monitoring (i.e. determine error on testing set) after each stage.\n   */\n  async staged_predict_proba(opts: {\n    /**\n      The input samples.\n     */\n    X?: ArrayLike[]\n  }): Promise<any[]> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This HistGradientBoostingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'HistGradientBoostingClassifier must call init() before staged_predict_proba()'\n      )\n    }\n\n    // set up method params\n    await this._py\n      .ex`pms_HistGradientBoostingClassifier_staged_predict_proba = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None}\n\npms_HistGradientBoostingClassifier_staged_predict_proba = {k: v for k, v in pms_HistGradientBoostingClassifier_staged_predict_proba.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_HistGradientBoostingClassifier_staged_predict_proba = bridgeHistGradientBoostingClassifier[${this.id}].staged_predict_proba(**pms_HistGradientBoostingClassifier_staged_predict_proba)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_HistGradientBoostingClassifier_staged_predict_proba.tolist() if hasattr(res_HistGradientBoostingClassifier_staged_predict_proba, 'tolist') else res_HistGradientBoostingClassifier_staged_predict_proba`\n  }\n\n  /**\n    Class labels.\n   */\n  get classes_(): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This HistGradientBoostingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'HistGradientBoostingClassifier must call init() before accessing classes_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_HistGradientBoostingClassifier_classes_ = bridgeHistGradientBoostingClassifier[${this.id}].classes_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_HistGradientBoostingClassifier_classes_.tolist() if hasattr(attr_HistGradientBoostingClassifier_classes_, 'tolist') else attr_HistGradientBoostingClassifier_classes_`\n    })()\n  }\n\n  /**\n    Indicates whether early stopping is used during training.\n   */\n  get do_early_stopping_(): Promise<boolean> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This HistGradientBoostingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'HistGradientBoostingClassifier must call init() before accessing do_early_stopping_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_HistGradientBoostingClassifier_do_early_stopping_ = bridgeHistGradientBoostingClassifier[${this.id}].do_early_stopping_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_HistGradientBoostingClassifier_do_early_stopping_.tolist() if hasattr(attr_HistGradientBoostingClassifier_do_early_stopping_, 'tolist') else attr_HistGradientBoostingClassifier_do_early_stopping_`\n    })()\n  }\n\n  /**\n    The number of tree that are built at each iteration. This is equal to 1 for binary classification, and to `n\\_classes` for multiclass classification.\n   */\n  get n_trees_per_iteration_(): Promise<number> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This HistGradientBoostingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'HistGradientBoostingClassifier must call init() before accessing n_trees_per_iteration_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_HistGradientBoostingClassifier_n_trees_per_iteration_ = bridgeHistGradientBoostingClassifier[${this.id}].n_trees_per_iteration_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_HistGradientBoostingClassifier_n_trees_per_iteration_.tolist() if hasattr(attr_HistGradientBoostingClassifier_n_trees_per_iteration_, 'tolist') else attr_HistGradientBoostingClassifier_n_trees_per_iteration_`\n    })()\n  }\n\n  /**\n    The scores at each iteration on the training data. The first entry is the score of the ensemble before the first iteration. Scores are computed according to the `scoring` parameter. If `scoring` is not ‘loss’, scores are computed on a subset of at most 10 000 samples. Empty if no early stopping.\n   */\n  get train_score_(): Promise<NDArray> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This HistGradientBoostingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'HistGradientBoostingClassifier must call init() before accessing train_score_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_HistGradientBoostingClassifier_train_score_ = bridgeHistGradientBoostingClassifier[${this.id}].train_score_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_HistGradientBoostingClassifier_train_score_.tolist() if hasattr(attr_HistGradientBoostingClassifier_train_score_, 'tolist') else attr_HistGradientBoostingClassifier_train_score_`\n    })()\n  }\n\n  /**\n    The scores at each iteration on the held-out validation data. The first entry is the score of the ensemble before the first iteration. Scores are computed according to the `scoring` parameter. Empty if no early stopping or if `validation\\_fraction` is `undefined`.\n   */\n  get validation_score_(): Promise<NDArray> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This HistGradientBoostingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'HistGradientBoostingClassifier must call init() before accessing validation_score_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_HistGradientBoostingClassifier_validation_score_ = bridgeHistGradientBoostingClassifier[${this.id}].validation_score_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_HistGradientBoostingClassifier_validation_score_.tolist() if hasattr(attr_HistGradientBoostingClassifier_validation_score_, 'tolist') else attr_HistGradientBoostingClassifier_validation_score_`\n    })()\n  }\n\n  /**\n    Boolean mask for the categorical features. `undefined` if there are no categorical features.\n   */\n  get is_categorical_(): Promise<NDArray> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This HistGradientBoostingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'HistGradientBoostingClassifier must call init() before accessing is_categorical_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_HistGradientBoostingClassifier_is_categorical_ = bridgeHistGradientBoostingClassifier[${this.id}].is_categorical_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_HistGradientBoostingClassifier_is_categorical_.tolist() if hasattr(attr_HistGradientBoostingClassifier_is_categorical_, 'tolist') else attr_HistGradientBoostingClassifier_is_categorical_`\n    })()\n  }\n\n  /**\n    Number of features seen during [fit](../../glossary.html#term-fit).\n   */\n  get n_features_in_(): Promise<number> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This HistGradientBoostingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'HistGradientBoostingClassifier must call init() before accessing n_features_in_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_HistGradientBoostingClassifier_n_features_in_ = bridgeHistGradientBoostingClassifier[${this.id}].n_features_in_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_HistGradientBoostingClassifier_n_features_in_.tolist() if hasattr(attr_HistGradientBoostingClassifier_n_features_in_, 'tolist') else attr_HistGradientBoostingClassifier_n_features_in_`\n    })()\n  }\n\n  /**\n    Names of features seen during [fit](../../glossary.html#term-fit). Defined only when `X` has feature names that are all strings.\n   */\n  get feature_names_in_(): Promise<NDArray> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This HistGradientBoostingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'HistGradientBoostingClassifier must call init() before accessing feature_names_in_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_HistGradientBoostingClassifier_feature_names_in_ = bridgeHistGradientBoostingClassifier[${this.id}].feature_names_in_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_HistGradientBoostingClassifier_feature_names_in_.tolist() if hasattr(attr_HistGradientBoostingClassifier_feature_names_in_, 'tolist') else attr_HistGradientBoostingClassifier_feature_names_in_`\n    })()\n  }\n}\n","/* eslint-disable */\n/* NOTE: This file is auto-generated. Do not edit it directly. */\n\nimport crypto from 'node:crypto'\n\nimport { PythonBridge, NDArray, ArrayLike, SparseMatrix } from '@/sklearn/types'\n\n/**\n  Histogram-based Gradient Boosting Regression Tree.\n\n  This estimator is much faster than [`GradientBoostingRegressor`](sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor \"sklearn.ensemble.GradientBoostingRegressor\") for big datasets (n\\_samples >= 10 000).\n\n  This estimator has native support for missing values (NaNs). During training, the tree grower learns at each split point whether samples with missing values should go to the left or right child, based on the potential gain. When predicting, samples with missing values are assigned to the left or right child consequently. If no missing values were encountered for a given feature during training, then samples with missing values are mapped to whichever child has the most samples.\n\n  This implementation is inspired by [LightGBM](https://github.com/Microsoft/LightGBM).\n\n  Read more in the [User Guide](../ensemble.html#histogram-based-gradient-boosting).\n\n  [Python Reference](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingRegressor.html)\n */\nexport class HistGradientBoostingRegressor {\n  id: string\n  opts: any\n\n  _py: PythonBridge\n  _isInitialized: boolean = false\n  _isDisposed: boolean = false\n\n  constructor(opts?: {\n    /**\n      The loss function to use in the boosting process. Note that the “squared error” and “poisson” losses actually implement “half least squares loss” and “half poisson deviance” to simplify the computation of the gradient. Furthermore, “poisson” loss internally uses a log-link and requires `y >= 0`. “quantile” uses the pinball loss.\n\n      @defaultValue `'squared_error'`\n     */\n    loss?: 'squared_error' | 'absolute_error' | 'poisson' | 'quantile'\n\n    /**\n      If loss is “quantile”, this parameter specifies which quantile to be estimated and must be between 0 and 1.\n     */\n    quantile?: number\n\n    /**\n      The learning rate, also known as *shrinkage*. This is used as a multiplicative factor for the leaves values. Use `1` for no shrinkage.\n\n      @defaultValue `0.1`\n     */\n    learning_rate?: number\n\n    /**\n      The maximum number of iterations of the boosting process, i.e. the maximum number of trees.\n\n      @defaultValue `100`\n     */\n    max_iter?: number\n\n    /**\n      The maximum number of leaves for each tree. Must be strictly greater than 1. If `undefined`, there is no maximum limit.\n\n      @defaultValue `31`\n     */\n    max_leaf_nodes?: number\n\n    /**\n      The maximum depth of each tree. The depth of a tree is the number of edges to go from the root to the deepest leaf. Depth isn’t constrained by default.\n     */\n    max_depth?: number\n\n    /**\n      The minimum number of samples per leaf. For small datasets with less than a few hundred samples, it is recommended to lower this value since only very shallow trees would be built.\n\n      @defaultValue `20`\n     */\n    min_samples_leaf?: number\n\n    /**\n      The L2 regularization parameter. Use `0` for no regularization (default).\n\n      @defaultValue `0`\n     */\n    l2_regularization?: number\n\n    /**\n      The maximum number of bins to use for non-missing values. Before training, each feature of the input array `X` is binned into integer-valued bins, which allows for a much faster training stage. Features with a small number of unique values may use less than `max\\_bins` bins. In addition to the `max\\_bins` bins, one more bin is always reserved for missing values. Must be no larger than 255.\n\n      @defaultValue `255`\n     */\n    max_bins?: number\n\n    /**\n      Indicates the categorical features.\n     */\n    categorical_features?: number\n\n    /**\n      Monotonic constraint to enforce on each feature are specified using the following integer values:\n     */\n    monotonic_cst?: any[] | any\n\n    /**\n      Specify interaction constraints, the sets of features which can interact with each other in child node splits.\n\n      Each item specifies the set of feature indices that are allowed to interact with each other. If there are more features than specified in these constraints, they are treated as if they were specified as an additional set.\n\n      The strings “pairwise” and “no\\_interactions” are shorthands for allowing only pairwise or no interactions, respectively.\n\n      For instance, with 5 features in total, `interaction\\_cst=\\[{0, 1}\\]` is equivalent to `interaction\\_cst=\\[{0, 1}, {2, 3, 4}\\]`, and specifies that each branch of a tree will either only split on features 0 and 1 or only split on features 2, 3 and 4.\n     */\n    interaction_cst?: 'pairwise' | 'no_interaction'\n\n    /**\n      When set to `true`, reuse the solution of the previous call to fit and add more estimators to the ensemble. For results to be valid, the estimator should be re-trained on the same data only. See [the Glossary](../../glossary.html#term-warm_start).\n\n      @defaultValue `false`\n     */\n    warm_start?: boolean\n\n    /**\n      If ‘auto’, early stopping is enabled if the sample size is larger than 10000. If `true`, early stopping is enabled, otherwise early stopping is disabled.\n\n      @defaultValue `'auto'`\n     */\n    early_stopping?: 'auto' | boolean\n\n    /**\n      Scoring parameter to use for early stopping. It can be a single string (see [The scoring parameter: defining model evaluation rules](../model_evaluation.html#scoring-parameter)) or a callable (see [Defining your scoring strategy from metric functions](../model_evaluation.html#scoring)). If `undefined`, the estimator’s default scorer is used. If `scoring='loss'`, early stopping is checked w.r.t the loss value. Only used if early stopping is performed.\n\n      @defaultValue `'loss'`\n     */\n    scoring?: string\n\n    /**\n      Proportion (or absolute size) of training data to set aside as validation data for early stopping. If `undefined`, early stopping is done on the training data. Only used if early stopping is performed.\n\n      @defaultValue `0.1`\n     */\n    validation_fraction?: number\n\n    /**\n      Used to determine when to “early stop”. The fitting process is stopped when none of the last `n\\_iter\\_no\\_change` scores are better than the `n\\_iter\\_no\\_change \\- 1` -th-to-last one, up to some tolerance. Only used if early stopping is performed.\n\n      @defaultValue `10`\n     */\n    n_iter_no_change?: number\n\n    /**\n      The absolute tolerance to use when comparing scores during early stopping. The higher the tolerance, the more likely we are to early stop: higher tolerance means that it will be harder for subsequent iterations to be considered an improvement upon the reference score.\n\n      @defaultValue `1e-7`\n     */\n    tol?: number\n\n    /**\n      The verbosity level. If not zero, print some information about the fitting process.\n\n      @defaultValue `0`\n     */\n    verbose?: number\n\n    /**\n      Pseudo-random number generator to control the subsampling in the binning process, and the train/validation data split if early stopping is enabled. Pass an int for reproducible output across multiple function calls. See [Glossary](../../glossary.html#term-random_state).\n     */\n    random_state?: number\n  }) {\n    this.id = `HistGradientBoostingRegressor${\n      crypto.randomUUID().split('-')[0]\n    }`\n    this.opts = opts || {}\n  }\n\n  get py(): PythonBridge {\n    return this._py\n  }\n\n  set py(pythonBridge: PythonBridge) {\n    this._py = pythonBridge\n  }\n\n  /**\n    Initializes the underlying Python resources.\n\n    This instance is not usable until the `Promise` returned by `init()` resolves.\n   */\n  async init(py: PythonBridge): Promise<void> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This HistGradientBoostingRegressor instance has already been disposed'\n      )\n    }\n\n    if (this._isInitialized) {\n      return\n    }\n\n    if (!py) {\n      throw new Error(\n        'HistGradientBoostingRegressor.init requires a PythonBridge instance'\n      )\n    }\n\n    this._py = py\n\n    await this._py.ex`\nimport numpy as np\nfrom sklearn.ensemble import HistGradientBoostingRegressor\ntry: bridgeHistGradientBoostingRegressor\nexcept NameError: bridgeHistGradientBoostingRegressor = {}\n`\n\n    // set up constructor params\n    await this._py.ex`ctor_HistGradientBoostingRegressor = {'loss': ${\n      this.opts['loss'] ?? undefined\n    }, 'quantile': ${this.opts['quantile'] ?? undefined}, 'learning_rate': ${\n      this.opts['learning_rate'] ?? undefined\n    }, 'max_iter': ${this.opts['max_iter'] ?? undefined}, 'max_leaf_nodes': ${\n      this.opts['max_leaf_nodes'] ?? undefined\n    }, 'max_depth': ${\n      this.opts['max_depth'] ?? undefined\n    }, 'min_samples_leaf': ${\n      this.opts['min_samples_leaf'] ?? undefined\n    }, 'l2_regularization': ${\n      this.opts['l2_regularization'] ?? undefined\n    }, 'max_bins': ${\n      this.opts['max_bins'] ?? undefined\n    }, 'categorical_features': np.array(${\n      this.opts['categorical_features'] ?? undefined\n    }) if ${\n      this.opts['categorical_features'] !== undefined\n    } else None, 'monotonic_cst': np.array(${\n      this.opts['monotonic_cst'] ?? undefined\n    }) if ${\n      this.opts['monotonic_cst'] !== undefined\n    } else None, 'interaction_cst': ${\n      this.opts['interaction_cst'] ?? undefined\n    }, 'warm_start': ${\n      this.opts['warm_start'] ?? undefined\n    }, 'early_stopping': ${\n      this.opts['early_stopping'] ?? undefined\n    }, 'scoring': ${\n      this.opts['scoring'] ?? undefined\n    }, 'validation_fraction': ${\n      this.opts['validation_fraction'] ?? undefined\n    }, 'n_iter_no_change': ${\n      this.opts['n_iter_no_change'] ?? undefined\n    }, 'tol': ${this.opts['tol'] ?? undefined}, 'verbose': ${\n      this.opts['verbose'] ?? undefined\n    }, 'random_state': ${this.opts['random_state'] ?? undefined}}\n\nctor_HistGradientBoostingRegressor = {k: v for k, v in ctor_HistGradientBoostingRegressor.items() if v is not None}`\n\n    await this._py\n      .ex`bridgeHistGradientBoostingRegressor[${this.id}] = HistGradientBoostingRegressor(**ctor_HistGradientBoostingRegressor)`\n\n    this._isInitialized = true\n  }\n\n  /**\n    Disposes of the underlying Python resources.\n\n    Once `dispose()` is called, the instance is no longer usable.\n   */\n  async dispose() {\n    if (this._isDisposed) {\n      return\n    }\n\n    if (!this._isInitialized) {\n      return\n    }\n\n    await this._py.ex`del bridgeHistGradientBoostingRegressor[${this.id}]`\n\n    this._isDisposed = true\n  }\n\n  /**\n    Fit the gradient boosting model.\n   */\n  async fit(opts: {\n    /**\n      The input samples.\n     */\n    X?: ArrayLike[]\n\n    /**\n      Target values.\n     */\n    y?: ArrayLike\n\n    /**\n      Weights of training data.\n     */\n    sample_weight?: any\n  }): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This HistGradientBoostingRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'HistGradientBoostingRegressor must call init() before fit()'\n      )\n    }\n\n    // set up method params\n    await this._py.ex`pms_HistGradientBoostingRegressor_fit = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None, 'y': np.array(${\n      opts['y'] ?? undefined\n    }) if ${opts['y'] !== undefined} else None, 'sample_weight': np.array(${\n      opts['sample_weight'] ?? undefined\n    }) if ${opts['sample_weight'] !== undefined} else None}\n\npms_HistGradientBoostingRegressor_fit = {k: v for k, v in pms_HistGradientBoostingRegressor_fit.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_HistGradientBoostingRegressor_fit = bridgeHistGradientBoostingRegressor[${this.id}].fit(**pms_HistGradientBoostingRegressor_fit)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_HistGradientBoostingRegressor_fit.tolist() if hasattr(res_HistGradientBoostingRegressor_fit, 'tolist') else res_HistGradientBoostingRegressor_fit`\n  }\n\n  /**\n    Predict values for X.\n   */\n  async predict(opts: {\n    /**\n      The input samples.\n     */\n    X?: ArrayLike\n  }): Promise<NDArray> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This HistGradientBoostingRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'HistGradientBoostingRegressor must call init() before predict()'\n      )\n    }\n\n    // set up method params\n    await this._py.ex`pms_HistGradientBoostingRegressor_predict = {'X': ${\n      opts['X'] ?? undefined\n    }}\n\npms_HistGradientBoostingRegressor_predict = {k: v for k, v in pms_HistGradientBoostingRegressor_predict.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_HistGradientBoostingRegressor_predict = bridgeHistGradientBoostingRegressor[${this.id}].predict(**pms_HistGradientBoostingRegressor_predict)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_HistGradientBoostingRegressor_predict.tolist() if hasattr(res_HistGradientBoostingRegressor_predict, 'tolist') else res_HistGradientBoostingRegressor_predict`\n  }\n\n  /**\n    Return the coefficient of determination of the prediction.\n\n    The coefficient of determination \\\\(R^2\\\\) is defined as \\\\((1 - \\\\frac{u}{v})\\\\), where \\\\(u\\\\) is the residual sum of squares `((y\\_true \\- y\\_pred)\\*\\* 2).sum()` and \\\\(v\\\\) is the total sum of squares `((y\\_true \\- y\\_true.mean()) \\*\\* 2).sum()`. The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of `y`, disregarding the input features, would get a \\\\(R^2\\\\) score of 0.0.\n   */\n  async score(opts: {\n    /**\n      Test samples. For some estimators this may be a precomputed kernel matrix or a list of generic objects instead with shape `(n\\_samples, n\\_samples\\_fitted)`, where `n\\_samples\\_fitted` is the number of samples used in the fitting for the estimator.\n     */\n    X?: ArrayLike[]\n\n    /**\n      True values for `X`.\n     */\n    y?: ArrayLike\n\n    /**\n      Sample weights.\n     */\n    sample_weight?: ArrayLike\n  }): Promise<number> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This HistGradientBoostingRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'HistGradientBoostingRegressor must call init() before score()'\n      )\n    }\n\n    // set up method params\n    await this._py\n      .ex`pms_HistGradientBoostingRegressor_score = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None, 'y': np.array(${\n      opts['y'] ?? undefined\n    }) if ${opts['y'] !== undefined} else None, 'sample_weight': np.array(${\n      opts['sample_weight'] ?? undefined\n    }) if ${opts['sample_weight'] !== undefined} else None}\n\npms_HistGradientBoostingRegressor_score = {k: v for k, v in pms_HistGradientBoostingRegressor_score.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_HistGradientBoostingRegressor_score = bridgeHistGradientBoostingRegressor[${this.id}].score(**pms_HistGradientBoostingRegressor_score)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_HistGradientBoostingRegressor_score.tolist() if hasattr(res_HistGradientBoostingRegressor_score, 'tolist') else res_HistGradientBoostingRegressor_score`\n  }\n\n  /**\n    Predict regression target for each iteration.\n\n    This method allows monitoring (i.e. determine error on testing set) after each stage.\n   */\n  async staged_predict(opts: {\n    /**\n      The input samples.\n     */\n    X?: ArrayLike[]\n  }): Promise<any[]> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This HistGradientBoostingRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'HistGradientBoostingRegressor must call init() before staged_predict()'\n      )\n    }\n\n    // set up method params\n    await this._py\n      .ex`pms_HistGradientBoostingRegressor_staged_predict = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None}\n\npms_HistGradientBoostingRegressor_staged_predict = {k: v for k, v in pms_HistGradientBoostingRegressor_staged_predict.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_HistGradientBoostingRegressor_staged_predict = bridgeHistGradientBoostingRegressor[${this.id}].staged_predict(**pms_HistGradientBoostingRegressor_staged_predict)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_HistGradientBoostingRegressor_staged_predict.tolist() if hasattr(res_HistGradientBoostingRegressor_staged_predict, 'tolist') else res_HistGradientBoostingRegressor_staged_predict`\n  }\n\n  /**\n    Indicates whether early stopping is used during training.\n   */\n  get do_early_stopping_(): Promise<boolean> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This HistGradientBoostingRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'HistGradientBoostingRegressor must call init() before accessing do_early_stopping_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_HistGradientBoostingRegressor_do_early_stopping_ = bridgeHistGradientBoostingRegressor[${this.id}].do_early_stopping_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_HistGradientBoostingRegressor_do_early_stopping_.tolist() if hasattr(attr_HistGradientBoostingRegressor_do_early_stopping_, 'tolist') else attr_HistGradientBoostingRegressor_do_early_stopping_`\n    })()\n  }\n\n  /**\n    The number of tree that are built at each iteration. For regressors, this is always 1.\n   */\n  get n_trees_per_iteration_(): Promise<number> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This HistGradientBoostingRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'HistGradientBoostingRegressor must call init() before accessing n_trees_per_iteration_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_HistGradientBoostingRegressor_n_trees_per_iteration_ = bridgeHistGradientBoostingRegressor[${this.id}].n_trees_per_iteration_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_HistGradientBoostingRegressor_n_trees_per_iteration_.tolist() if hasattr(attr_HistGradientBoostingRegressor_n_trees_per_iteration_, 'tolist') else attr_HistGradientBoostingRegressor_n_trees_per_iteration_`\n    })()\n  }\n\n  /**\n    The scores at each iteration on the training data. The first entry is the score of the ensemble before the first iteration. Scores are computed according to the `scoring` parameter. If `scoring` is not ‘loss’, scores are computed on a subset of at most 10 000 samples. Empty if no early stopping.\n   */\n  get train_score_(): Promise<NDArray> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This HistGradientBoostingRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'HistGradientBoostingRegressor must call init() before accessing train_score_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_HistGradientBoostingRegressor_train_score_ = bridgeHistGradientBoostingRegressor[${this.id}].train_score_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_HistGradientBoostingRegressor_train_score_.tolist() if hasattr(attr_HistGradientBoostingRegressor_train_score_, 'tolist') else attr_HistGradientBoostingRegressor_train_score_`\n    })()\n  }\n\n  /**\n    The scores at each iteration on the held-out validation data. The first entry is the score of the ensemble before the first iteration. Scores are computed according to the `scoring` parameter. Empty if no early stopping or if `validation\\_fraction` is `undefined`.\n   */\n  get validation_score_(): Promise<NDArray> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This HistGradientBoostingRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'HistGradientBoostingRegressor must call init() before accessing validation_score_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_HistGradientBoostingRegressor_validation_score_ = bridgeHistGradientBoostingRegressor[${this.id}].validation_score_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_HistGradientBoostingRegressor_validation_score_.tolist() if hasattr(attr_HistGradientBoostingRegressor_validation_score_, 'tolist') else attr_HistGradientBoostingRegressor_validation_score_`\n    })()\n  }\n\n  /**\n    Boolean mask for the categorical features. `undefined` if there are no categorical features.\n   */\n  get is_categorical_(): Promise<NDArray> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This HistGradientBoostingRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'HistGradientBoostingRegressor must call init() before accessing is_categorical_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_HistGradientBoostingRegressor_is_categorical_ = bridgeHistGradientBoostingRegressor[${this.id}].is_categorical_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_HistGradientBoostingRegressor_is_categorical_.tolist() if hasattr(attr_HistGradientBoostingRegressor_is_categorical_, 'tolist') else attr_HistGradientBoostingRegressor_is_categorical_`\n    })()\n  }\n\n  /**\n    Number of features seen during [fit](../../glossary.html#term-fit).\n   */\n  get n_features_in_(): Promise<number> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This HistGradientBoostingRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'HistGradientBoostingRegressor must call init() before accessing n_features_in_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_HistGradientBoostingRegressor_n_features_in_ = bridgeHistGradientBoostingRegressor[${this.id}].n_features_in_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_HistGradientBoostingRegressor_n_features_in_.tolist() if hasattr(attr_HistGradientBoostingRegressor_n_features_in_, 'tolist') else attr_HistGradientBoostingRegressor_n_features_in_`\n    })()\n  }\n\n  /**\n    Names of features seen during [fit](../../glossary.html#term-fit). Defined only when `X` has feature names that are all strings.\n   */\n  get feature_names_in_(): Promise<NDArray> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This HistGradientBoostingRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'HistGradientBoostingRegressor must call init() before accessing feature_names_in_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_HistGradientBoostingRegressor_feature_names_in_ = bridgeHistGradientBoostingRegressor[${this.id}].feature_names_in_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_HistGradientBoostingRegressor_feature_names_in_.tolist() if hasattr(attr_HistGradientBoostingRegressor_feature_names_in_, 'tolist') else attr_HistGradientBoostingRegressor_feature_names_in_`\n    })()\n  }\n}\n","/* eslint-disable */\n/* NOTE: This file is auto-generated. Do not edit it directly. */\n\nimport crypto from 'node:crypto'\n\nimport { PythonBridge, NDArray, ArrayLike, SparseMatrix } from '@/sklearn/types'\n\n/**\n  Isolation Forest Algorithm.\n\n  Return the anomaly score of each sample using the IsolationForest algorithm\n\n  The IsolationForest ‘isolates’ observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature.\n\n  Since recursive partitioning can be represented by a tree structure, the number of splittings required to isolate a sample is equivalent to the path length from the root node to the terminating node.\n\n  This path length, averaged over a forest of such random trees, is a measure of normality and our decision function.\n\n  Random partitioning produces noticeably shorter paths for anomalies. Hence, when a forest of random trees collectively produce shorter path lengths for particular samples, they are highly likely to be anomalies.\n\n  Read more in the [User Guide](../outlier_detection.html#isolation-forest).\n\n  [Python Reference](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html)\n */\nexport class IsolationForest {\n  id: string\n  opts: any\n\n  _py: PythonBridge\n  _isInitialized: boolean = false\n  _isDisposed: boolean = false\n\n  constructor(opts?: {\n    /**\n      The number of base estimators in the ensemble.\n\n      @defaultValue `100`\n     */\n    n_estimators?: number\n\n    /**\n      If int, then draw `max\\_samples` samples.\n\n      @defaultValue `'auto'`\n     */\n    max_samples?: 'auto' | number | number\n\n    /**\n      The amount of contamination of the data set, i.e. the proportion of outliers in the data set. Used when fitting to define the threshold on the scores of the samples.\n\n      @defaultValue `'auto'`\n     */\n    contamination?: 'auto' | number\n\n    /**\n      The number of features to draw from X to train each base estimator.\n\n      @defaultValue `1`\n     */\n    max_features?: number\n\n    /**\n      If `true`, individual trees are fit on random subsets of the training data sampled with replacement. If `false`, sampling without replacement is performed.\n\n      @defaultValue `false`\n     */\n    bootstrap?: boolean\n\n    /**\n      The number of jobs to run in parallel for both [`fit`](#sklearn.ensemble.IsolationForest.fit \"sklearn.ensemble.IsolationForest.fit\") and [`predict`](#sklearn.ensemble.IsolationForest.predict \"sklearn.ensemble.IsolationForest.predict\"). `undefined` means 1 unless in a [`joblib.parallel\\_backend`](https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend \"(in joblib v1.3.0.dev0)\") context. `\\-1` means using all processors. See [Glossary](../../glossary.html#term-n_jobs) for more details.\n     */\n    n_jobs?: number\n\n    /**\n      Controls the pseudo-randomness of the selection of the feature and split values for each branching step and each tree in the forest.\n\n      Pass an int for reproducible results across multiple function calls. See [Glossary](../../glossary.html#term-random_state).\n     */\n    random_state?: number\n\n    /**\n      Controls the verbosity of the tree building process.\n\n      @defaultValue `0`\n     */\n    verbose?: number\n\n    /**\n      When set to `true`, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new forest. See [the Glossary](../../glossary.html#term-warm_start).\n\n      @defaultValue `false`\n     */\n    warm_start?: boolean\n  }) {\n    this.id = `IsolationForest${crypto.randomUUID().split('-')[0]}`\n    this.opts = opts || {}\n  }\n\n  get py(): PythonBridge {\n    return this._py\n  }\n\n  set py(pythonBridge: PythonBridge) {\n    this._py = pythonBridge\n  }\n\n  /**\n    Initializes the underlying Python resources.\n\n    This instance is not usable until the `Promise` returned by `init()` resolves.\n   */\n  async init(py: PythonBridge): Promise<void> {\n    if (this._isDisposed) {\n      throw new Error('This IsolationForest instance has already been disposed')\n    }\n\n    if (this._isInitialized) {\n      return\n    }\n\n    if (!py) {\n      throw new Error('IsolationForest.init requires a PythonBridge instance')\n    }\n\n    this._py = py\n\n    await this._py.ex`\nimport numpy as np\nfrom sklearn.ensemble import IsolationForest\ntry: bridgeIsolationForest\nexcept NameError: bridgeIsolationForest = {}\n`\n\n    // set up constructor params\n    await this._py.ex`ctor_IsolationForest = {'n_estimators': ${\n      this.opts['n_estimators'] ?? undefined\n    }, 'max_samples': ${\n      this.opts['max_samples'] ?? undefined\n    }, 'contamination': ${\n      this.opts['contamination'] ?? undefined\n    }, 'max_features': ${\n      this.opts['max_features'] ?? undefined\n    }, 'bootstrap': ${this.opts['bootstrap'] ?? undefined}, 'n_jobs': ${\n      this.opts['n_jobs'] ?? undefined\n    }, 'random_state': ${this.opts['random_state'] ?? undefined}, 'verbose': ${\n      this.opts['verbose'] ?? undefined\n    }, 'warm_start': ${this.opts['warm_start'] ?? undefined}}\n\nctor_IsolationForest = {k: v for k, v in ctor_IsolationForest.items() if v is not None}`\n\n    await this._py\n      .ex`bridgeIsolationForest[${this.id}] = IsolationForest(**ctor_IsolationForest)`\n\n    this._isInitialized = true\n  }\n\n  /**\n    Disposes of the underlying Python resources.\n\n    Once `dispose()` is called, the instance is no longer usable.\n   */\n  async dispose() {\n    if (this._isDisposed) {\n      return\n    }\n\n    if (!this._isInitialized) {\n      return\n    }\n\n    await this._py.ex`del bridgeIsolationForest[${this.id}]`\n\n    this._isDisposed = true\n  }\n\n  /**\n    Average anomaly score of X of the base classifiers.\n\n    The anomaly score of an input sample is computed as the mean anomaly score of the trees in the forest.\n\n    The measure of normality of an observation given a tree is the depth of the leaf containing this observation, which is equivalent to the number of splittings required to isolate this point. In case of several observations n\\_left in the leaf, the average path length of a n\\_left samples isolation tree is added.\n   */\n  async decision_function(opts: {\n    /**\n      The input samples. Internally, it will be converted to `dtype=np.float32` and if a sparse matrix is provided to a sparse `csr\\_matrix`.\n     */\n    X?: ArrayLike | SparseMatrix[]\n  }): Promise<NDArray> {\n    if (this._isDisposed) {\n      throw new Error('This IsolationForest instance has already been disposed')\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'IsolationForest must call init() before decision_function()'\n      )\n    }\n\n    // set up method params\n    await this._py.ex`pms_IsolationForest_decision_function = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None}\n\npms_IsolationForest_decision_function = {k: v for k, v in pms_IsolationForest_decision_function.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_IsolationForest_decision_function = bridgeIsolationForest[${this.id}].decision_function(**pms_IsolationForest_decision_function)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_IsolationForest_decision_function.tolist() if hasattr(res_IsolationForest_decision_function, 'tolist') else res_IsolationForest_decision_function`\n  }\n\n  /**\n    Fit estimator.\n   */\n  async fit(opts: {\n    /**\n      The input samples. Use `dtype=np.float32` for maximum efficiency. Sparse matrices are also supported, use sparse `csc\\_matrix` for maximum efficiency.\n     */\n    X?: ArrayLike | SparseMatrix[]\n\n    /**\n      Not used, present for API consistency by convention.\n     */\n    y?: any\n\n    /**\n      Sample weights. If `undefined`, then samples are equally weighted.\n     */\n    sample_weight?: ArrayLike\n  }): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error('This IsolationForest instance has already been disposed')\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('IsolationForest must call init() before fit()')\n    }\n\n    // set up method params\n    await this._py.ex`pms_IsolationForest_fit = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None, 'y': ${\n      opts['y'] ?? undefined\n    }, 'sample_weight': np.array(${opts['sample_weight'] ?? undefined}) if ${\n      opts['sample_weight'] !== undefined\n    } else None}\n\npms_IsolationForest_fit = {k: v for k, v in pms_IsolationForest_fit.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_IsolationForest_fit = bridgeIsolationForest[${this.id}].fit(**pms_IsolationForest_fit)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_IsolationForest_fit.tolist() if hasattr(res_IsolationForest_fit, 'tolist') else res_IsolationForest_fit`\n  }\n\n  /**\n    Perform fit on X and returns labels for X.\n\n    Returns -1 for outliers and 1 for inliers.\n   */\n  async fit_predict(opts: {\n    /**\n      The input samples.\n     */\n    X?: ArrayLike | SparseMatrix[]\n\n    /**\n      Not used, present for API consistency by convention.\n     */\n    y?: any\n  }): Promise<NDArray> {\n    if (this._isDisposed) {\n      throw new Error('This IsolationForest instance has already been disposed')\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('IsolationForest must call init() before fit_predict()')\n    }\n\n    // set up method params\n    await this._py.ex`pms_IsolationForest_fit_predict = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None, 'y': ${opts['y'] ?? undefined}}\n\npms_IsolationForest_fit_predict = {k: v for k, v in pms_IsolationForest_fit_predict.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_IsolationForest_fit_predict = bridgeIsolationForest[${this.id}].fit_predict(**pms_IsolationForest_fit_predict)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_IsolationForest_fit_predict.tolist() if hasattr(res_IsolationForest_fit_predict, 'tolist') else res_IsolationForest_fit_predict`\n  }\n\n  /**\n    Predict if a particular sample is an outlier or not.\n   */\n  async predict(opts: {\n    /**\n      The input samples. Internally, it will be converted to `dtype=np.float32` and if a sparse matrix is provided to a sparse `csr\\_matrix`.\n     */\n    X?: ArrayLike | SparseMatrix[]\n  }): Promise<NDArray> {\n    if (this._isDisposed) {\n      throw new Error('This IsolationForest instance has already been disposed')\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('IsolationForest must call init() before predict()')\n    }\n\n    // set up method params\n    await this._py.ex`pms_IsolationForest_predict = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None}\n\npms_IsolationForest_predict = {k: v for k, v in pms_IsolationForest_predict.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_IsolationForest_predict = bridgeIsolationForest[${this.id}].predict(**pms_IsolationForest_predict)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_IsolationForest_predict.tolist() if hasattr(res_IsolationForest_predict, 'tolist') else res_IsolationForest_predict`\n  }\n\n  /**\n    Opposite of the anomaly score defined in the original paper.\n\n    The anomaly score of an input sample is computed as the mean anomaly score of the trees in the forest.\n\n    The measure of normality of an observation given a tree is the depth of the leaf containing this observation, which is equivalent to the number of splittings required to isolate this point. In case of several observations n\\_left in the leaf, the average path length of a n\\_left samples isolation tree is added.\n   */\n  async score_samples(opts: {\n    /**\n      The input samples.\n     */\n    X?: ArrayLike | SparseMatrix[]\n  }): Promise<NDArray> {\n    if (this._isDisposed) {\n      throw new Error('This IsolationForest instance has already been disposed')\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('IsolationForest must call init() before score_samples()')\n    }\n\n    // set up method params\n    await this._py.ex`pms_IsolationForest_score_samples = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None}\n\npms_IsolationForest_score_samples = {k: v for k, v in pms_IsolationForest_score_samples.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_IsolationForest_score_samples = bridgeIsolationForest[${this.id}].score_samples(**pms_IsolationForest_score_samples)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_IsolationForest_score_samples.tolist() if hasattr(res_IsolationForest_score_samples, 'tolist') else res_IsolationForest_score_samples`\n  }\n\n  /**\n    The child estimator template used to create the collection of fitted sub-estimators.\n   */\n  get estimator_(): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error('This IsolationForest instance has already been disposed')\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'IsolationForest must call init() before accessing estimator_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_IsolationForest_estimator_ = bridgeIsolationForest[${this.id}].estimator_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_IsolationForest_estimator_.tolist() if hasattr(attr_IsolationForest_estimator_, 'tolist') else attr_IsolationForest_estimator_`\n    })()\n  }\n\n  /**\n    The collection of fitted sub-estimators.\n   */\n  get estimators_(): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error('This IsolationForest instance has already been disposed')\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'IsolationForest must call init() before accessing estimators_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_IsolationForest_estimators_ = bridgeIsolationForest[${this.id}].estimators_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_IsolationForest_estimators_.tolist() if hasattr(attr_IsolationForest_estimators_, 'tolist') else attr_IsolationForest_estimators_`\n    })()\n  }\n\n  /**\n    The subset of drawn features for each base estimator.\n   */\n  get estimators_features_(): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error('This IsolationForest instance has already been disposed')\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'IsolationForest must call init() before accessing estimators_features_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_IsolationForest_estimators_features_ = bridgeIsolationForest[${this.id}].estimators_features_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_IsolationForest_estimators_features_.tolist() if hasattr(attr_IsolationForest_estimators_features_, 'tolist') else attr_IsolationForest_estimators_features_`\n    })()\n  }\n\n  /**\n    The actual number of samples.\n   */\n  get max_samples_(): Promise<number> {\n    if (this._isDisposed) {\n      throw new Error('This IsolationForest instance has already been disposed')\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'IsolationForest must call init() before accessing max_samples_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_IsolationForest_max_samples_ = bridgeIsolationForest[${this.id}].max_samples_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_IsolationForest_max_samples_.tolist() if hasattr(attr_IsolationForest_max_samples_, 'tolist') else attr_IsolationForest_max_samples_`\n    })()\n  }\n\n  /**\n    Offset used to define the decision function from the raw scores. We have the relation: `decision\\_function \\= score\\_samples \\- offset\\_`. `offset\\_` is defined as follows. When the contamination parameter is set to “auto”, the offset is equal to -0.5 as the scores of inliers are close to 0 and the scores of outliers are close to -1. When a contamination parameter different than “auto” is provided, the offset is defined in such a way we obtain the expected number of outliers (samples with decision function < 0) in training.\n   */\n  get offset_(): Promise<number> {\n    if (this._isDisposed) {\n      throw new Error('This IsolationForest instance has already been disposed')\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'IsolationForest must call init() before accessing offset_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_IsolationForest_offset_ = bridgeIsolationForest[${this.id}].offset_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_IsolationForest_offset_.tolist() if hasattr(attr_IsolationForest_offset_, 'tolist') else attr_IsolationForest_offset_`\n    })()\n  }\n\n  /**\n    Number of features seen during [fit](../../glossary.html#term-fit).\n   */\n  get n_features_in_(): Promise<number> {\n    if (this._isDisposed) {\n      throw new Error('This IsolationForest instance has already been disposed')\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'IsolationForest must call init() before accessing n_features_in_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_IsolationForest_n_features_in_ = bridgeIsolationForest[${this.id}].n_features_in_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_IsolationForest_n_features_in_.tolist() if hasattr(attr_IsolationForest_n_features_in_, 'tolist') else attr_IsolationForest_n_features_in_`\n    })()\n  }\n\n  /**\n    Names of features seen during [fit](../../glossary.html#term-fit). Defined only when `X` has feature names that are all strings.\n   */\n  get feature_names_in_(): Promise<NDArray> {\n    if (this._isDisposed) {\n      throw new Error('This IsolationForest instance has already been disposed')\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'IsolationForest must call init() before accessing feature_names_in_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_IsolationForest_feature_names_in_ = bridgeIsolationForest[${this.id}].feature_names_in_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_IsolationForest_feature_names_in_.tolist() if hasattr(attr_IsolationForest_feature_names_in_, 'tolist') else attr_IsolationForest_feature_names_in_`\n    })()\n  }\n}\n","/* eslint-disable */\n/* NOTE: This file is auto-generated. Do not edit it directly. */\n\nimport crypto from 'node:crypto'\n\nimport { PythonBridge, NDArray, ArrayLike, SparseMatrix } from '@/sklearn/types'\n\n/**\n  A random forest classifier.\n\n  A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is controlled with the `max\\_samples` parameter if `bootstrap=True` (default), otherwise the whole dataset is used to build each tree.\n\n  Read more in the [User Guide](../ensemble.html#forest).\n\n  [Python Reference](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n */\nexport class RandomForestClassifier {\n  id: string\n  opts: any\n\n  _py: PythonBridge\n  _isInitialized: boolean = false\n  _isDisposed: boolean = false\n\n  constructor(opts?: {\n    /**\n      The number of trees in the forest.\n\n      @defaultValue `100`\n     */\n    n_estimators?: number\n\n    /**\n      The function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “log\\_loss” and “entropy” both for the Shannon information gain, see [Mathematical formulation](../tree.html#tree-mathematical-formulation). Note: This parameter is tree-specific.\n\n      @defaultValue `'gini'`\n     */\n    criterion?: 'gini' | 'entropy' | 'log_loss'\n\n    /**\n      The maximum depth of the tree. If `undefined`, then nodes are expanded until all leaves are pure or until all leaves contain less than min\\_samples\\_split samples.\n     */\n    max_depth?: number\n\n    /**\n      The minimum number of samples required to split an internal node:\n\n      @defaultValue `2`\n     */\n    min_samples_split?: number\n\n    /**\n      The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least `min\\_samples\\_leaf` training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.\n\n      @defaultValue `1`\n     */\n    min_samples_leaf?: number\n\n    /**\n      The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample\\_weight is not provided.\n\n      @defaultValue `0`\n     */\n    min_weight_fraction_leaf?: number\n\n    /**\n      The number of features to consider when looking for the best split:\n\n      @defaultValue `'sqrt'`\n     */\n    max_features?: 'sqrt' | 'log2' | number | number\n\n    /**\n      Grow trees with `max\\_leaf\\_nodes` in best-first fashion. Best nodes are defined as relative reduction in impurity. If `undefined` then unlimited number of leaf nodes.\n     */\n    max_leaf_nodes?: number\n\n    /**\n      A node will be split if this split induces a decrease of the impurity greater than or equal to this value.\n\n      The weighted impurity decrease equation is the following:\n\n      @defaultValue `0`\n     */\n    min_impurity_decrease?: number\n\n    /**\n      Whether bootstrap samples are used when building trees. If `false`, the whole dataset is used to build each tree.\n\n      @defaultValue `true`\n     */\n    bootstrap?: boolean\n\n    /**\n      Whether to use out-of-bag samples to estimate the generalization score. Only available if bootstrap=`true`.\n\n      @defaultValue `false`\n     */\n    oob_score?: boolean\n\n    /**\n      The number of jobs to run in parallel. [`fit`](#sklearn.ensemble.RandomForestClassifier.fit \"sklearn.ensemble.RandomForestClassifier.fit\"), [`predict`](#sklearn.ensemble.RandomForestClassifier.predict \"sklearn.ensemble.RandomForestClassifier.predict\"), [`decision\\_path`](#sklearn.ensemble.RandomForestClassifier.decision_path \"sklearn.ensemble.RandomForestClassifier.decision_path\") and [`apply`](#sklearn.ensemble.RandomForestClassifier.apply \"sklearn.ensemble.RandomForestClassifier.apply\") are all parallelized over the trees. `undefined` means 1 unless in a [`joblib.parallel\\_backend`](https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend \"(in joblib v1.3.0.dev0)\") context. `\\-1` means using all processors. See [Glossary](../../glossary.html#term-n_jobs) for more details.\n     */\n    n_jobs?: number\n\n    /**\n      Controls both the randomness of the bootstrapping of the samples used when building trees (if `bootstrap=True`) and the sampling of the features to consider when looking for the best split at each node (if `max\\_features < n\\_features`). See [Glossary](../../glossary.html#term-random_state) for details.\n     */\n    random_state?: number\n\n    /**\n      Controls the verbosity when fitting and predicting.\n\n      @defaultValue `0`\n     */\n    verbose?: number\n\n    /**\n      When set to `true`, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new forest. See [Glossary](../../glossary.html#term-warm_start) and [Fitting additional weak-learners](../ensemble.html#gradient-boosting-warm-start) for details.\n\n      @defaultValue `false`\n     */\n    warm_start?: boolean\n\n    /**\n      Weights associated with classes in the form `{class\\_label: weight}`. If not given, all classes are supposed to have weight one. For multi-output problems, a list of dicts can be provided in the same order as the columns of y.\n\n      Note that for multioutput (including multilabel) weights should be defined for each class of every column in its own dict. For example, for four-class multilabel classification weights should be \\[{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}\\] instead of \\[{1:1}, {2:5}, {3:1}, {4:1}\\].\n\n      The “balanced” mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as `n\\_samples / (n\\_classes \\* np.bincount(y))`\n\n      The “balanced\\_subsample” mode is the same as “balanced” except that weights are computed based on the bootstrap sample for every tree grown.\n\n      For multi-output, the weights of each column of y will be multiplied.\n\n      Note that these weights will be multiplied with sample\\_weight (passed through the fit method) if sample\\_weight is specified.\n     */\n    class_weight?: 'balanced' | 'balanced_subsample' | any\n\n    /**\n      Complexity parameter used for Minimal Cost-Complexity Pruning. The subtree with the largest cost complexity that is smaller than `ccp\\_alpha` will be chosen. By default, no pruning is performed. See [Minimal Cost-Complexity Pruning](../tree.html#minimal-cost-complexity-pruning) for details.\n\n      @defaultValue `0`\n     */\n    ccp_alpha?: any\n\n    /**\n      If bootstrap is `true`, the number of samples to draw from X to train each base estimator.\n     */\n    max_samples?: number\n  }) {\n    this.id = `RandomForestClassifier${crypto.randomUUID().split('-')[0]}`\n    this.opts = opts || {}\n  }\n\n  get py(): PythonBridge {\n    return this._py\n  }\n\n  set py(pythonBridge: PythonBridge) {\n    this._py = pythonBridge\n  }\n\n  /**\n    Initializes the underlying Python resources.\n\n    This instance is not usable until the `Promise` returned by `init()` resolves.\n   */\n  async init(py: PythonBridge): Promise<void> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This RandomForestClassifier instance has already been disposed'\n      )\n    }\n\n    if (this._isInitialized) {\n      return\n    }\n\n    if (!py) {\n      throw new Error(\n        'RandomForestClassifier.init requires a PythonBridge instance'\n      )\n    }\n\n    this._py = py\n\n    await this._py.ex`\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\ntry: bridgeRandomForestClassifier\nexcept NameError: bridgeRandomForestClassifier = {}\n`\n\n    // set up constructor params\n    await this._py.ex`ctor_RandomForestClassifier = {'n_estimators': ${\n      this.opts['n_estimators'] ?? undefined\n    }, 'criterion': ${this.opts['criterion'] ?? undefined}, 'max_depth': ${\n      this.opts['max_depth'] ?? undefined\n    }, 'min_samples_split': ${\n      this.opts['min_samples_split'] ?? undefined\n    }, 'min_samples_leaf': ${\n      this.opts['min_samples_leaf'] ?? undefined\n    }, 'min_weight_fraction_leaf': ${\n      this.opts['min_weight_fraction_leaf'] ?? undefined\n    }, 'max_features': ${\n      this.opts['max_features'] ?? undefined\n    }, 'max_leaf_nodes': ${\n      this.opts['max_leaf_nodes'] ?? undefined\n    }, 'min_impurity_decrease': ${\n      this.opts['min_impurity_decrease'] ?? undefined\n    }, 'bootstrap': ${this.opts['bootstrap'] ?? undefined}, 'oob_score': ${\n      this.opts['oob_score'] ?? undefined\n    }, 'n_jobs': ${this.opts['n_jobs'] ?? undefined}, 'random_state': ${\n      this.opts['random_state'] ?? undefined\n    }, 'verbose': ${this.opts['verbose'] ?? undefined}, 'warm_start': ${\n      this.opts['warm_start'] ?? undefined\n    }, 'class_weight': ${\n      this.opts['class_weight'] ?? undefined\n    }, 'ccp_alpha': ${this.opts['ccp_alpha'] ?? undefined}, 'max_samples': ${\n      this.opts['max_samples'] ?? undefined\n    }}\n\nctor_RandomForestClassifier = {k: v for k, v in ctor_RandomForestClassifier.items() if v is not None}`\n\n    await this._py\n      .ex`bridgeRandomForestClassifier[${this.id}] = RandomForestClassifier(**ctor_RandomForestClassifier)`\n\n    this._isInitialized = true\n  }\n\n  /**\n    Disposes of the underlying Python resources.\n\n    Once `dispose()` is called, the instance is no longer usable.\n   */\n  async dispose() {\n    if (this._isDisposed) {\n      return\n    }\n\n    if (!this._isInitialized) {\n      return\n    }\n\n    await this._py.ex`del bridgeRandomForestClassifier[${this.id}]`\n\n    this._isDisposed = true\n  }\n\n  /**\n    Apply trees in the forest to X, return leaf indices.\n   */\n  async apply(opts: {\n    /**\n      The input samples. Internally, its dtype will be converted to `dtype=np.float32`. If a sparse matrix is provided, it will be converted into a sparse `csr\\_matrix`.\n     */\n    X?: ArrayLike | SparseMatrix[]\n  }): Promise<NDArray[]> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This RandomForestClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('RandomForestClassifier must call init() before apply()')\n    }\n\n    // set up method params\n    await this._py.ex`pms_RandomForestClassifier_apply = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None}\n\npms_RandomForestClassifier_apply = {k: v for k, v in pms_RandomForestClassifier_apply.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_RandomForestClassifier_apply = bridgeRandomForestClassifier[${this.id}].apply(**pms_RandomForestClassifier_apply)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_RandomForestClassifier_apply.tolist() if hasattr(res_RandomForestClassifier_apply, 'tolist') else res_RandomForestClassifier_apply`\n  }\n\n  /**\n    Return the decision path in the forest.\n   */\n  async decision_path(opts: {\n    /**\n      The input samples. Internally, its dtype will be converted to `dtype=np.float32`. If a sparse matrix is provided, it will be converted into a sparse `csr\\_matrix`.\n     */\n    X?: ArrayLike | SparseMatrix[]\n  }): Promise<SparseMatrix[]> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This RandomForestClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'RandomForestClassifier must call init() before decision_path()'\n      )\n    }\n\n    // set up method params\n    await this._py\n      .ex`pms_RandomForestClassifier_decision_path = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None}\n\npms_RandomForestClassifier_decision_path = {k: v for k, v in pms_RandomForestClassifier_decision_path.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_RandomForestClassifier_decision_path = bridgeRandomForestClassifier[${this.id}].decision_path(**pms_RandomForestClassifier_decision_path)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_RandomForestClassifier_decision_path.tolist() if hasattr(res_RandomForestClassifier_decision_path, 'tolist') else res_RandomForestClassifier_decision_path`\n  }\n\n  /**\n    Build a forest of trees from the training set (X, y).\n   */\n  async fit(opts: {\n    /**\n      The training input samples. Internally, its dtype will be converted to `dtype=np.float32`. If a sparse matrix is provided, it will be converted into a sparse `csc\\_matrix`.\n     */\n    X?: ArrayLike | SparseMatrix[]\n\n    /**\n      The target values (class labels in classification, real numbers in regression).\n     */\n    y?: ArrayLike\n\n    /**\n      Sample weights. If `undefined`, then samples are equally weighted. Splits that would create child nodes with net zero or negative weight are ignored while searching for a split in each node. In the case of classification, splits are also ignored if they would result in any single class carrying a negative weight in either child node.\n     */\n    sample_weight?: ArrayLike\n  }): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This RandomForestClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('RandomForestClassifier must call init() before fit()')\n    }\n\n    // set up method params\n    await this._py.ex`pms_RandomForestClassifier_fit = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None, 'y': np.array(${\n      opts['y'] ?? undefined\n    }) if ${opts['y'] !== undefined} else None, 'sample_weight': np.array(${\n      opts['sample_weight'] ?? undefined\n    }) if ${opts['sample_weight'] !== undefined} else None}\n\npms_RandomForestClassifier_fit = {k: v for k, v in pms_RandomForestClassifier_fit.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_RandomForestClassifier_fit = bridgeRandomForestClassifier[${this.id}].fit(**pms_RandomForestClassifier_fit)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_RandomForestClassifier_fit.tolist() if hasattr(res_RandomForestClassifier_fit, 'tolist') else res_RandomForestClassifier_fit`\n  }\n\n  /**\n    Predict class for X.\n\n    The predicted class of an input sample is a vote by the trees in the forest, weighted by their probability estimates. That is, the predicted class is the one with highest mean probability estimate across the trees.\n   */\n  async predict(opts: {\n    /**\n      The input samples. Internally, its dtype will be converted to `dtype=np.float32`. If a sparse matrix is provided, it will be converted into a sparse `csr\\_matrix`.\n     */\n    X?: ArrayLike | SparseMatrix[]\n  }): Promise<NDArray> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This RandomForestClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'RandomForestClassifier must call init() before predict()'\n      )\n    }\n\n    // set up method params\n    await this._py.ex`pms_RandomForestClassifier_predict = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None}\n\npms_RandomForestClassifier_predict = {k: v for k, v in pms_RandomForestClassifier_predict.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_RandomForestClassifier_predict = bridgeRandomForestClassifier[${this.id}].predict(**pms_RandomForestClassifier_predict)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_RandomForestClassifier_predict.tolist() if hasattr(res_RandomForestClassifier_predict, 'tolist') else res_RandomForestClassifier_predict`\n  }\n\n  /**\n    Predict class log-probabilities for X.\n\n    The predicted class log-probabilities of an input sample is computed as the log of the mean predicted class probabilities of the trees in the forest.\n   */\n  async predict_log_proba(opts: {\n    /**\n      The input samples. Internally, its dtype will be converted to `dtype=np.float32`. If a sparse matrix is provided, it will be converted into a sparse `csr\\_matrix`.\n     */\n    X?: ArrayLike | SparseMatrix[]\n  }): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This RandomForestClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'RandomForestClassifier must call init() before predict_log_proba()'\n      )\n    }\n\n    // set up method params\n    await this._py\n      .ex`pms_RandomForestClassifier_predict_log_proba = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None}\n\npms_RandomForestClassifier_predict_log_proba = {k: v for k, v in pms_RandomForestClassifier_predict_log_proba.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_RandomForestClassifier_predict_log_proba = bridgeRandomForestClassifier[${this.id}].predict_log_proba(**pms_RandomForestClassifier_predict_log_proba)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_RandomForestClassifier_predict_log_proba.tolist() if hasattr(res_RandomForestClassifier_predict_log_proba, 'tolist') else res_RandomForestClassifier_predict_log_proba`\n  }\n\n  /**\n    Predict class probabilities for X.\n\n    The predicted class probabilities of an input sample are computed as the mean predicted class probabilities of the trees in the forest. The class probability of a single tree is the fraction of samples of the same class in a leaf.\n   */\n  async predict_proba(opts: {\n    /**\n      The input samples. Internally, its dtype will be converted to `dtype=np.float32`. If a sparse matrix is provided, it will be converted into a sparse `csr\\_matrix`.\n     */\n    X?: ArrayLike | SparseMatrix[]\n  }): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This RandomForestClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'RandomForestClassifier must call init() before predict_proba()'\n      )\n    }\n\n    // set up method params\n    await this._py\n      .ex`pms_RandomForestClassifier_predict_proba = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None}\n\npms_RandomForestClassifier_predict_proba = {k: v for k, v in pms_RandomForestClassifier_predict_proba.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_RandomForestClassifier_predict_proba = bridgeRandomForestClassifier[${this.id}].predict_proba(**pms_RandomForestClassifier_predict_proba)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_RandomForestClassifier_predict_proba.tolist() if hasattr(res_RandomForestClassifier_predict_proba, 'tolist') else res_RandomForestClassifier_predict_proba`\n  }\n\n  /**\n    Return the mean accuracy on the given test data and labels.\n\n    In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted.\n   */\n  async score(opts: {\n    /**\n      Test samples.\n     */\n    X?: ArrayLike[]\n\n    /**\n      True labels for `X`.\n     */\n    y?: ArrayLike\n\n    /**\n      Sample weights.\n     */\n    sample_weight?: ArrayLike\n  }): Promise<number> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This RandomForestClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('RandomForestClassifier must call init() before score()')\n    }\n\n    // set up method params\n    await this._py.ex`pms_RandomForestClassifier_score = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None, 'y': np.array(${\n      opts['y'] ?? undefined\n    }) if ${opts['y'] !== undefined} else None, 'sample_weight': np.array(${\n      opts['sample_weight'] ?? undefined\n    }) if ${opts['sample_weight'] !== undefined} else None}\n\npms_RandomForestClassifier_score = {k: v for k, v in pms_RandomForestClassifier_score.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_RandomForestClassifier_score = bridgeRandomForestClassifier[${this.id}].score(**pms_RandomForestClassifier_score)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_RandomForestClassifier_score.tolist() if hasattr(res_RandomForestClassifier_score, 'tolist') else res_RandomForestClassifier_score`\n  }\n\n  /**\n    The child estimator template used to create the collection of fitted sub-estimators.\n   */\n  get estimator_(): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This RandomForestClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'RandomForestClassifier must call init() before accessing estimator_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_RandomForestClassifier_estimator_ = bridgeRandomForestClassifier[${this.id}].estimator_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_RandomForestClassifier_estimator_.tolist() if hasattr(attr_RandomForestClassifier_estimator_, 'tolist') else attr_RandomForestClassifier_estimator_`\n    })()\n  }\n\n  /**\n    The collection of fitted sub-estimators.\n   */\n  get estimators_(): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This RandomForestClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'RandomForestClassifier must call init() before accessing estimators_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_RandomForestClassifier_estimators_ = bridgeRandomForestClassifier[${this.id}].estimators_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_RandomForestClassifier_estimators_.tolist() if hasattr(attr_RandomForestClassifier_estimators_, 'tolist') else attr_RandomForestClassifier_estimators_`\n    })()\n  }\n\n  /**\n    The classes labels (single output problem), or a list of arrays of class labels (multi-output problem).\n   */\n  get classes_(): Promise<NDArray> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This RandomForestClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'RandomForestClassifier must call init() before accessing classes_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_RandomForestClassifier_classes_ = bridgeRandomForestClassifier[${this.id}].classes_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_RandomForestClassifier_classes_.tolist() if hasattr(attr_RandomForestClassifier_classes_, 'tolist') else attr_RandomForestClassifier_classes_`\n    })()\n  }\n\n  /**\n    The number of classes (single output problem), or a list containing the number of classes for each output (multi-output problem).\n   */\n  get n_classes_(): Promise<number | any[]> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This RandomForestClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'RandomForestClassifier must call init() before accessing n_classes_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_RandomForestClassifier_n_classes_ = bridgeRandomForestClassifier[${this.id}].n_classes_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_RandomForestClassifier_n_classes_.tolist() if hasattr(attr_RandomForestClassifier_n_classes_, 'tolist') else attr_RandomForestClassifier_n_classes_`\n    })()\n  }\n\n  /**\n    Number of features seen during [fit](../../glossary.html#term-fit).\n   */\n  get n_features_in_(): Promise<number> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This RandomForestClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'RandomForestClassifier must call init() before accessing n_features_in_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_RandomForestClassifier_n_features_in_ = bridgeRandomForestClassifier[${this.id}].n_features_in_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_RandomForestClassifier_n_features_in_.tolist() if hasattr(attr_RandomForestClassifier_n_features_in_, 'tolist') else attr_RandomForestClassifier_n_features_in_`\n    })()\n  }\n\n  /**\n    Names of features seen during [fit](../../glossary.html#term-fit). Defined only when `X` has feature names that are all strings.\n   */\n  get feature_names_in_(): Promise<NDArray> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This RandomForestClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'RandomForestClassifier must call init() before accessing feature_names_in_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_RandomForestClassifier_feature_names_in_ = bridgeRandomForestClassifier[${this.id}].feature_names_in_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_RandomForestClassifier_feature_names_in_.tolist() if hasattr(attr_RandomForestClassifier_feature_names_in_, 'tolist') else attr_RandomForestClassifier_feature_names_in_`\n    })()\n  }\n\n  /**\n    The number of outputs when `fit` is performed.\n   */\n  get n_outputs_(): Promise<number> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This RandomForestClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'RandomForestClassifier must call init() before accessing n_outputs_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_RandomForestClassifier_n_outputs_ = bridgeRandomForestClassifier[${this.id}].n_outputs_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_RandomForestClassifier_n_outputs_.tolist() if hasattr(attr_RandomForestClassifier_n_outputs_, 'tolist') else attr_RandomForestClassifier_n_outputs_`\n    })()\n  }\n\n  /**\n    Score of the training dataset obtained using an out-of-bag estimate. This attribute exists only when `oob\\_score` is `true`.\n   */\n  get oob_score_(): Promise<number> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This RandomForestClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'RandomForestClassifier must call init() before accessing oob_score_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_RandomForestClassifier_oob_score_ = bridgeRandomForestClassifier[${this.id}].oob_score_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_RandomForestClassifier_oob_score_.tolist() if hasattr(attr_RandomForestClassifier_oob_score_, 'tolist') else attr_RandomForestClassifier_oob_score_`\n    })()\n  }\n\n  /**\n    Decision function computed with out-of-bag estimate on the training set. If n\\_estimators is small it might be possible that a data point was never left out during the bootstrap. In this case, `oob\\_decision\\_function\\_` might contain NaN. This attribute exists only when `oob\\_score` is `true`.\n   */\n  get oob_decision_function_(): Promise<NDArray[]> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This RandomForestClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'RandomForestClassifier must call init() before accessing oob_decision_function_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_RandomForestClassifier_oob_decision_function_ = bridgeRandomForestClassifier[${this.id}].oob_decision_function_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_RandomForestClassifier_oob_decision_function_.tolist() if hasattr(attr_RandomForestClassifier_oob_decision_function_, 'tolist') else attr_RandomForestClassifier_oob_decision_function_`\n    })()\n  }\n}\n","/* eslint-disable */\n/* NOTE: This file is auto-generated. Do not edit it directly. */\n\nimport crypto from 'node:crypto'\n\nimport { PythonBridge, NDArray, ArrayLike, SparseMatrix } from '@/sklearn/types'\n\n/**\n  A random forest regressor.\n\n  A random forest is a meta estimator that fits a number of classifying decision trees on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is controlled with the `max\\_samples` parameter if `bootstrap=True` (default), otherwise the whole dataset is used to build each tree.\n\n  Read more in the [User Guide](../ensemble.html#forest).\n\n  [Python Reference](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)\n */\nexport class RandomForestRegressor {\n  id: string\n  opts: any\n\n  _py: PythonBridge\n  _isInitialized: boolean = false\n  _isDisposed: boolean = false\n\n  constructor(opts?: {\n    /**\n      The number of trees in the forest.\n\n      @defaultValue `100`\n     */\n    n_estimators?: number\n\n    /**\n      The function to measure the quality of a split. Supported criteria are “squared\\_error” for the mean squared error, which is equal to variance reduction as feature selection criterion and minimizes the L2 loss using the mean of each terminal node, “friedman\\_mse”, which uses mean squared error with Friedman’s improvement score for potential splits, “absolute\\_error” for the mean absolute error, which minimizes the L1 loss using the median of each terminal node, and “poisson” which uses reduction in Poisson deviance to find splits. Training using “absolute\\_error” is significantly slower than when using “squared\\_error”.\n\n      @defaultValue `'squared_error'`\n     */\n    criterion?: 'squared_error' | 'absolute_error' | 'friedman_mse' | 'poisson'\n\n    /**\n      The maximum depth of the tree. If `undefined`, then nodes are expanded until all leaves are pure or until all leaves contain less than min\\_samples\\_split samples.\n     */\n    max_depth?: number\n\n    /**\n      The minimum number of samples required to split an internal node:\n\n      @defaultValue `2`\n     */\n    min_samples_split?: number\n\n    /**\n      The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least `min\\_samples\\_leaf` training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.\n\n      @defaultValue `1`\n     */\n    min_samples_leaf?: number\n\n    /**\n      The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample\\_weight is not provided.\n\n      @defaultValue `0`\n     */\n    min_weight_fraction_leaf?: number\n\n    /**\n      The number of features to consider when looking for the best split:\n\n      @defaultValue `1`\n     */\n    max_features?: 'sqrt' | 'log2' | number | number\n\n    /**\n      Grow trees with `max\\_leaf\\_nodes` in best-first fashion. Best nodes are defined as relative reduction in impurity. If `undefined` then unlimited number of leaf nodes.\n     */\n    max_leaf_nodes?: number\n\n    /**\n      A node will be split if this split induces a decrease of the impurity greater than or equal to this value.\n\n      The weighted impurity decrease equation is the following:\n\n      @defaultValue `0`\n     */\n    min_impurity_decrease?: number\n\n    /**\n      Whether bootstrap samples are used when building trees. If `false`, the whole dataset is used to build each tree.\n\n      @defaultValue `true`\n     */\n    bootstrap?: boolean\n\n    /**\n      Whether to use out-of-bag samples to estimate the generalization score. Only available if bootstrap=`true`.\n\n      @defaultValue `false`\n     */\n    oob_score?: boolean\n\n    /**\n      The number of jobs to run in parallel. [`fit`](#sklearn.ensemble.RandomForestRegressor.fit \"sklearn.ensemble.RandomForestRegressor.fit\"), [`predict`](#sklearn.ensemble.RandomForestRegressor.predict \"sklearn.ensemble.RandomForestRegressor.predict\"), [`decision\\_path`](#sklearn.ensemble.RandomForestRegressor.decision_path \"sklearn.ensemble.RandomForestRegressor.decision_path\") and [`apply`](#sklearn.ensemble.RandomForestRegressor.apply \"sklearn.ensemble.RandomForestRegressor.apply\") are all parallelized over the trees. `undefined` means 1 unless in a [`joblib.parallel\\_backend`](https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend \"(in joblib v1.3.0.dev0)\") context. `\\-1` means using all processors. See [Glossary](../../glossary.html#term-n_jobs) for more details.\n     */\n    n_jobs?: number\n\n    /**\n      Controls both the randomness of the bootstrapping of the samples used when building trees (if `bootstrap=True`) and the sampling of the features to consider when looking for the best split at each node (if `max\\_features < n\\_features`). See [Glossary](../../glossary.html#term-random_state) for details.\n     */\n    random_state?: number\n\n    /**\n      Controls the verbosity when fitting and predicting.\n\n      @defaultValue `0`\n     */\n    verbose?: number\n\n    /**\n      When set to `true`, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new forest. See [Glossary](../../glossary.html#term-warm_start) and [Fitting additional weak-learners](../ensemble.html#gradient-boosting-warm-start) for details.\n\n      @defaultValue `false`\n     */\n    warm_start?: boolean\n\n    /**\n      Complexity parameter used for Minimal Cost-Complexity Pruning. The subtree with the largest cost complexity that is smaller than `ccp\\_alpha` will be chosen. By default, no pruning is performed. See [Minimal Cost-Complexity Pruning](../tree.html#minimal-cost-complexity-pruning) for details.\n\n      @defaultValue `0`\n     */\n    ccp_alpha?: any\n\n    /**\n      If bootstrap is `true`, the number of samples to draw from X to train each base estimator.\n     */\n    max_samples?: number\n  }) {\n    this.id = `RandomForestRegressor${crypto.randomUUID().split('-')[0]}`\n    this.opts = opts || {}\n  }\n\n  get py(): PythonBridge {\n    return this._py\n  }\n\n  set py(pythonBridge: PythonBridge) {\n    this._py = pythonBridge\n  }\n\n  /**\n    Initializes the underlying Python resources.\n\n    This instance is not usable until the `Promise` returned by `init()` resolves.\n   */\n  async init(py: PythonBridge): Promise<void> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This RandomForestRegressor instance has already been disposed'\n      )\n    }\n\n    if (this._isInitialized) {\n      return\n    }\n\n    if (!py) {\n      throw new Error(\n        'RandomForestRegressor.init requires a PythonBridge instance'\n      )\n    }\n\n    this._py = py\n\n    await this._py.ex`\nimport numpy as np\nfrom sklearn.ensemble import RandomForestRegressor\ntry: bridgeRandomForestRegressor\nexcept NameError: bridgeRandomForestRegressor = {}\n`\n\n    // set up constructor params\n    await this._py.ex`ctor_RandomForestRegressor = {'n_estimators': ${\n      this.opts['n_estimators'] ?? undefined\n    }, 'criterion': ${this.opts['criterion'] ?? undefined}, 'max_depth': ${\n      this.opts['max_depth'] ?? undefined\n    }, 'min_samples_split': ${\n      this.opts['min_samples_split'] ?? undefined\n    }, 'min_samples_leaf': ${\n      this.opts['min_samples_leaf'] ?? undefined\n    }, 'min_weight_fraction_leaf': ${\n      this.opts['min_weight_fraction_leaf'] ?? undefined\n    }, 'max_features': ${\n      this.opts['max_features'] ?? undefined\n    }, 'max_leaf_nodes': ${\n      this.opts['max_leaf_nodes'] ?? undefined\n    }, 'min_impurity_decrease': ${\n      this.opts['min_impurity_decrease'] ?? undefined\n    }, 'bootstrap': ${this.opts['bootstrap'] ?? undefined}, 'oob_score': ${\n      this.opts['oob_score'] ?? undefined\n    }, 'n_jobs': ${this.opts['n_jobs'] ?? undefined}, 'random_state': ${\n      this.opts['random_state'] ?? undefined\n    }, 'verbose': ${this.opts['verbose'] ?? undefined}, 'warm_start': ${\n      this.opts['warm_start'] ?? undefined\n    }, 'ccp_alpha': ${this.opts['ccp_alpha'] ?? undefined}, 'max_samples': ${\n      this.opts['max_samples'] ?? undefined\n    }}\n\nctor_RandomForestRegressor = {k: v for k, v in ctor_RandomForestRegressor.items() if v is not None}`\n\n    await this._py\n      .ex`bridgeRandomForestRegressor[${this.id}] = RandomForestRegressor(**ctor_RandomForestRegressor)`\n\n    this._isInitialized = true\n  }\n\n  /**\n    Disposes of the underlying Python resources.\n\n    Once `dispose()` is called, the instance is no longer usable.\n   */\n  async dispose() {\n    if (this._isDisposed) {\n      return\n    }\n\n    if (!this._isInitialized) {\n      return\n    }\n\n    await this._py.ex`del bridgeRandomForestRegressor[${this.id}]`\n\n    this._isDisposed = true\n  }\n\n  /**\n    Apply trees in the forest to X, return leaf indices.\n   */\n  async apply(opts: {\n    /**\n      The input samples. Internally, its dtype will be converted to `dtype=np.float32`. If a sparse matrix is provided, it will be converted into a sparse `csr\\_matrix`.\n     */\n    X?: ArrayLike | SparseMatrix[]\n  }): Promise<NDArray[]> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This RandomForestRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('RandomForestRegressor must call init() before apply()')\n    }\n\n    // set up method params\n    await this._py.ex`pms_RandomForestRegressor_apply = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None}\n\npms_RandomForestRegressor_apply = {k: v for k, v in pms_RandomForestRegressor_apply.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_RandomForestRegressor_apply = bridgeRandomForestRegressor[${this.id}].apply(**pms_RandomForestRegressor_apply)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_RandomForestRegressor_apply.tolist() if hasattr(res_RandomForestRegressor_apply, 'tolist') else res_RandomForestRegressor_apply`\n  }\n\n  /**\n    Return the decision path in the forest.\n   */\n  async decision_path(opts: {\n    /**\n      The input samples. Internally, its dtype will be converted to `dtype=np.float32`. If a sparse matrix is provided, it will be converted into a sparse `csr\\_matrix`.\n     */\n    X?: ArrayLike | SparseMatrix[]\n  }): Promise<SparseMatrix[]> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This RandomForestRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'RandomForestRegressor must call init() before decision_path()'\n      )\n    }\n\n    // set up method params\n    await this._py\n      .ex`pms_RandomForestRegressor_decision_path = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None}\n\npms_RandomForestRegressor_decision_path = {k: v for k, v in pms_RandomForestRegressor_decision_path.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_RandomForestRegressor_decision_path = bridgeRandomForestRegressor[${this.id}].decision_path(**pms_RandomForestRegressor_decision_path)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_RandomForestRegressor_decision_path.tolist() if hasattr(res_RandomForestRegressor_decision_path, 'tolist') else res_RandomForestRegressor_decision_path`\n  }\n\n  /**\n    Build a forest of trees from the training set (X, y).\n   */\n  async fit(opts: {\n    /**\n      The training input samples. Internally, its dtype will be converted to `dtype=np.float32`. If a sparse matrix is provided, it will be converted into a sparse `csc\\_matrix`.\n     */\n    X?: ArrayLike | SparseMatrix[]\n\n    /**\n      The target values (class labels in classification, real numbers in regression).\n     */\n    y?: ArrayLike\n\n    /**\n      Sample weights. If `undefined`, then samples are equally weighted. Splits that would create child nodes with net zero or negative weight are ignored while searching for a split in each node. In the case of classification, splits are also ignored if they would result in any single class carrying a negative weight in either child node.\n     */\n    sample_weight?: ArrayLike\n  }): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This RandomForestRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('RandomForestRegressor must call init() before fit()')\n    }\n\n    // set up method params\n    await this._py.ex`pms_RandomForestRegressor_fit = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None, 'y': np.array(${\n      opts['y'] ?? undefined\n    }) if ${opts['y'] !== undefined} else None, 'sample_weight': np.array(${\n      opts['sample_weight'] ?? undefined\n    }) if ${opts['sample_weight'] !== undefined} else None}\n\npms_RandomForestRegressor_fit = {k: v for k, v in pms_RandomForestRegressor_fit.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_RandomForestRegressor_fit = bridgeRandomForestRegressor[${this.id}].fit(**pms_RandomForestRegressor_fit)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_RandomForestRegressor_fit.tolist() if hasattr(res_RandomForestRegressor_fit, 'tolist') else res_RandomForestRegressor_fit`\n  }\n\n  /**\n    Predict regression target for X.\n\n    The predicted regression target of an input sample is computed as the mean predicted regression targets of the trees in the forest.\n   */\n  async predict(opts: {\n    /**\n      The input samples. Internally, its dtype will be converted to `dtype=np.float32`. If a sparse matrix is provided, it will be converted into a sparse `csr\\_matrix`.\n     */\n    X?: ArrayLike | SparseMatrix[]\n  }): Promise<NDArray> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This RandomForestRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('RandomForestRegressor must call init() before predict()')\n    }\n\n    // set up method params\n    await this._py.ex`pms_RandomForestRegressor_predict = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None}\n\npms_RandomForestRegressor_predict = {k: v for k, v in pms_RandomForestRegressor_predict.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_RandomForestRegressor_predict = bridgeRandomForestRegressor[${this.id}].predict(**pms_RandomForestRegressor_predict)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_RandomForestRegressor_predict.tolist() if hasattr(res_RandomForestRegressor_predict, 'tolist') else res_RandomForestRegressor_predict`\n  }\n\n  /**\n    Return the coefficient of determination of the prediction.\n\n    The coefficient of determination \\\\(R^2\\\\) is defined as \\\\((1 - \\\\frac{u}{v})\\\\), where \\\\(u\\\\) is the residual sum of squares `((y\\_true \\- y\\_pred)\\*\\* 2).sum()` and \\\\(v\\\\) is the total sum of squares `((y\\_true \\- y\\_true.mean()) \\*\\* 2).sum()`. The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of `y`, disregarding the input features, would get a \\\\(R^2\\\\) score of 0.0.\n   */\n  async score(opts: {\n    /**\n      Test samples. For some estimators this may be a precomputed kernel matrix or a list of generic objects instead with shape `(n\\_samples, n\\_samples\\_fitted)`, where `n\\_samples\\_fitted` is the number of samples used in the fitting for the estimator.\n     */\n    X?: ArrayLike[]\n\n    /**\n      True values for `X`.\n     */\n    y?: ArrayLike\n\n    /**\n      Sample weights.\n     */\n    sample_weight?: ArrayLike\n  }): Promise<number> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This RandomForestRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('RandomForestRegressor must call init() before score()')\n    }\n\n    // set up method params\n    await this._py.ex`pms_RandomForestRegressor_score = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None, 'y': np.array(${\n      opts['y'] ?? undefined\n    }) if ${opts['y'] !== undefined} else None, 'sample_weight': np.array(${\n      opts['sample_weight'] ?? undefined\n    }) if ${opts['sample_weight'] !== undefined} else None}\n\npms_RandomForestRegressor_score = {k: v for k, v in pms_RandomForestRegressor_score.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_RandomForestRegressor_score = bridgeRandomForestRegressor[${this.id}].score(**pms_RandomForestRegressor_score)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_RandomForestRegressor_score.tolist() if hasattr(res_RandomForestRegressor_score, 'tolist') else res_RandomForestRegressor_score`\n  }\n\n  /**\n    The child estimator template used to create the collection of fitted sub-estimators.\n   */\n  get estimator_(): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This RandomForestRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'RandomForestRegressor must call init() before accessing estimator_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_RandomForestRegressor_estimator_ = bridgeRandomForestRegressor[${this.id}].estimator_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_RandomForestRegressor_estimator_.tolist() if hasattr(attr_RandomForestRegressor_estimator_, 'tolist') else attr_RandomForestRegressor_estimator_`\n    })()\n  }\n\n  /**\n    The collection of fitted sub-estimators.\n   */\n  get estimators_(): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This RandomForestRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'RandomForestRegressor must call init() before accessing estimators_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_RandomForestRegressor_estimators_ = bridgeRandomForestRegressor[${this.id}].estimators_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_RandomForestRegressor_estimators_.tolist() if hasattr(attr_RandomForestRegressor_estimators_, 'tolist') else attr_RandomForestRegressor_estimators_`\n    })()\n  }\n\n  /**\n    Number of features seen during [fit](../../glossary.html#term-fit).\n   */\n  get n_features_in_(): Promise<number> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This RandomForestRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'RandomForestRegressor must call init() before accessing n_features_in_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_RandomForestRegressor_n_features_in_ = bridgeRandomForestRegressor[${this.id}].n_features_in_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_RandomForestRegressor_n_features_in_.tolist() if hasattr(attr_RandomForestRegressor_n_features_in_, 'tolist') else attr_RandomForestRegressor_n_features_in_`\n    })()\n  }\n\n  /**\n    Names of features seen during [fit](../../glossary.html#term-fit). Defined only when `X` has feature names that are all strings.\n   */\n  get feature_names_in_(): Promise<NDArray> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This RandomForestRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'RandomForestRegressor must call init() before accessing feature_names_in_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_RandomForestRegressor_feature_names_in_ = bridgeRandomForestRegressor[${this.id}].feature_names_in_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_RandomForestRegressor_feature_names_in_.tolist() if hasattr(attr_RandomForestRegressor_feature_names_in_, 'tolist') else attr_RandomForestRegressor_feature_names_in_`\n    })()\n  }\n\n  /**\n    The number of outputs when `fit` is performed.\n   */\n  get n_outputs_(): Promise<number> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This RandomForestRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'RandomForestRegressor must call init() before accessing n_outputs_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_RandomForestRegressor_n_outputs_ = bridgeRandomForestRegressor[${this.id}].n_outputs_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_RandomForestRegressor_n_outputs_.tolist() if hasattr(attr_RandomForestRegressor_n_outputs_, 'tolist') else attr_RandomForestRegressor_n_outputs_`\n    })()\n  }\n\n  /**\n    Score of the training dataset obtained using an out-of-bag estimate. This attribute exists only when `oob\\_score` is `true`.\n   */\n  get oob_score_(): Promise<number> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This RandomForestRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'RandomForestRegressor must call init() before accessing oob_score_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_RandomForestRegressor_oob_score_ = bridgeRandomForestRegressor[${this.id}].oob_score_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_RandomForestRegressor_oob_score_.tolist() if hasattr(attr_RandomForestRegressor_oob_score_, 'tolist') else attr_RandomForestRegressor_oob_score_`\n    })()\n  }\n\n  /**\n    Prediction computed with out-of-bag estimate on the training set. This attribute exists only when `oob\\_score` is `true`.\n   */\n  get oob_prediction_(): Promise<NDArray> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This RandomForestRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'RandomForestRegressor must call init() before accessing oob_prediction_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_RandomForestRegressor_oob_prediction_ = bridgeRandomForestRegressor[${this.id}].oob_prediction_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_RandomForestRegressor_oob_prediction_.tolist() if hasattr(attr_RandomForestRegressor_oob_prediction_, 'tolist') else attr_RandomForestRegressor_oob_prediction_`\n    })()\n  }\n}\n","/* eslint-disable */\n/* NOTE: This file is auto-generated. Do not edit it directly. */\n\nimport crypto from 'node:crypto'\n\nimport { PythonBridge, NDArray, ArrayLike, SparseMatrix } from '@/sklearn/types'\n\n/**\n  An ensemble of totally random trees.\n\n  An unsupervised transformation of a dataset to a high-dimensional sparse representation. A datapoint is coded according to which leaf of each tree it is sorted into. Using a one-hot encoding of the leaves, this leads to a binary coding with as many ones as there are trees in the forest.\n\n  The dimensionality of the resulting representation is `n\\_out <= n\\_estimators \\* max\\_leaf\\_nodes`. If `max\\_leaf\\_nodes \\== None`, the number of leaf nodes is at most `n\\_estimators \\* 2 \\*\\* max\\_depth`.\n\n  Read more in the [User Guide](../ensemble.html#random-trees-embedding).\n\n  [Python Reference](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomTreesEmbedding.html)\n */\nexport class RandomTreesEmbedding {\n  id: string\n  opts: any\n\n  _py: PythonBridge\n  _isInitialized: boolean = false\n  _isDisposed: boolean = false\n\n  constructor(opts?: {\n    /**\n      Number of trees in the forest.\n\n      @defaultValue `100`\n     */\n    n_estimators?: number\n\n    /**\n      The maximum depth of each tree. If `undefined`, then nodes are expanded until all leaves are pure or until all leaves contain less than min\\_samples\\_split samples.\n\n      @defaultValue `5`\n     */\n    max_depth?: number\n\n    /**\n      The minimum number of samples required to split an internal node:\n\n      @defaultValue `2`\n     */\n    min_samples_split?: number\n\n    /**\n      The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least `min\\_samples\\_leaf` training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.\n\n      @defaultValue `1`\n     */\n    min_samples_leaf?: number\n\n    /**\n      The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample\\_weight is not provided.\n\n      @defaultValue `0`\n     */\n    min_weight_fraction_leaf?: number\n\n    /**\n      Grow trees with `max\\_leaf\\_nodes` in best-first fashion. Best nodes are defined as relative reduction in impurity. If `undefined` then unlimited number of leaf nodes.\n     */\n    max_leaf_nodes?: number\n\n    /**\n      A node will be split if this split induces a decrease of the impurity greater than or equal to this value.\n\n      The weighted impurity decrease equation is the following:\n\n      @defaultValue `0`\n     */\n    min_impurity_decrease?: number\n\n    /**\n      Whether or not to return a sparse CSR matrix, as default behavior, or to return a dense array compatible with dense pipeline operators.\n\n      @defaultValue `true`\n     */\n    sparse_output?: boolean\n\n    /**\n      The number of jobs to run in parallel. [`fit`](#sklearn.ensemble.RandomTreesEmbedding.fit \"sklearn.ensemble.RandomTreesEmbedding.fit\"), [`transform`](#sklearn.ensemble.RandomTreesEmbedding.transform \"sklearn.ensemble.RandomTreesEmbedding.transform\"), [`decision\\_path`](#sklearn.ensemble.RandomTreesEmbedding.decision_path \"sklearn.ensemble.RandomTreesEmbedding.decision_path\") and [`apply`](#sklearn.ensemble.RandomTreesEmbedding.apply \"sklearn.ensemble.RandomTreesEmbedding.apply\") are all parallelized over the trees. `undefined` means 1 unless in a [`joblib.parallel\\_backend`](https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend \"(in joblib v1.3.0.dev0)\") context. `\\-1` means using all processors. See [Glossary](../../glossary.html#term-n_jobs) for more details.\n     */\n    n_jobs?: number\n\n    /**\n      Controls the generation of the random `y` used to fit the trees and the draw of the splits for each feature at the trees’ nodes. See [Glossary](../../glossary.html#term-random_state) for details.\n     */\n    random_state?: number\n\n    /**\n      Controls the verbosity when fitting and predicting.\n\n      @defaultValue `0`\n     */\n    verbose?: number\n\n    /**\n      When set to `true`, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new forest. See [Glossary](../../glossary.html#term-warm_start) and [Fitting additional weak-learners](../ensemble.html#gradient-boosting-warm-start) for details.\n\n      @defaultValue `false`\n     */\n    warm_start?: boolean\n  }) {\n    this.id = `RandomTreesEmbedding${crypto.randomUUID().split('-')[0]}`\n    this.opts = opts || {}\n  }\n\n  get py(): PythonBridge {\n    return this._py\n  }\n\n  set py(pythonBridge: PythonBridge) {\n    this._py = pythonBridge\n  }\n\n  /**\n    Initializes the underlying Python resources.\n\n    This instance is not usable until the `Promise` returned by `init()` resolves.\n   */\n  async init(py: PythonBridge): Promise<void> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This RandomTreesEmbedding instance has already been disposed'\n      )\n    }\n\n    if (this._isInitialized) {\n      return\n    }\n\n    if (!py) {\n      throw new Error(\n        'RandomTreesEmbedding.init requires a PythonBridge instance'\n      )\n    }\n\n    this._py = py\n\n    await this._py.ex`\nimport numpy as np\nfrom sklearn.ensemble import RandomTreesEmbedding\ntry: bridgeRandomTreesEmbedding\nexcept NameError: bridgeRandomTreesEmbedding = {}\n`\n\n    // set up constructor params\n    await this._py.ex`ctor_RandomTreesEmbedding = {'n_estimators': ${\n      this.opts['n_estimators'] ?? undefined\n    }, 'max_depth': ${\n      this.opts['max_depth'] ?? undefined\n    }, 'min_samples_split': ${\n      this.opts['min_samples_split'] ?? undefined\n    }, 'min_samples_leaf': ${\n      this.opts['min_samples_leaf'] ?? undefined\n    }, 'min_weight_fraction_leaf': ${\n      this.opts['min_weight_fraction_leaf'] ?? undefined\n    }, 'max_leaf_nodes': ${\n      this.opts['max_leaf_nodes'] ?? undefined\n    }, 'min_impurity_decrease': ${\n      this.opts['min_impurity_decrease'] ?? undefined\n    }, 'sparse_output': ${this.opts['sparse_output'] ?? undefined}, 'n_jobs': ${\n      this.opts['n_jobs'] ?? undefined\n    }, 'random_state': ${this.opts['random_state'] ?? undefined}, 'verbose': ${\n      this.opts['verbose'] ?? undefined\n    }, 'warm_start': ${this.opts['warm_start'] ?? undefined}}\n\nctor_RandomTreesEmbedding = {k: v for k, v in ctor_RandomTreesEmbedding.items() if v is not None}`\n\n    await this._py\n      .ex`bridgeRandomTreesEmbedding[${this.id}] = RandomTreesEmbedding(**ctor_RandomTreesEmbedding)`\n\n    this._isInitialized = true\n  }\n\n  /**\n    Disposes of the underlying Python resources.\n\n    Once `dispose()` is called, the instance is no longer usable.\n   */\n  async dispose() {\n    if (this._isDisposed) {\n      return\n    }\n\n    if (!this._isInitialized) {\n      return\n    }\n\n    await this._py.ex`del bridgeRandomTreesEmbedding[${this.id}]`\n\n    this._isDisposed = true\n  }\n\n  /**\n    Apply trees in the forest to X, return leaf indices.\n   */\n  async apply(opts: {\n    /**\n      The input samples. Internally, its dtype will be converted to `dtype=np.float32`. If a sparse matrix is provided, it will be converted into a sparse `csr\\_matrix`.\n     */\n    X?: ArrayLike | SparseMatrix[]\n  }): Promise<NDArray[]> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This RandomTreesEmbedding instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('RandomTreesEmbedding must call init() before apply()')\n    }\n\n    // set up method params\n    await this._py.ex`pms_RandomTreesEmbedding_apply = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None}\n\npms_RandomTreesEmbedding_apply = {k: v for k, v in pms_RandomTreesEmbedding_apply.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_RandomTreesEmbedding_apply = bridgeRandomTreesEmbedding[${this.id}].apply(**pms_RandomTreesEmbedding_apply)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_RandomTreesEmbedding_apply.tolist() if hasattr(res_RandomTreesEmbedding_apply, 'tolist') else res_RandomTreesEmbedding_apply`\n  }\n\n  /**\n    Return the decision path in the forest.\n   */\n  async decision_path(opts: {\n    /**\n      The input samples. Internally, its dtype will be converted to `dtype=np.float32`. If a sparse matrix is provided, it will be converted into a sparse `csr\\_matrix`.\n     */\n    X?: ArrayLike | SparseMatrix[]\n  }): Promise<SparseMatrix[]> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This RandomTreesEmbedding instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'RandomTreesEmbedding must call init() before decision_path()'\n      )\n    }\n\n    // set up method params\n    await this._py.ex`pms_RandomTreesEmbedding_decision_path = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None}\n\npms_RandomTreesEmbedding_decision_path = {k: v for k, v in pms_RandomTreesEmbedding_decision_path.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_RandomTreesEmbedding_decision_path = bridgeRandomTreesEmbedding[${this.id}].decision_path(**pms_RandomTreesEmbedding_decision_path)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_RandomTreesEmbedding_decision_path.tolist() if hasattr(res_RandomTreesEmbedding_decision_path, 'tolist') else res_RandomTreesEmbedding_decision_path`\n  }\n\n  /**\n    Fit estimator.\n   */\n  async fit(opts: {\n    /**\n      The input samples. Use `dtype=np.float32` for maximum efficiency. Sparse matrices are also supported, use sparse `csc\\_matrix` for maximum efficiency.\n     */\n    X?: ArrayLike | SparseMatrix[]\n\n    /**\n      Not used, present for API consistency by convention.\n     */\n    y?: any\n\n    /**\n      Sample weights. If `undefined`, then samples are equally weighted. Splits that would create child nodes with net zero or negative weight are ignored while searching for a split in each node. In the case of classification, splits are also ignored if they would result in any single class carrying a negative weight in either child node.\n     */\n    sample_weight?: ArrayLike\n  }): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This RandomTreesEmbedding instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('RandomTreesEmbedding must call init() before fit()')\n    }\n\n    // set up method params\n    await this._py.ex`pms_RandomTreesEmbedding_fit = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None, 'y': ${\n      opts['y'] ?? undefined\n    }, 'sample_weight': np.array(${opts['sample_weight'] ?? undefined}) if ${\n      opts['sample_weight'] !== undefined\n    } else None}\n\npms_RandomTreesEmbedding_fit = {k: v for k, v in pms_RandomTreesEmbedding_fit.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_RandomTreesEmbedding_fit = bridgeRandomTreesEmbedding[${this.id}].fit(**pms_RandomTreesEmbedding_fit)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_RandomTreesEmbedding_fit.tolist() if hasattr(res_RandomTreesEmbedding_fit, 'tolist') else res_RandomTreesEmbedding_fit`\n  }\n\n  /**\n    Fit estimator and transform dataset.\n   */\n  async fit_transform(opts: {\n    /**\n      Input data used to build forests. Use `dtype=np.float32` for maximum efficiency.\n     */\n    X?: ArrayLike | SparseMatrix[]\n\n    /**\n      Not used, present for API consistency by convention.\n     */\n    y?: any\n\n    /**\n      Sample weights. If `undefined`, then samples are equally weighted. Splits that would create child nodes with net zero or negative weight are ignored while searching for a split in each node. In the case of classification, splits are also ignored if they would result in any single class carrying a negative weight in either child node.\n     */\n    sample_weight?: ArrayLike\n  }): Promise<SparseMatrix[]> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This RandomTreesEmbedding instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'RandomTreesEmbedding must call init() before fit_transform()'\n      )\n    }\n\n    // set up method params\n    await this._py.ex`pms_RandomTreesEmbedding_fit_transform = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None, 'y': ${\n      opts['y'] ?? undefined\n    }, 'sample_weight': np.array(${opts['sample_weight'] ?? undefined}) if ${\n      opts['sample_weight'] !== undefined\n    } else None}\n\npms_RandomTreesEmbedding_fit_transform = {k: v for k, v in pms_RandomTreesEmbedding_fit_transform.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_RandomTreesEmbedding_fit_transform = bridgeRandomTreesEmbedding[${this.id}].fit_transform(**pms_RandomTreesEmbedding_fit_transform)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_RandomTreesEmbedding_fit_transform.tolist() if hasattr(res_RandomTreesEmbedding_fit_transform, 'tolist') else res_RandomTreesEmbedding_fit_transform`\n  }\n\n  /**\n    Get output feature names for transformation.\n   */\n  async get_feature_names_out(opts: {\n    /**\n      Only used to validate feature names with the names seen in [`fit`](#sklearn.ensemble.RandomTreesEmbedding.fit \"sklearn.ensemble.RandomTreesEmbedding.fit\").\n     */\n    input_features?: any\n  }): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This RandomTreesEmbedding instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'RandomTreesEmbedding must call init() before get_feature_names_out()'\n      )\n    }\n\n    // set up method params\n    await this._py\n      .ex`pms_RandomTreesEmbedding_get_feature_names_out = {'input_features': ${\n      opts['input_features'] ?? undefined\n    }}\n\npms_RandomTreesEmbedding_get_feature_names_out = {k: v for k, v in pms_RandomTreesEmbedding_get_feature_names_out.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_RandomTreesEmbedding_get_feature_names_out = bridgeRandomTreesEmbedding[${this.id}].get_feature_names_out(**pms_RandomTreesEmbedding_get_feature_names_out)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_RandomTreesEmbedding_get_feature_names_out.tolist() if hasattr(res_RandomTreesEmbedding_get_feature_names_out, 'tolist') else res_RandomTreesEmbedding_get_feature_names_out`\n  }\n\n  /**\n    Set output container.\n\n    See [Introducing the set\\_output API](../../auto_examples/miscellaneous/plot_set_output.html#sphx-glr-auto-examples-miscellaneous-plot-set-output-py) for an example on how to use the API.\n   */\n  async set_output(opts: {\n    /**\n      Configure output of `transform` and `fit\\_transform`.\n     */\n    transform?: 'default' | 'pandas'\n  }): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This RandomTreesEmbedding instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'RandomTreesEmbedding must call init() before set_output()'\n      )\n    }\n\n    // set up method params\n    await this._py.ex`pms_RandomTreesEmbedding_set_output = {'transform': ${\n      opts['transform'] ?? undefined\n    }}\n\npms_RandomTreesEmbedding_set_output = {k: v for k, v in pms_RandomTreesEmbedding_set_output.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_RandomTreesEmbedding_set_output = bridgeRandomTreesEmbedding[${this.id}].set_output(**pms_RandomTreesEmbedding_set_output)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_RandomTreesEmbedding_set_output.tolist() if hasattr(res_RandomTreesEmbedding_set_output, 'tolist') else res_RandomTreesEmbedding_set_output`\n  }\n\n  /**\n    Transform dataset.\n   */\n  async transform(opts: {\n    /**\n      Input data to be transformed. Use `dtype=np.float32` for maximum efficiency. Sparse matrices are also supported, use sparse `csr\\_matrix` for maximum efficiency.\n     */\n    X?: ArrayLike | SparseMatrix[]\n  }): Promise<SparseMatrix[]> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This RandomTreesEmbedding instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'RandomTreesEmbedding must call init() before transform()'\n      )\n    }\n\n    // set up method params\n    await this._py.ex`pms_RandomTreesEmbedding_transform = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None}\n\npms_RandomTreesEmbedding_transform = {k: v for k, v in pms_RandomTreesEmbedding_transform.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_RandomTreesEmbedding_transform = bridgeRandomTreesEmbedding[${this.id}].transform(**pms_RandomTreesEmbedding_transform)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_RandomTreesEmbedding_transform.tolist() if hasattr(res_RandomTreesEmbedding_transform, 'tolist') else res_RandomTreesEmbedding_transform`\n  }\n\n  /**\n    The child estimator template used to create the collection of fitted sub-estimators.\n   */\n  get estimator_(): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This RandomTreesEmbedding instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'RandomTreesEmbedding must call init() before accessing estimator_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_RandomTreesEmbedding_estimator_ = bridgeRandomTreesEmbedding[${this.id}].estimator_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_RandomTreesEmbedding_estimator_.tolist() if hasattr(attr_RandomTreesEmbedding_estimator_, 'tolist') else attr_RandomTreesEmbedding_estimator_`\n    })()\n  }\n\n  /**\n    The collection of fitted sub-estimators.\n   */\n  get estimators_(): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This RandomTreesEmbedding instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'RandomTreesEmbedding must call init() before accessing estimators_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_RandomTreesEmbedding_estimators_ = bridgeRandomTreesEmbedding[${this.id}].estimators_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_RandomTreesEmbedding_estimators_.tolist() if hasattr(attr_RandomTreesEmbedding_estimators_, 'tolist') else attr_RandomTreesEmbedding_estimators_`\n    })()\n  }\n\n  /**\n    Number of features seen during [fit](../../glossary.html#term-fit).\n   */\n  get n_features_in_(): Promise<number> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This RandomTreesEmbedding instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'RandomTreesEmbedding must call init() before accessing n_features_in_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_RandomTreesEmbedding_n_features_in_ = bridgeRandomTreesEmbedding[${this.id}].n_features_in_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_RandomTreesEmbedding_n_features_in_.tolist() if hasattr(attr_RandomTreesEmbedding_n_features_in_, 'tolist') else attr_RandomTreesEmbedding_n_features_in_`\n    })()\n  }\n\n  /**\n    Names of features seen during [fit](../../glossary.html#term-fit). Defined only when `X` has feature names that are all strings.\n   */\n  get feature_names_in_(): Promise<NDArray> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This RandomTreesEmbedding instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'RandomTreesEmbedding must call init() before accessing feature_names_in_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_RandomTreesEmbedding_feature_names_in_ = bridgeRandomTreesEmbedding[${this.id}].feature_names_in_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_RandomTreesEmbedding_feature_names_in_.tolist() if hasattr(attr_RandomTreesEmbedding_feature_names_in_, 'tolist') else attr_RandomTreesEmbedding_feature_names_in_`\n    })()\n  }\n\n  /**\n    The number of outputs when `fit` is performed.\n   */\n  get n_outputs_(): Promise<number> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This RandomTreesEmbedding instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'RandomTreesEmbedding must call init() before accessing n_outputs_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_RandomTreesEmbedding_n_outputs_ = bridgeRandomTreesEmbedding[${this.id}].n_outputs_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_RandomTreesEmbedding_n_outputs_.tolist() if hasattr(attr_RandomTreesEmbedding_n_outputs_, 'tolist') else attr_RandomTreesEmbedding_n_outputs_`\n    })()\n  }\n\n  /**\n    One-hot encoder used to create the sparse embedding.\n   */\n  get one_hot_encoder_(): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This RandomTreesEmbedding instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'RandomTreesEmbedding must call init() before accessing one_hot_encoder_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_RandomTreesEmbedding_one_hot_encoder_ = bridgeRandomTreesEmbedding[${this.id}].one_hot_encoder_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_RandomTreesEmbedding_one_hot_encoder_.tolist() if hasattr(attr_RandomTreesEmbedding_one_hot_encoder_, 'tolist') else attr_RandomTreesEmbedding_one_hot_encoder_`\n    })()\n  }\n}\n","/* eslint-disable */\n/* NOTE: This file is auto-generated. Do not edit it directly. */\n\nimport crypto from 'node:crypto'\n\nimport { PythonBridge, NDArray, ArrayLike, SparseMatrix } from '@/sklearn/types'\n\n/**\n  Stack of estimators with a final classifier.\n\n  Stacked generalization consists in stacking the output of individual estimator and use a classifier to compute the final prediction. Stacking allows to use the strength of each individual estimator by using their output as input of a final estimator.\n\n  Note that `estimators\\_` are fitted on the full `X` while `final\\_estimator\\_` is trained using cross-validated predictions of the base estimators using `cross\\_val\\_predict`.\n\n  Read more in the [User Guide](../ensemble.html#stacking).\n\n  [Python Reference](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html)\n */\nexport class StackingClassifier {\n  id: string\n  opts: any\n\n  _py: PythonBridge\n  _isInitialized: boolean = false\n  _isDisposed: boolean = false\n\n  constructor(opts?: {\n    /**\n      Base estimators which will be stacked together. Each element of the list is defined as a tuple of string (i.e. name) and an estimator instance. An estimator can be set to ‘drop’ using `set\\_params`.\n\n      The type of estimator is generally expected to be a classifier. However, one can pass a regressor for some use case (e.g. ordinal regression).\n     */\n    estimators?: any\n\n    /**\n      A classifier which will be used to combine the base estimators. The default classifier is a [`LogisticRegression`](sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression \"sklearn.linear_model.LogisticRegression\").\n     */\n    final_estimator?: any\n\n    /**\n      Determines the cross-validation splitting strategy used in `cross\\_val\\_predict` to train `final\\_estimator`. Possible inputs for cv are:\n     */\n    cv?: number | 'prefit'\n\n    /**\n      Methods called for each base estimator. It can be:\n\n      @defaultValue `'auto'`\n     */\n    stack_method?: 'auto' | 'predict_proba' | 'decision_function' | 'predict'\n\n    /**\n      The number of jobs to run in parallel all `estimators` `fit`. `undefined` means 1 unless in a `joblib.parallel\\_backend` context. -1 means using all processors. See Glossary for more details.\n     */\n    n_jobs?: number\n\n    /**\n      When `false`, only the predictions of estimators will be used as training data for `final\\_estimator`. When `true`, the `final\\_estimator` is trained on the predictions as well as the original training data.\n\n      @defaultValue `false`\n     */\n    passthrough?: boolean\n\n    /**\n      Verbosity level.\n\n      @defaultValue `0`\n     */\n    verbose?: number\n  }) {\n    this.id = `StackingClassifier${crypto.randomUUID().split('-')[0]}`\n    this.opts = opts || {}\n  }\n\n  get py(): PythonBridge {\n    return this._py\n  }\n\n  set py(pythonBridge: PythonBridge) {\n    this._py = pythonBridge\n  }\n\n  /**\n    Initializes the underlying Python resources.\n\n    This instance is not usable until the `Promise` returned by `init()` resolves.\n   */\n  async init(py: PythonBridge): Promise<void> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This StackingClassifier instance has already been disposed'\n      )\n    }\n\n    if (this._isInitialized) {\n      return\n    }\n\n    if (!py) {\n      throw new Error(\n        'StackingClassifier.init requires a PythonBridge instance'\n      )\n    }\n\n    this._py = py\n\n    await this._py.ex`\nimport numpy as np\nfrom sklearn.ensemble import StackingClassifier\ntry: bridgeStackingClassifier\nexcept NameError: bridgeStackingClassifier = {}\n`\n\n    // set up constructor params\n    await this._py.ex`ctor_StackingClassifier = {'estimators': ${\n      this.opts['estimators'] ?? undefined\n    }, 'final_estimator': ${this.opts['final_estimator'] ?? undefined}, 'cv': ${\n      this.opts['cv'] ?? undefined\n    }, 'stack_method': ${this.opts['stack_method'] ?? undefined}, 'n_jobs': ${\n      this.opts['n_jobs'] ?? undefined\n    }, 'passthrough': ${this.opts['passthrough'] ?? undefined}, 'verbose': ${\n      this.opts['verbose'] ?? undefined\n    }}\n\nctor_StackingClassifier = {k: v for k, v in ctor_StackingClassifier.items() if v is not None}`\n\n    await this._py\n      .ex`bridgeStackingClassifier[${this.id}] = StackingClassifier(**ctor_StackingClassifier)`\n\n    this._isInitialized = true\n  }\n\n  /**\n    Disposes of the underlying Python resources.\n\n    Once `dispose()` is called, the instance is no longer usable.\n   */\n  async dispose() {\n    if (this._isDisposed) {\n      return\n    }\n\n    if (!this._isInitialized) {\n      return\n    }\n\n    await this._py.ex`del bridgeStackingClassifier[${this.id}]`\n\n    this._isDisposed = true\n  }\n\n  /**\n    Decision function for samples in `X` using the final estimator.\n   */\n  async decision_function(opts: {\n    /**\n      Training vectors, where `n\\_samples` is the number of samples and `n\\_features` is the number of features.\n     */\n    X?: ArrayLike | SparseMatrix[]\n  }): Promise<NDArray> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This StackingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'StackingClassifier must call init() before decision_function()'\n      )\n    }\n\n    // set up method params\n    await this._py\n      .ex`pms_StackingClassifier_decision_function = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None}\n\npms_StackingClassifier_decision_function = {k: v for k, v in pms_StackingClassifier_decision_function.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_StackingClassifier_decision_function = bridgeStackingClassifier[${this.id}].decision_function(**pms_StackingClassifier_decision_function)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_StackingClassifier_decision_function.tolist() if hasattr(res_StackingClassifier_decision_function, 'tolist') else res_StackingClassifier_decision_function`\n  }\n\n  /**\n    Fit the estimators.\n   */\n  async fit(opts: {\n    /**\n      Training vectors, where `n\\_samples` is the number of samples and `n\\_features` is the number of features.\n     */\n    X?: ArrayLike | SparseMatrix[]\n\n    /**\n      Target values. Note that `y` will be internally encoded in numerically increasing order or lexicographic order. If the order matter (e.g. for ordinal regression), one should numerically encode the target `y` before calling [fit](../../glossary.html#term-fit).\n     */\n    y?: ArrayLike\n\n    /**\n      Sample weights. If `undefined`, then samples are equally weighted. Note that this is supported only if all underlying estimators support sample weights.\n     */\n    sample_weight?: ArrayLike\n  }): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This StackingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('StackingClassifier must call init() before fit()')\n    }\n\n    // set up method params\n    await this._py.ex`pms_StackingClassifier_fit = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None, 'y': np.array(${\n      opts['y'] ?? undefined\n    }) if ${opts['y'] !== undefined} else None, 'sample_weight': np.array(${\n      opts['sample_weight'] ?? undefined\n    }) if ${opts['sample_weight'] !== undefined} else None}\n\npms_StackingClassifier_fit = {k: v for k, v in pms_StackingClassifier_fit.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_StackingClassifier_fit = bridgeStackingClassifier[${this.id}].fit(**pms_StackingClassifier_fit)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_StackingClassifier_fit.tolist() if hasattr(res_StackingClassifier_fit, 'tolist') else res_StackingClassifier_fit`\n  }\n\n  /**\n    Fit to data, then transform it.\n\n    Fits transformer to `X` and `y` with optional parameters `fit\\_params` and returns a transformed version of `X`.\n   */\n  async fit_transform(opts: {\n    /**\n      Input samples.\n     */\n    X?: ArrayLike[]\n\n    /**\n      Target values (`undefined` for unsupervised transformations).\n     */\n    y?: ArrayLike\n\n    /**\n      Additional fit parameters.\n     */\n    fit_params?: any\n  }): Promise<any[]> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This StackingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'StackingClassifier must call init() before fit_transform()'\n      )\n    }\n\n    // set up method params\n    await this._py.ex`pms_StackingClassifier_fit_transform = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None, 'y': np.array(${\n      opts['y'] ?? undefined\n    }) if ${opts['y'] !== undefined} else None, 'fit_params': ${\n      opts['fit_params'] ?? undefined\n    }}\n\npms_StackingClassifier_fit_transform = {k: v for k, v in pms_StackingClassifier_fit_transform.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_StackingClassifier_fit_transform = bridgeStackingClassifier[${this.id}].fit_transform(**pms_StackingClassifier_fit_transform)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_StackingClassifier_fit_transform.tolist() if hasattr(res_StackingClassifier_fit_transform, 'tolist') else res_StackingClassifier_fit_transform`\n  }\n\n  /**\n    Get output feature names for transformation.\n   */\n  async get_feature_names_out(opts: {\n    /**\n      Input features. The input feature names are only used when `passthrough` is `true`.\n     */\n    input_features?: any\n  }): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This StackingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'StackingClassifier must call init() before get_feature_names_out()'\n      )\n    }\n\n    // set up method params\n    await this._py\n      .ex`pms_StackingClassifier_get_feature_names_out = {'input_features': ${\n      opts['input_features'] ?? undefined\n    }}\n\npms_StackingClassifier_get_feature_names_out = {k: v for k, v in pms_StackingClassifier_get_feature_names_out.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_StackingClassifier_get_feature_names_out = bridgeStackingClassifier[${this.id}].get_feature_names_out(**pms_StackingClassifier_get_feature_names_out)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_StackingClassifier_get_feature_names_out.tolist() if hasattr(res_StackingClassifier_get_feature_names_out, 'tolist') else res_StackingClassifier_get_feature_names_out`\n  }\n\n  /**\n    Predict target for X.\n   */\n  async predict(opts: {\n    /**\n      Training vectors, where `n\\_samples` is the number of samples and `n\\_features` is the number of features.\n     */\n    X?: ArrayLike | SparseMatrix[]\n\n    /**\n      Parameters to the `predict` called by the `final\\_estimator`. Note that this may be used to return uncertainties from some estimators with `return\\_std` or `return\\_cov`. Be aware that it will only accounts for uncertainty in the final estimator.\n     */\n    predict_params?: any\n  }): Promise<NDArray> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This StackingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('StackingClassifier must call init() before predict()')\n    }\n\n    // set up method params\n    await this._py.ex`pms_StackingClassifier_predict = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None, 'predict_params': ${\n      opts['predict_params'] ?? undefined\n    }}\n\npms_StackingClassifier_predict = {k: v for k, v in pms_StackingClassifier_predict.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_StackingClassifier_predict = bridgeStackingClassifier[${this.id}].predict(**pms_StackingClassifier_predict)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_StackingClassifier_predict.tolist() if hasattr(res_StackingClassifier_predict, 'tolist') else res_StackingClassifier_predict`\n  }\n\n  /**\n    Predict class probabilities for `X` using the final estimator.\n   */\n  async predict_proba(opts: {\n    /**\n      Training vectors, where `n\\_samples` is the number of samples and `n\\_features` is the number of features.\n     */\n    X?: ArrayLike | SparseMatrix[]\n  }): Promise<NDArray[] | any[]> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This StackingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'StackingClassifier must call init() before predict_proba()'\n      )\n    }\n\n    // set up method params\n    await this._py.ex`pms_StackingClassifier_predict_proba = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None}\n\npms_StackingClassifier_predict_proba = {k: v for k, v in pms_StackingClassifier_predict_proba.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_StackingClassifier_predict_proba = bridgeStackingClassifier[${this.id}].predict_proba(**pms_StackingClassifier_predict_proba)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_StackingClassifier_predict_proba.tolist() if hasattr(res_StackingClassifier_predict_proba, 'tolist') else res_StackingClassifier_predict_proba`\n  }\n\n  /**\n    Return the mean accuracy on the given test data and labels.\n\n    In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted.\n   */\n  async score(opts: {\n    /**\n      Test samples.\n     */\n    X?: ArrayLike[]\n\n    /**\n      True labels for `X`.\n     */\n    y?: ArrayLike\n\n    /**\n      Sample weights.\n     */\n    sample_weight?: ArrayLike\n  }): Promise<number> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This StackingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('StackingClassifier must call init() before score()')\n    }\n\n    // set up method params\n    await this._py.ex`pms_StackingClassifier_score = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None, 'y': np.array(${\n      opts['y'] ?? undefined\n    }) if ${opts['y'] !== undefined} else None, 'sample_weight': np.array(${\n      opts['sample_weight'] ?? undefined\n    }) if ${opts['sample_weight'] !== undefined} else None}\n\npms_StackingClassifier_score = {k: v for k, v in pms_StackingClassifier_score.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_StackingClassifier_score = bridgeStackingClassifier[${this.id}].score(**pms_StackingClassifier_score)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_StackingClassifier_score.tolist() if hasattr(res_StackingClassifier_score, 'tolist') else res_StackingClassifier_score`\n  }\n\n  /**\n    Set output container.\n\n    See [Introducing the set\\_output API](../../auto_examples/miscellaneous/plot_set_output.html#sphx-glr-auto-examples-miscellaneous-plot-set-output-py) for an example on how to use the API.\n   */\n  async set_output(opts: {\n    /**\n      Configure output of `transform` and `fit\\_transform`.\n     */\n    transform?: 'default' | 'pandas'\n  }): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This StackingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('StackingClassifier must call init() before set_output()')\n    }\n\n    // set up method params\n    await this._py.ex`pms_StackingClassifier_set_output = {'transform': ${\n      opts['transform'] ?? undefined\n    }}\n\npms_StackingClassifier_set_output = {k: v for k, v in pms_StackingClassifier_set_output.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_StackingClassifier_set_output = bridgeStackingClassifier[${this.id}].set_output(**pms_StackingClassifier_set_output)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_StackingClassifier_set_output.tolist() if hasattr(res_StackingClassifier_set_output, 'tolist') else res_StackingClassifier_set_output`\n  }\n\n  /**\n    Return class labels or probabilities for X for each estimator.\n   */\n  async transform(opts: {\n    /**\n      Training vectors, where `n\\_samples` is the number of samples and `n\\_features` is the number of features.\n     */\n    X?: ArrayLike | SparseMatrix[]\n  }): Promise<NDArray[]> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This StackingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('StackingClassifier must call init() before transform()')\n    }\n\n    // set up method params\n    await this._py.ex`pms_StackingClassifier_transform = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None}\n\npms_StackingClassifier_transform = {k: v for k, v in pms_StackingClassifier_transform.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_StackingClassifier_transform = bridgeStackingClassifier[${this.id}].transform(**pms_StackingClassifier_transform)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_StackingClassifier_transform.tolist() if hasattr(res_StackingClassifier_transform, 'tolist') else res_StackingClassifier_transform`\n  }\n\n  /**\n    Class labels.\n   */\n  get classes_(): Promise<NDArray> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This StackingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'StackingClassifier must call init() before accessing classes_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_StackingClassifier_classes_ = bridgeStackingClassifier[${this.id}].classes_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_StackingClassifier_classes_.tolist() if hasattr(attr_StackingClassifier_classes_, 'tolist') else attr_StackingClassifier_classes_`\n    })()\n  }\n\n  /**\n    The elements of the `estimators` parameter, having been fitted on the training data. If an estimator has been set to `'drop'`, it will not appear in `estimators\\_`. When `cv=\"prefit\"`, `estimators\\_` is set to `estimators` and is not fitted again.\n   */\n  get estimators_(): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This StackingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'StackingClassifier must call init() before accessing estimators_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_StackingClassifier_estimators_ = bridgeStackingClassifier[${this.id}].estimators_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_StackingClassifier_estimators_.tolist() if hasattr(attr_StackingClassifier_estimators_, 'tolist') else attr_StackingClassifier_estimators_`\n    })()\n  }\n\n  /**\n    Attribute to access any fitted sub-estimators by name.\n   */\n  get named_estimators_(): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This StackingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'StackingClassifier must call init() before accessing named_estimators_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_StackingClassifier_named_estimators_ = bridgeStackingClassifier[${this.id}].named_estimators_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_StackingClassifier_named_estimators_.tolist() if hasattr(attr_StackingClassifier_named_estimators_, 'tolist') else attr_StackingClassifier_named_estimators_`\n    })()\n  }\n\n  /**\n    Names of features seen during [fit](../../glossary.html#term-fit). Only defined if the underlying estimators expose such an attribute when fit.\n   */\n  get feature_names_in_(): Promise<NDArray> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This StackingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'StackingClassifier must call init() before accessing feature_names_in_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_StackingClassifier_feature_names_in_ = bridgeStackingClassifier[${this.id}].feature_names_in_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_StackingClassifier_feature_names_in_.tolist() if hasattr(attr_StackingClassifier_feature_names_in_, 'tolist') else attr_StackingClassifier_feature_names_in_`\n    })()\n  }\n\n  /**\n    The classifier which predicts given the output of `estimators\\_`.\n   */\n  get final_estimator_(): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This StackingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'StackingClassifier must call init() before accessing final_estimator_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_StackingClassifier_final_estimator_ = bridgeStackingClassifier[${this.id}].final_estimator_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_StackingClassifier_final_estimator_.tolist() if hasattr(attr_StackingClassifier_final_estimator_, 'tolist') else attr_StackingClassifier_final_estimator_`\n    })()\n  }\n\n  /**\n    The method used by each base estimator.\n   */\n  get stack_method_(): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This StackingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'StackingClassifier must call init() before accessing stack_method_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_StackingClassifier_stack_method_ = bridgeStackingClassifier[${this.id}].stack_method_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_StackingClassifier_stack_method_.tolist() if hasattr(attr_StackingClassifier_stack_method_, 'tolist') else attr_StackingClassifier_stack_method_`\n    })()\n  }\n}\n","/* eslint-disable */\n/* NOTE: This file is auto-generated. Do not edit it directly. */\n\nimport crypto from 'node:crypto'\n\nimport { PythonBridge, NDArray, ArrayLike, SparseMatrix } from '@/sklearn/types'\n\n/**\n  Stack of estimators with a final regressor.\n\n  Stacked generalization consists in stacking the output of individual estimator and use a regressor to compute the final prediction. Stacking allows to use the strength of each individual estimator by using their output as input of a final estimator.\n\n  Note that `estimators\\_` are fitted on the full `X` while `final\\_estimator\\_` is trained using cross-validated predictions of the base estimators using `cross\\_val\\_predict`.\n\n  Read more in the [User Guide](../ensemble.html#stacking).\n\n  [Python Reference](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingRegressor.html)\n */\nexport class StackingRegressor {\n  id: string\n  opts: any\n\n  _py: PythonBridge\n  _isInitialized: boolean = false\n  _isDisposed: boolean = false\n\n  constructor(opts?: {\n    /**\n      Base estimators which will be stacked together. Each element of the list is defined as a tuple of string (i.e. name) and an estimator instance. An estimator can be set to ‘drop’ using `set\\_params`.\n     */\n    estimators?: any\n\n    /**\n      A regressor which will be used to combine the base estimators. The default regressor is a [`RidgeCV`](sklearn.linear_model.RidgeCV.html#sklearn.linear_model.RidgeCV \"sklearn.linear_model.RidgeCV\").\n     */\n    final_estimator?: any\n\n    /**\n      Determines the cross-validation splitting strategy used in `cross\\_val\\_predict` to train `final\\_estimator`. Possible inputs for cv are:\n     */\n    cv?: number | 'prefit'\n\n    /**\n      The number of jobs to run in parallel for `fit` of all `estimators`. `undefined` means 1 unless in a `joblib.parallel\\_backend` context. -1 means using all processors. See Glossary for more details.\n     */\n    n_jobs?: number\n\n    /**\n      When `false`, only the predictions of estimators will be used as training data for `final\\_estimator`. When `true`, the `final\\_estimator` is trained on the predictions as well as the original training data.\n\n      @defaultValue `false`\n     */\n    passthrough?: boolean\n\n    /**\n      Verbosity level.\n\n      @defaultValue `0`\n     */\n    verbose?: number\n  }) {\n    this.id = `StackingRegressor${crypto.randomUUID().split('-')[0]}`\n    this.opts = opts || {}\n  }\n\n  get py(): PythonBridge {\n    return this._py\n  }\n\n  set py(pythonBridge: PythonBridge) {\n    this._py = pythonBridge\n  }\n\n  /**\n    Initializes the underlying Python resources.\n\n    This instance is not usable until the `Promise` returned by `init()` resolves.\n   */\n  async init(py: PythonBridge): Promise<void> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This StackingRegressor instance has already been disposed'\n      )\n    }\n\n    if (this._isInitialized) {\n      return\n    }\n\n    if (!py) {\n      throw new Error('StackingRegressor.init requires a PythonBridge instance')\n    }\n\n    this._py = py\n\n    await this._py.ex`\nimport numpy as np\nfrom sklearn.ensemble import StackingRegressor\ntry: bridgeStackingRegressor\nexcept NameError: bridgeStackingRegressor = {}\n`\n\n    // set up constructor params\n    await this._py.ex`ctor_StackingRegressor = {'estimators': ${\n      this.opts['estimators'] ?? undefined\n    }, 'final_estimator': ${this.opts['final_estimator'] ?? undefined}, 'cv': ${\n      this.opts['cv'] ?? undefined\n    }, 'n_jobs': ${this.opts['n_jobs'] ?? undefined}, 'passthrough': ${\n      this.opts['passthrough'] ?? undefined\n    }, 'verbose': ${this.opts['verbose'] ?? undefined}}\n\nctor_StackingRegressor = {k: v for k, v in ctor_StackingRegressor.items() if v is not None}`\n\n    await this._py\n      .ex`bridgeStackingRegressor[${this.id}] = StackingRegressor(**ctor_StackingRegressor)`\n\n    this._isInitialized = true\n  }\n\n  /**\n    Disposes of the underlying Python resources.\n\n    Once `dispose()` is called, the instance is no longer usable.\n   */\n  async dispose() {\n    if (this._isDisposed) {\n      return\n    }\n\n    if (!this._isInitialized) {\n      return\n    }\n\n    await this._py.ex`del bridgeStackingRegressor[${this.id}]`\n\n    this._isDisposed = true\n  }\n\n  /**\n    Fit the estimators.\n   */\n  async fit(opts: {\n    /**\n      Training vectors, where `n\\_samples` is the number of samples and `n\\_features` is the number of features.\n     */\n    X?: ArrayLike | SparseMatrix[]\n\n    /**\n      Target values.\n     */\n    y?: ArrayLike\n\n    /**\n      Sample weights. If `undefined`, then samples are equally weighted. Note that this is supported only if all underlying estimators support sample weights.\n     */\n    sample_weight?: ArrayLike\n  }): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This StackingRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('StackingRegressor must call init() before fit()')\n    }\n\n    // set up method params\n    await this._py.ex`pms_StackingRegressor_fit = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None, 'y': np.array(${\n      opts['y'] ?? undefined\n    }) if ${opts['y'] !== undefined} else None, 'sample_weight': np.array(${\n      opts['sample_weight'] ?? undefined\n    }) if ${opts['sample_weight'] !== undefined} else None}\n\npms_StackingRegressor_fit = {k: v for k, v in pms_StackingRegressor_fit.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_StackingRegressor_fit = bridgeStackingRegressor[${this.id}].fit(**pms_StackingRegressor_fit)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_StackingRegressor_fit.tolist() if hasattr(res_StackingRegressor_fit, 'tolist') else res_StackingRegressor_fit`\n  }\n\n  /**\n    Fit the estimators and return the predictions for X for each estimator.\n   */\n  async fit_transform(opts: {\n    /**\n      Training vectors, where `n\\_samples` is the number of samples and `n\\_features` is the number of features.\n     */\n    X?: ArrayLike | SparseMatrix[]\n\n    /**\n      Target values.\n     */\n    y?: ArrayLike\n\n    /**\n      Sample weights. If `undefined`, then samples are equally weighted. Note that this is supported only if all underlying estimators support sample weights.\n     */\n    sample_weight?: ArrayLike\n  }): Promise<NDArray[]> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This StackingRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'StackingRegressor must call init() before fit_transform()'\n      )\n    }\n\n    // set up method params\n    await this._py.ex`pms_StackingRegressor_fit_transform = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None, 'y': np.array(${\n      opts['y'] ?? undefined\n    }) if ${opts['y'] !== undefined} else None, 'sample_weight': np.array(${\n      opts['sample_weight'] ?? undefined\n    }) if ${opts['sample_weight'] !== undefined} else None}\n\npms_StackingRegressor_fit_transform = {k: v for k, v in pms_StackingRegressor_fit_transform.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_StackingRegressor_fit_transform = bridgeStackingRegressor[${this.id}].fit_transform(**pms_StackingRegressor_fit_transform)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_StackingRegressor_fit_transform.tolist() if hasattr(res_StackingRegressor_fit_transform, 'tolist') else res_StackingRegressor_fit_transform`\n  }\n\n  /**\n    Get output feature names for transformation.\n   */\n  async get_feature_names_out(opts: {\n    /**\n      Input features. The input feature names are only used when `passthrough` is `true`.\n     */\n    input_features?: any\n  }): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This StackingRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'StackingRegressor must call init() before get_feature_names_out()'\n      )\n    }\n\n    // set up method params\n    await this._py\n      .ex`pms_StackingRegressor_get_feature_names_out = {'input_features': ${\n      opts['input_features'] ?? undefined\n    }}\n\npms_StackingRegressor_get_feature_names_out = {k: v for k, v in pms_StackingRegressor_get_feature_names_out.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_StackingRegressor_get_feature_names_out = bridgeStackingRegressor[${this.id}].get_feature_names_out(**pms_StackingRegressor_get_feature_names_out)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_StackingRegressor_get_feature_names_out.tolist() if hasattr(res_StackingRegressor_get_feature_names_out, 'tolist') else res_StackingRegressor_get_feature_names_out`\n  }\n\n  /**\n    Predict target for X.\n   */\n  async predict(opts: {\n    /**\n      Training vectors, where `n\\_samples` is the number of samples and `n\\_features` is the number of features.\n     */\n    X?: ArrayLike | SparseMatrix[]\n\n    /**\n      Parameters to the `predict` called by the `final\\_estimator`. Note that this may be used to return uncertainties from some estimators with `return\\_std` or `return\\_cov`. Be aware that it will only accounts for uncertainty in the final estimator.\n     */\n    predict_params?: any\n  }): Promise<NDArray> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This StackingRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('StackingRegressor must call init() before predict()')\n    }\n\n    // set up method params\n    await this._py.ex`pms_StackingRegressor_predict = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None, 'predict_params': ${\n      opts['predict_params'] ?? undefined\n    }}\n\npms_StackingRegressor_predict = {k: v for k, v in pms_StackingRegressor_predict.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_StackingRegressor_predict = bridgeStackingRegressor[${this.id}].predict(**pms_StackingRegressor_predict)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_StackingRegressor_predict.tolist() if hasattr(res_StackingRegressor_predict, 'tolist') else res_StackingRegressor_predict`\n  }\n\n  /**\n    Return the coefficient of determination of the prediction.\n\n    The coefficient of determination \\\\(R^2\\\\) is defined as \\\\((1 - \\\\frac{u}{v})\\\\), where \\\\(u\\\\) is the residual sum of squares `((y\\_true \\- y\\_pred)\\*\\* 2).sum()` and \\\\(v\\\\) is the total sum of squares `((y\\_true \\- y\\_true.mean()) \\*\\* 2).sum()`. The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of `y`, disregarding the input features, would get a \\\\(R^2\\\\) score of 0.0.\n   */\n  async score(opts: {\n    /**\n      Test samples. For some estimators this may be a precomputed kernel matrix or a list of generic objects instead with shape `(n\\_samples, n\\_samples\\_fitted)`, where `n\\_samples\\_fitted` is the number of samples used in the fitting for the estimator.\n     */\n    X?: ArrayLike[]\n\n    /**\n      True values for `X`.\n     */\n    y?: ArrayLike\n\n    /**\n      Sample weights.\n     */\n    sample_weight?: ArrayLike\n  }): Promise<number> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This StackingRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('StackingRegressor must call init() before score()')\n    }\n\n    // set up method params\n    await this._py.ex`pms_StackingRegressor_score = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None, 'y': np.array(${\n      opts['y'] ?? undefined\n    }) if ${opts['y'] !== undefined} else None, 'sample_weight': np.array(${\n      opts['sample_weight'] ?? undefined\n    }) if ${opts['sample_weight'] !== undefined} else None}\n\npms_StackingRegressor_score = {k: v for k, v in pms_StackingRegressor_score.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_StackingRegressor_score = bridgeStackingRegressor[${this.id}].score(**pms_StackingRegressor_score)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_StackingRegressor_score.tolist() if hasattr(res_StackingRegressor_score, 'tolist') else res_StackingRegressor_score`\n  }\n\n  /**\n    Set output container.\n\n    See [Introducing the set\\_output API](../../auto_examples/miscellaneous/plot_set_output.html#sphx-glr-auto-examples-miscellaneous-plot-set-output-py) for an example on how to use the API.\n   */\n  async set_output(opts: {\n    /**\n      Configure output of `transform` and `fit\\_transform`.\n     */\n    transform?: 'default' | 'pandas'\n  }): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This StackingRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('StackingRegressor must call init() before set_output()')\n    }\n\n    // set up method params\n    await this._py.ex`pms_StackingRegressor_set_output = {'transform': ${\n      opts['transform'] ?? undefined\n    }}\n\npms_StackingRegressor_set_output = {k: v for k, v in pms_StackingRegressor_set_output.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_StackingRegressor_set_output = bridgeStackingRegressor[${this.id}].set_output(**pms_StackingRegressor_set_output)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_StackingRegressor_set_output.tolist() if hasattr(res_StackingRegressor_set_output, 'tolist') else res_StackingRegressor_set_output`\n  }\n\n  /**\n    Return the predictions for X for each estimator.\n   */\n  async transform(opts: {\n    /**\n      Training vectors, where `n\\_samples` is the number of samples and `n\\_features` is the number of features.\n     */\n    X?: ArrayLike | SparseMatrix[]\n  }): Promise<NDArray[]> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This StackingRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('StackingRegressor must call init() before transform()')\n    }\n\n    // set up method params\n    await this._py.ex`pms_StackingRegressor_transform = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None}\n\npms_StackingRegressor_transform = {k: v for k, v in pms_StackingRegressor_transform.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_StackingRegressor_transform = bridgeStackingRegressor[${this.id}].transform(**pms_StackingRegressor_transform)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_StackingRegressor_transform.tolist() if hasattr(res_StackingRegressor_transform, 'tolist') else res_StackingRegressor_transform`\n  }\n\n  /**\n    The elements of the `estimators` parameter, having been fitted on the training data. If an estimator has been set to `'drop'`, it will not appear in `estimators\\_`. When `cv=\"prefit\"`, `estimators\\_` is set to `estimators` and is not fitted again.\n   */\n  get estimators_(): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This StackingRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'StackingRegressor must call init() before accessing estimators_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_StackingRegressor_estimators_ = bridgeStackingRegressor[${this.id}].estimators_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_StackingRegressor_estimators_.tolist() if hasattr(attr_StackingRegressor_estimators_, 'tolist') else attr_StackingRegressor_estimators_`\n    })()\n  }\n\n  /**\n    Attribute to access any fitted sub-estimators by name.\n   */\n  get named_estimators_(): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This StackingRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'StackingRegressor must call init() before accessing named_estimators_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_StackingRegressor_named_estimators_ = bridgeStackingRegressor[${this.id}].named_estimators_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_StackingRegressor_named_estimators_.tolist() if hasattr(attr_StackingRegressor_named_estimators_, 'tolist') else attr_StackingRegressor_named_estimators_`\n    })()\n  }\n\n  /**\n    Names of features seen during [fit](../../glossary.html#term-fit). Only defined if the underlying estimators expose such an attribute when fit.\n   */\n  get feature_names_in_(): Promise<NDArray> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This StackingRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'StackingRegressor must call init() before accessing feature_names_in_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_StackingRegressor_feature_names_in_ = bridgeStackingRegressor[${this.id}].feature_names_in_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_StackingRegressor_feature_names_in_.tolist() if hasattr(attr_StackingRegressor_feature_names_in_, 'tolist') else attr_StackingRegressor_feature_names_in_`\n    })()\n  }\n\n  /**\n    The regressor to stacked the base estimators fitted.\n   */\n  get final_estimator_(): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This StackingRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'StackingRegressor must call init() before accessing final_estimator_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_StackingRegressor_final_estimator_ = bridgeStackingRegressor[${this.id}].final_estimator_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_StackingRegressor_final_estimator_.tolist() if hasattr(attr_StackingRegressor_final_estimator_, 'tolist') else attr_StackingRegressor_final_estimator_`\n    })()\n  }\n\n  /**\n    The method used by each base estimator.\n   */\n  get stack_method_(): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This StackingRegressor instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'StackingRegressor must call init() before accessing stack_method_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_StackingRegressor_stack_method_ = bridgeStackingRegressor[${this.id}].stack_method_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_StackingRegressor_stack_method_.tolist() if hasattr(attr_StackingRegressor_stack_method_, 'tolist') else attr_StackingRegressor_stack_method_`\n    })()\n  }\n}\n","/* eslint-disable */\n/* NOTE: This file is auto-generated. Do not edit it directly. */\n\nimport crypto from 'node:crypto'\n\nimport { PythonBridge, NDArray, ArrayLike, SparseMatrix } from '@/sklearn/types'\n\n/**\n  Soft Voting/Majority Rule classifier for unfitted estimators.\n\n  Read more in the [User Guide](../ensemble.html#voting-classifier).\n\n  [Python Reference](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html)\n */\nexport class VotingClassifier {\n  id: string\n  opts: any\n\n  _py: PythonBridge\n  _isInitialized: boolean = false\n  _isDisposed: boolean = false\n\n  constructor(opts?: {\n    /**\n      Invoking the `fit` method on the `VotingClassifier` will fit clones of those original estimators that will be stored in the class attribute `self.estimators\\_`. An estimator can be set to `'drop'` using [`set\\_params`](#sklearn.ensemble.VotingClassifier.set_params \"sklearn.ensemble.VotingClassifier.set_params\").\n     */\n    estimators?: any\n\n    /**\n      If ‘hard’, uses predicted class labels for majority rule voting. Else if ‘soft’, predicts the class label based on the argmax of the sums of the predicted probabilities, which is recommended for an ensemble of well-calibrated classifiers.\n\n      @defaultValue `'hard'`\n     */\n    voting?: 'hard' | 'soft'\n\n    /**\n      Sequence of weights (`float` or `int`) to weight the occurrences of predicted class labels (`hard` voting) or class probabilities before averaging (`soft` voting). Uses uniform weights if `undefined`.\n     */\n    weights?: ArrayLike\n\n    /**\n      The number of jobs to run in parallel for `fit`. `undefined` means 1 unless in a [`joblib.parallel\\_backend`](https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend \"(in joblib v1.3.0.dev0)\") context. `\\-1` means using all processors. See [Glossary](../../glossary.html#term-n_jobs) for more details.\n     */\n    n_jobs?: number\n\n    /**\n      Affects shape of transform output only when voting=’soft’ If voting=’soft’ and flatten\\_transform=`true`, transform method returns matrix with shape (n\\_samples, n\\_classifiers \\* n\\_classes). If flatten\\_transform=`false`, it returns (n\\_classifiers, n\\_samples, n\\_classes).\n\n      @defaultValue `true`\n     */\n    flatten_transform?: boolean\n\n    /**\n      If `true`, the time elapsed while fitting will be printed as it is completed.\n\n      @defaultValue `false`\n     */\n    verbose?: boolean\n  }) {\n    this.id = `VotingClassifier${crypto.randomUUID().split('-')[0]}`\n    this.opts = opts || {}\n  }\n\n  get py(): PythonBridge {\n    return this._py\n  }\n\n  set py(pythonBridge: PythonBridge) {\n    this._py = pythonBridge\n  }\n\n  /**\n    Initializes the underlying Python resources.\n\n    This instance is not usable until the `Promise` returned by `init()` resolves.\n   */\n  async init(py: PythonBridge): Promise<void> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This VotingClassifier instance has already been disposed'\n      )\n    }\n\n    if (this._isInitialized) {\n      return\n    }\n\n    if (!py) {\n      throw new Error('VotingClassifier.init requires a PythonBridge instance')\n    }\n\n    this._py = py\n\n    await this._py.ex`\nimport numpy as np\nfrom sklearn.ensemble import VotingClassifier\ntry: bridgeVotingClassifier\nexcept NameError: bridgeVotingClassifier = {}\n`\n\n    // set up constructor params\n    await this._py.ex`ctor_VotingClassifier = {'estimators': ${\n      this.opts['estimators'] ?? undefined\n    }, 'voting': ${this.opts['voting'] ?? undefined}, 'weights': np.array(${\n      this.opts['weights'] ?? undefined\n    }) if ${this.opts['weights'] !== undefined} else None, 'n_jobs': ${\n      this.opts['n_jobs'] ?? undefined\n    }, 'flatten_transform': ${\n      this.opts['flatten_transform'] ?? undefined\n    }, 'verbose': ${this.opts['verbose'] ?? undefined}}\n\nctor_VotingClassifier = {k: v for k, v in ctor_VotingClassifier.items() if v is not None}`\n\n    await this._py\n      .ex`bridgeVotingClassifier[${this.id}] = VotingClassifier(**ctor_VotingClassifier)`\n\n    this._isInitialized = true\n  }\n\n  /**\n    Disposes of the underlying Python resources.\n\n    Once `dispose()` is called, the instance is no longer usable.\n   */\n  async dispose() {\n    if (this._isDisposed) {\n      return\n    }\n\n    if (!this._isInitialized) {\n      return\n    }\n\n    await this._py.ex`del bridgeVotingClassifier[${this.id}]`\n\n    this._isDisposed = true\n  }\n\n  /**\n    Fit the estimators.\n   */\n  async fit(opts: {\n    /**\n      Training vectors, where `n\\_samples` is the number of samples and `n\\_features` is the number of features.\n     */\n    X?: ArrayLike | SparseMatrix[]\n\n    /**\n      Target values.\n     */\n    y?: ArrayLike\n\n    /**\n      Sample weights. If `undefined`, then samples are equally weighted. Note that this is supported only if all underlying estimators support sample weights.\n     */\n    sample_weight?: ArrayLike\n  }): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This VotingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('VotingClassifier must call init() before fit()')\n    }\n\n    // set up method params\n    await this._py.ex`pms_VotingClassifier_fit = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None, 'y': np.array(${\n      opts['y'] ?? undefined\n    }) if ${opts['y'] !== undefined} else None, 'sample_weight': np.array(${\n      opts['sample_weight'] ?? undefined\n    }) if ${opts['sample_weight'] !== undefined} else None}\n\npms_VotingClassifier_fit = {k: v for k, v in pms_VotingClassifier_fit.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_VotingClassifier_fit = bridgeVotingClassifier[${this.id}].fit(**pms_VotingClassifier_fit)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_VotingClassifier_fit.tolist() if hasattr(res_VotingClassifier_fit, 'tolist') else res_VotingClassifier_fit`\n  }\n\n  /**\n    Return class labels or probabilities for each estimator.\n\n    Return predictions for X for each estimator.\n   */\n  async fit_transform(opts: {\n    /**\n      Input samples.\n     */\n    X?: ArrayLike | SparseMatrix[]\n\n    /**\n      Target values (`undefined` for unsupervised transformations).\n     */\n    y?: NDArray\n\n    /**\n      Additional fit parameters.\n     */\n    fit_params?: any\n  }): Promise<any[]> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This VotingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'VotingClassifier must call init() before fit_transform()'\n      )\n    }\n\n    // set up method params\n    await this._py.ex`pms_VotingClassifier_fit_transform = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None, 'y': np.array(${\n      opts['y'] ?? undefined\n    }) if ${opts['y'] !== undefined} else None, 'fit_params': ${\n      opts['fit_params'] ?? undefined\n    }}\n\npms_VotingClassifier_fit_transform = {k: v for k, v in pms_VotingClassifier_fit_transform.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_VotingClassifier_fit_transform = bridgeVotingClassifier[${this.id}].fit_transform(**pms_VotingClassifier_fit_transform)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_VotingClassifier_fit_transform.tolist() if hasattr(res_VotingClassifier_fit_transform, 'tolist') else res_VotingClassifier_fit_transform`\n  }\n\n  /**\n    Get output feature names for transformation.\n   */\n  async get_feature_names_out(opts: {\n    /**\n      Not used, present here for API consistency by convention.\n     */\n    input_features?: any\n  }): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This VotingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'VotingClassifier must call init() before get_feature_names_out()'\n      )\n    }\n\n    // set up method params\n    await this._py\n      .ex`pms_VotingClassifier_get_feature_names_out = {'input_features': ${\n      opts['input_features'] ?? undefined\n    }}\n\npms_VotingClassifier_get_feature_names_out = {k: v for k, v in pms_VotingClassifier_get_feature_names_out.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_VotingClassifier_get_feature_names_out = bridgeVotingClassifier[${this.id}].get_feature_names_out(**pms_VotingClassifier_get_feature_names_out)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_VotingClassifier_get_feature_names_out.tolist() if hasattr(res_VotingClassifier_get_feature_names_out, 'tolist') else res_VotingClassifier_get_feature_names_out`\n  }\n\n  /**\n    Predict class labels for X.\n   */\n  async predict(opts: {\n    /**\n      The input samples.\n     */\n    X?: ArrayLike | SparseMatrix[]\n  }): Promise<ArrayLike> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This VotingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('VotingClassifier must call init() before predict()')\n    }\n\n    // set up method params\n    await this._py.ex`pms_VotingClassifier_predict = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None}\n\npms_VotingClassifier_predict = {k: v for k, v in pms_VotingClassifier_predict.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_VotingClassifier_predict = bridgeVotingClassifier[${this.id}].predict(**pms_VotingClassifier_predict)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_VotingClassifier_predict.tolist() if hasattr(res_VotingClassifier_predict, 'tolist') else res_VotingClassifier_predict`\n  }\n\n  /**\n    Compute probabilities of possible outcomes for samples in X.\n   */\n  async predict_proba(opts: {\n    /**\n      The input samples.\n     */\n    X?: ArrayLike | SparseMatrix[]\n  }): Promise<ArrayLike[]> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This VotingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'VotingClassifier must call init() before predict_proba()'\n      )\n    }\n\n    // set up method params\n    await this._py.ex`pms_VotingClassifier_predict_proba = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None}\n\npms_VotingClassifier_predict_proba = {k: v for k, v in pms_VotingClassifier_predict_proba.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_VotingClassifier_predict_proba = bridgeVotingClassifier[${this.id}].predict_proba(**pms_VotingClassifier_predict_proba)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_VotingClassifier_predict_proba.tolist() if hasattr(res_VotingClassifier_predict_proba, 'tolist') else res_VotingClassifier_predict_proba`\n  }\n\n  /**\n    Return the mean accuracy on the given test data and labels.\n\n    In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted.\n   */\n  async score(opts: {\n    /**\n      Test samples.\n     */\n    X?: ArrayLike[]\n\n    /**\n      True labels for `X`.\n     */\n    y?: ArrayLike\n\n    /**\n      Sample weights.\n     */\n    sample_weight?: ArrayLike\n  }): Promise<number> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This VotingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('VotingClassifier must call init() before score()')\n    }\n\n    // set up method params\n    await this._py.ex`pms_VotingClassifier_score = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None, 'y': np.array(${\n      opts['y'] ?? undefined\n    }) if ${opts['y'] !== undefined} else None, 'sample_weight': np.array(${\n      opts['sample_weight'] ?? undefined\n    }) if ${opts['sample_weight'] !== undefined} else None}\n\npms_VotingClassifier_score = {k: v for k, v in pms_VotingClassifier_score.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_VotingClassifier_score = bridgeVotingClassifier[${this.id}].score(**pms_VotingClassifier_score)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_VotingClassifier_score.tolist() if hasattr(res_VotingClassifier_score, 'tolist') else res_VotingClassifier_score`\n  }\n\n  /**\n    Set output container.\n\n    See [Introducing the set\\_output API](../../auto_examples/miscellaneous/plot_set_output.html#sphx-glr-auto-examples-miscellaneous-plot-set-output-py) for an example on how to use the API.\n   */\n  async set_output(opts: {\n    /**\n      Configure output of `transform` and `fit\\_transform`.\n     */\n    transform?: 'default' | 'pandas'\n  }): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This VotingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('VotingClassifier must call init() before set_output()')\n    }\n\n    // set up method params\n    await this._py.ex`pms_VotingClassifier_set_output = {'transform': ${\n      opts['transform'] ?? undefined\n    }}\n\npms_VotingClassifier_set_output = {k: v for k, v in pms_VotingClassifier_set_output.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_VotingClassifier_set_output = bridgeVotingClassifier[${this.id}].set_output(**pms_VotingClassifier_set_output)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_VotingClassifier_set_output.tolist() if hasattr(res_VotingClassifier_set_output, 'tolist') else res_VotingClassifier_set_output`\n  }\n\n  /**\n    Return class labels or probabilities for X for each estimator.\n   */\n  async transform(opts: {\n    /**\n      Training vectors, where `n\\_samples` is the number of samples and `n\\_features` is the number of features.\n     */\n    X?: ArrayLike | SparseMatrix[]\n  }): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This VotingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('VotingClassifier must call init() before transform()')\n    }\n\n    // set up method params\n    await this._py.ex`pms_VotingClassifier_transform = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None}\n\npms_VotingClassifier_transform = {k: v for k, v in pms_VotingClassifier_transform.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_VotingClassifier_transform = bridgeVotingClassifier[${this.id}].transform(**pms_VotingClassifier_transform)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_VotingClassifier_transform.tolist() if hasattr(res_VotingClassifier_transform, 'tolist') else res_VotingClassifier_transform`\n  }\n\n  /**\n    The collection of fitted sub-estimators as defined in `estimators` that are not ‘drop’.\n   */\n  get estimators_(): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This VotingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'VotingClassifier must call init() before accessing estimators_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_VotingClassifier_estimators_ = bridgeVotingClassifier[${this.id}].estimators_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_VotingClassifier_estimators_.tolist() if hasattr(attr_VotingClassifier_estimators_, 'tolist') else attr_VotingClassifier_estimators_`\n    })()\n  }\n\n  /**\n    Attribute to access any fitted sub-estimators by name.\n   */\n  get named_estimators_(): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This VotingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'VotingClassifier must call init() before accessing named_estimators_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_VotingClassifier_named_estimators_ = bridgeVotingClassifier[${this.id}].named_estimators_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_VotingClassifier_named_estimators_.tolist() if hasattr(attr_VotingClassifier_named_estimators_, 'tolist') else attr_VotingClassifier_named_estimators_`\n    })()\n  }\n\n  /**\n    Transformer used to encode the labels during fit and decode during prediction.\n   */\n  get le_(): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This VotingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('VotingClassifier must call init() before accessing le_')\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_VotingClassifier_le_ = bridgeVotingClassifier[${this.id}].le_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_VotingClassifier_le_.tolist() if hasattr(attr_VotingClassifier_le_, 'tolist') else attr_VotingClassifier_le_`\n    })()\n  }\n\n  /**\n    The classes labels.\n   */\n  get classes_(): Promise<NDArray> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This VotingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'VotingClassifier must call init() before accessing classes_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_VotingClassifier_classes_ = bridgeVotingClassifier[${this.id}].classes_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_VotingClassifier_classes_.tolist() if hasattr(attr_VotingClassifier_classes_, 'tolist') else attr_VotingClassifier_classes_`\n    })()\n  }\n\n  /**\n    Names of features seen during [fit](../../glossary.html#term-fit). Only defined if the underlying estimators expose such an attribute when fit.\n   */\n  get feature_names_in_(): Promise<NDArray> {\n    if (this._isDisposed) {\n      throw new Error(\n        'This VotingClassifier instance has already been disposed'\n      )\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'VotingClassifier must call init() before accessing feature_names_in_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_VotingClassifier_feature_names_in_ = bridgeVotingClassifier[${this.id}].feature_names_in_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_VotingClassifier_feature_names_in_.tolist() if hasattr(attr_VotingClassifier_feature_names_in_, 'tolist') else attr_VotingClassifier_feature_names_in_`\n    })()\n  }\n}\n","/* eslint-disable */\n/* NOTE: This file is auto-generated. Do not edit it directly. */\n\nimport crypto from 'node:crypto'\n\nimport { PythonBridge, NDArray, ArrayLike, SparseMatrix } from '@/sklearn/types'\n\n/**\n  Prediction voting regressor for unfitted estimators.\n\n  A voting regressor is an ensemble meta-estimator that fits several base regressors, each on the whole dataset. Then it averages the individual predictions to form a final prediction.\n\n  Read more in the [User Guide](../ensemble.html#voting-regressor).\n\n  [Python Reference](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingRegressor.html)\n */\nexport class VotingRegressor {\n  id: string\n  opts: any\n\n  _py: PythonBridge\n  _isInitialized: boolean = false\n  _isDisposed: boolean = false\n\n  constructor(opts?: {\n    /**\n      Invoking the `fit` method on the `VotingRegressor` will fit clones of those original estimators that will be stored in the class attribute `self.estimators\\_`. An estimator can be set to `'drop'` using [`set\\_params`](#sklearn.ensemble.VotingRegressor.set_params \"sklearn.ensemble.VotingRegressor.set_params\").\n     */\n    estimators?: any\n\n    /**\n      Sequence of weights (`float` or `int`) to weight the occurrences of predicted values before averaging. Uses uniform weights if `undefined`.\n     */\n    weights?: ArrayLike\n\n    /**\n      The number of jobs to run in parallel for `fit`. `undefined` means 1 unless in a [`joblib.parallel\\_backend`](https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend \"(in joblib v1.3.0.dev0)\") context. `\\-1` means using all processors. See [Glossary](../../glossary.html#term-n_jobs) for more details.\n     */\n    n_jobs?: number\n\n    /**\n      If `true`, the time elapsed while fitting will be printed as it is completed.\n\n      @defaultValue `false`\n     */\n    verbose?: boolean\n  }) {\n    this.id = `VotingRegressor${crypto.randomUUID().split('-')[0]}`\n    this.opts = opts || {}\n  }\n\n  get py(): PythonBridge {\n    return this._py\n  }\n\n  set py(pythonBridge: PythonBridge) {\n    this._py = pythonBridge\n  }\n\n  /**\n    Initializes the underlying Python resources.\n\n    This instance is not usable until the `Promise` returned by `init()` resolves.\n   */\n  async init(py: PythonBridge): Promise<void> {\n    if (this._isDisposed) {\n      throw new Error('This VotingRegressor instance has already been disposed')\n    }\n\n    if (this._isInitialized) {\n      return\n    }\n\n    if (!py) {\n      throw new Error('VotingRegressor.init requires a PythonBridge instance')\n    }\n\n    this._py = py\n\n    await this._py.ex`\nimport numpy as np\nfrom sklearn.ensemble import VotingRegressor\ntry: bridgeVotingRegressor\nexcept NameError: bridgeVotingRegressor = {}\n`\n\n    // set up constructor params\n    await this._py.ex`ctor_VotingRegressor = {'estimators': ${\n      this.opts['estimators'] ?? undefined\n    }, 'weights': np.array(${this.opts['weights'] ?? undefined}) if ${\n      this.opts['weights'] !== undefined\n    } else None, 'n_jobs': ${this.opts['n_jobs'] ?? undefined}, 'verbose': ${\n      this.opts['verbose'] ?? undefined\n    }}\n\nctor_VotingRegressor = {k: v for k, v in ctor_VotingRegressor.items() if v is not None}`\n\n    await this._py\n      .ex`bridgeVotingRegressor[${this.id}] = VotingRegressor(**ctor_VotingRegressor)`\n\n    this._isInitialized = true\n  }\n\n  /**\n    Disposes of the underlying Python resources.\n\n    Once `dispose()` is called, the instance is no longer usable.\n   */\n  async dispose() {\n    if (this._isDisposed) {\n      return\n    }\n\n    if (!this._isInitialized) {\n      return\n    }\n\n    await this._py.ex`del bridgeVotingRegressor[${this.id}]`\n\n    this._isDisposed = true\n  }\n\n  /**\n    Fit the estimators.\n   */\n  async fit(opts: {\n    /**\n      Training vectors, where `n\\_samples` is the number of samples and `n\\_features` is the number of features.\n     */\n    X?: ArrayLike | SparseMatrix[]\n\n    /**\n      Target values.\n     */\n    y?: ArrayLike\n\n    /**\n      Sample weights. If `undefined`, then samples are equally weighted. Note that this is supported only if all underlying estimators support sample weights.\n     */\n    sample_weight?: ArrayLike\n  }): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error('This VotingRegressor instance has already been disposed')\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('VotingRegressor must call init() before fit()')\n    }\n\n    // set up method params\n    await this._py.ex`pms_VotingRegressor_fit = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None, 'y': np.array(${\n      opts['y'] ?? undefined\n    }) if ${opts['y'] !== undefined} else None, 'sample_weight': np.array(${\n      opts['sample_weight'] ?? undefined\n    }) if ${opts['sample_weight'] !== undefined} else None}\n\npms_VotingRegressor_fit = {k: v for k, v in pms_VotingRegressor_fit.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_VotingRegressor_fit = bridgeVotingRegressor[${this.id}].fit(**pms_VotingRegressor_fit)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_VotingRegressor_fit.tolist() if hasattr(res_VotingRegressor_fit, 'tolist') else res_VotingRegressor_fit`\n  }\n\n  /**\n    Return class labels or probabilities for each estimator.\n\n    Return predictions for X for each estimator.\n   */\n  async fit_transform(opts: {\n    /**\n      Input samples.\n     */\n    X?: ArrayLike | SparseMatrix[]\n\n    /**\n      Target values (`undefined` for unsupervised transformations).\n     */\n    y?: NDArray\n\n    /**\n      Additional fit parameters.\n     */\n    fit_params?: any\n  }): Promise<any[]> {\n    if (this._isDisposed) {\n      throw new Error('This VotingRegressor instance has already been disposed')\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('VotingRegressor must call init() before fit_transform()')\n    }\n\n    // set up method params\n    await this._py.ex`pms_VotingRegressor_fit_transform = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None, 'y': np.array(${\n      opts['y'] ?? undefined\n    }) if ${opts['y'] !== undefined} else None, 'fit_params': ${\n      opts['fit_params'] ?? undefined\n    }}\n\npms_VotingRegressor_fit_transform = {k: v for k, v in pms_VotingRegressor_fit_transform.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_VotingRegressor_fit_transform = bridgeVotingRegressor[${this.id}].fit_transform(**pms_VotingRegressor_fit_transform)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_VotingRegressor_fit_transform.tolist() if hasattr(res_VotingRegressor_fit_transform, 'tolist') else res_VotingRegressor_fit_transform`\n  }\n\n  /**\n    Get output feature names for transformation.\n   */\n  async get_feature_names_out(opts: {\n    /**\n      Not used, present here for API consistency by convention.\n     */\n    input_features?: any\n  }): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error('This VotingRegressor instance has already been disposed')\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'VotingRegressor must call init() before get_feature_names_out()'\n      )\n    }\n\n    // set up method params\n    await this._py\n      .ex`pms_VotingRegressor_get_feature_names_out = {'input_features': ${\n      opts['input_features'] ?? undefined\n    }}\n\npms_VotingRegressor_get_feature_names_out = {k: v for k, v in pms_VotingRegressor_get_feature_names_out.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_VotingRegressor_get_feature_names_out = bridgeVotingRegressor[${this.id}].get_feature_names_out(**pms_VotingRegressor_get_feature_names_out)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_VotingRegressor_get_feature_names_out.tolist() if hasattr(res_VotingRegressor_get_feature_names_out, 'tolist') else res_VotingRegressor_get_feature_names_out`\n  }\n\n  /**\n    Predict regression target for X.\n\n    The predicted regression target of an input sample is computed as the mean predicted regression targets of the estimators in the ensemble.\n   */\n  async predict(opts: {\n    /**\n      The input samples.\n     */\n    X?: ArrayLike | SparseMatrix[]\n  }): Promise<NDArray> {\n    if (this._isDisposed) {\n      throw new Error('This VotingRegressor instance has already been disposed')\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('VotingRegressor must call init() before predict()')\n    }\n\n    // set up method params\n    await this._py.ex`pms_VotingRegressor_predict = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None}\n\npms_VotingRegressor_predict = {k: v for k, v in pms_VotingRegressor_predict.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_VotingRegressor_predict = bridgeVotingRegressor[${this.id}].predict(**pms_VotingRegressor_predict)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_VotingRegressor_predict.tolist() if hasattr(res_VotingRegressor_predict, 'tolist') else res_VotingRegressor_predict`\n  }\n\n  /**\n    Return the coefficient of determination of the prediction.\n\n    The coefficient of determination \\\\(R^2\\\\) is defined as \\\\((1 - \\\\frac{u}{v})\\\\), where \\\\(u\\\\) is the residual sum of squares `((y\\_true \\- y\\_pred)\\*\\* 2).sum()` and \\\\(v\\\\) is the total sum of squares `((y\\_true \\- y\\_true.mean()) \\*\\* 2).sum()`. The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of `y`, disregarding the input features, would get a \\\\(R^2\\\\) score of 0.0.\n   */\n  async score(opts: {\n    /**\n      Test samples. For some estimators this may be a precomputed kernel matrix or a list of generic objects instead with shape `(n\\_samples, n\\_samples\\_fitted)`, where `n\\_samples\\_fitted` is the number of samples used in the fitting for the estimator.\n     */\n    X?: ArrayLike[]\n\n    /**\n      True values for `X`.\n     */\n    y?: ArrayLike\n\n    /**\n      Sample weights.\n     */\n    sample_weight?: ArrayLike\n  }): Promise<number> {\n    if (this._isDisposed) {\n      throw new Error('This VotingRegressor instance has already been disposed')\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('VotingRegressor must call init() before score()')\n    }\n\n    // set up method params\n    await this._py.ex`pms_VotingRegressor_score = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None, 'y': np.array(${\n      opts['y'] ?? undefined\n    }) if ${opts['y'] !== undefined} else None, 'sample_weight': np.array(${\n      opts['sample_weight'] ?? undefined\n    }) if ${opts['sample_weight'] !== undefined} else None}\n\npms_VotingRegressor_score = {k: v for k, v in pms_VotingRegressor_score.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_VotingRegressor_score = bridgeVotingRegressor[${this.id}].score(**pms_VotingRegressor_score)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_VotingRegressor_score.tolist() if hasattr(res_VotingRegressor_score, 'tolist') else res_VotingRegressor_score`\n  }\n\n  /**\n    Set output container.\n\n    See [Introducing the set\\_output API](../../auto_examples/miscellaneous/plot_set_output.html#sphx-glr-auto-examples-miscellaneous-plot-set-output-py) for an example on how to use the API.\n   */\n  async set_output(opts: {\n    /**\n      Configure output of `transform` and `fit\\_transform`.\n     */\n    transform?: 'default' | 'pandas'\n  }): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error('This VotingRegressor instance has already been disposed')\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('VotingRegressor must call init() before set_output()')\n    }\n\n    // set up method params\n    await this._py.ex`pms_VotingRegressor_set_output = {'transform': ${\n      opts['transform'] ?? undefined\n    }}\n\npms_VotingRegressor_set_output = {k: v for k, v in pms_VotingRegressor_set_output.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_VotingRegressor_set_output = bridgeVotingRegressor[${this.id}].set_output(**pms_VotingRegressor_set_output)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_VotingRegressor_set_output.tolist() if hasattr(res_VotingRegressor_set_output, 'tolist') else res_VotingRegressor_set_output`\n  }\n\n  /**\n    Return predictions for X for each estimator.\n   */\n  async transform(opts: {\n    /**\n      The input samples.\n     */\n    X?: ArrayLike | SparseMatrix[]\n  }): Promise<NDArray[]> {\n    if (this._isDisposed) {\n      throw new Error('This VotingRegressor instance has already been disposed')\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('VotingRegressor must call init() before transform()')\n    }\n\n    // set up method params\n    await this._py.ex`pms_VotingRegressor_transform = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None}\n\npms_VotingRegressor_transform = {k: v for k, v in pms_VotingRegressor_transform.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_VotingRegressor_transform = bridgeVotingRegressor[${this.id}].transform(**pms_VotingRegressor_transform)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_VotingRegressor_transform.tolist() if hasattr(res_VotingRegressor_transform, 'tolist') else res_VotingRegressor_transform`\n  }\n\n  /**\n    The collection of fitted sub-estimators as defined in `estimators` that are not ‘drop’.\n   */\n  get estimators_(): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error('This VotingRegressor instance has already been disposed')\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'VotingRegressor must call init() before accessing estimators_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_VotingRegressor_estimators_ = bridgeVotingRegressor[${this.id}].estimators_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_VotingRegressor_estimators_.tolist() if hasattr(attr_VotingRegressor_estimators_, 'tolist') else attr_VotingRegressor_estimators_`\n    })()\n  }\n\n  /**\n    Attribute to access any fitted sub-estimators by name.\n   */\n  get named_estimators_(): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error('This VotingRegressor instance has already been disposed')\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'VotingRegressor must call init() before accessing named_estimators_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_VotingRegressor_named_estimators_ = bridgeVotingRegressor[${this.id}].named_estimators_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_VotingRegressor_named_estimators_.tolist() if hasattr(attr_VotingRegressor_named_estimators_, 'tolist') else attr_VotingRegressor_named_estimators_`\n    })()\n  }\n\n  /**\n    Names of features seen during [fit](../../glossary.html#term-fit). Only defined if the underlying estimators expose such an attribute when fit.\n   */\n  get feature_names_in_(): Promise<NDArray> {\n    if (this._isDisposed) {\n      throw new Error('This VotingRegressor instance has already been disposed')\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'VotingRegressor must call init() before accessing feature_names_in_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_VotingRegressor_feature_names_in_ = bridgeVotingRegressor[${this.id}].feature_names_in_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_VotingRegressor_feature_names_in_.tolist() if hasattr(attr_VotingRegressor_feature_names_in_, 'tolist') else attr_VotingRegressor_feature_names_in_`\n    })()\n  }\n}\n"],"mappings":";AAGA,OAAO,YAAY;AAeZ,IAAM,qBAAN,MAAyB;AAAA,EAQ9B,YAAY,MAoCT;AAvCH,0BAA0B;AAC1B,uBAAuB;AAuCrB,SAAK,KAAK,qBAAqB,OAAO,WAAW,EAAE,MAAM,GAAG,EAAE,CAAC;AAC/D,SAAK,OAAO,QAAQ,CAAC;AAAA,EACvB;AAAA,EAEA,IAAI,KAAmB;AACrB,WAAO,KAAK;AAAA,EACd;AAAA,EAEA,IAAI,GAAG,cAA4B;AACjC,SAAK,MAAM;AAAA,EACb;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,KAAK,IAAiC;AAC1C,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,KAAK,gBAAgB;AACvB;AAAA,IACF;AAEA,QAAI,CAAC,IAAI;AACP,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,SAAK,MAAM;AAEX,UAAM,KAAK,IAAI;AAAA;AAAA;AAAA;AAAA;AAAA;AAQf,UAAM,KAAK,IAAI,6CACb,KAAK,KAAK,WAAW,KAAK,2BAE1B,KAAK,KAAK,cAAc,KAAK,4BAE7B,KAAK,KAAK,eAAe,KAAK,wBACd,KAAK,KAAK,WAAW,KAAK,2BAC1C,KAAK,KAAK,cAAc,KAAK,6BACR,KAAK,KAAK,gBAAgB,KAAK;AAAA;AAAA;AAItD,UAAM,KAAK,IACR,8BAA8B,KAAK;AAEtC,SAAK,iBAAiB;AAAA,EACxB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,UAAU;AACd,QAAI,KAAK,aAAa;AACpB;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB;AAAA,IACF;AAEA,UAAM,KAAK,IAAI,kCAAkC,KAAK;AAEtD,SAAK,cAAc;AAAA,EACrB;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,kBAAkB,MAKP;AACf,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IACR,+DACD,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM;AAAA;AAAA;AAKtB,UAAM,KAAK,IACR,yEAAyE,KAAK;AAGjF,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,IAAI,MAeO;AACf,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,kDAAkD;AAAA,IACpE;AAGA,UAAM,KAAK,IAAI,iDACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,mCACpB,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,+CACpB,KAAK,eAAe,KAAK,cACnB,KAAK,eAAe,MAAM;AAAA;AAAA;AAKlC,UAAM,KAAK,IACR,2DAA2D,KAAK;AAGnE,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,QAAQ,MAKO;AACnB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,sDAAsD;AAAA,IACxE;AAGA,UAAM,KAAK,IAAI,qDACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM;AAAA;AAAA;AAKtB,UAAM,KAAK,IACR,+DAA+D,KAAK;AAGvE,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,kBAAkB,MAKD;AACrB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IACR,+DACD,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM;AAAA;AAAA;AAKtB,UAAM,KAAK,IACR,yEAAyE,KAAK;AAGjF,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,cAAc,MAKG;AACrB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IAAI,2DACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM;AAAA;AAAA;AAKtB,UAAM,KAAK,IACR,qEAAqE,KAAK;AAG7E,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,MAAM,MAeQ;AAClB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,oDAAoD;AAAA,IACtE;AAGA,UAAM,KAAK,IAAI,mDACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,mCACpB,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,+CACpB,KAAK,eAAe,KAAK,cACnB,KAAK,eAAe,MAAM;AAAA;AAAA;AAKlC,UAAM,KAAK,IACR,6DAA6D,KAAK;AAGrE,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,yBAAyB,MAKZ;AACjB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IACR,sEACD,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM;AAAA;AAAA;AAKtB,UAAM,KAAK,IACR,gFAAgF,KAAK;AAGxF,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EASA,MAAM,eAAe,MAKF;AACjB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IAAI,4DACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM;AAAA;AAAA;AAKtB,UAAM,KAAK,IACR,sEAAsE,KAAK;AAG9E,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EASA,MAAM,qBAAqB,MAKR;AACjB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IACR,kEACD,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM;AAAA;AAAA;AAKtB,UAAM,KAAK,IACR,4EAA4E,KAAK;AAGpF,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,aAAa,MAeC;AAClB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IAAI,0DACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,mCACpB,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,+CACpB,KAAK,eAAe,KAAK,cACnB,KAAK,eAAe,MAAM;AAAA;AAAA;AAKlC,UAAM,KAAK,IACR,oEAAoE,KAAK;AAG5E,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,aAA2B;AAC7B,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,mEAAmE,KAAK;AAG3E,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,cAA4B;AAC9B,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,oEAAoE,KAAK;AAG5E,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,WAA6B;AAC/B,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,iEAAiE,KAAK;AAGzE,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,aAA8B;AAChC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,mEAAmE,KAAK;AAG3E,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,qBAAmC;AACrC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,2EAA2E,KAAK;AAGnF,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,oBAAkC;AACpC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,0EAA0E,KAAK;AAGlF,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,iBAAkC;AACpC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,uEAAuE,KAAK;AAG/E,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,oBAAsC;AACxC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,0EAA0E,KAAK;AAGlF,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AACF;;;AClxBA,OAAOA,aAAY;AAeZ,IAAM,oBAAN,MAAwB;AAAA,EAQ7B,YAAY,MAoCT;AAvCH,0BAA0B;AAC1B,uBAAuB;AAuCrB,SAAK,KAAK,oBAAoBA,QAAO,WAAW,EAAE,MAAM,GAAG,EAAE,CAAC;AAC9D,SAAK,OAAO,QAAQ,CAAC;AAAA,EACvB;AAAA,EAEA,IAAI,KAAmB;AACrB,WAAO,KAAK;AAAA,EACd;AAAA,EAEA,IAAI,GAAG,cAA4B;AACjC,SAAK,MAAM;AAAA,EACb;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,KAAK,IAAiC;AAC1C,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,KAAK,gBAAgB;AACvB;AAAA,IACF;AAEA,QAAI,CAAC,IAAI;AACP,YAAM,IAAI,MAAM,yDAAyD;AAAA,IAC3E;AAEA,SAAK,MAAM;AAEX,UAAM,KAAK,IAAI;AAAA;AAAA;AAAA;AAAA;AAAA;AAQf,UAAM,KAAK,IAAI,4CACb,KAAK,KAAK,WAAW,KAAK,2BAE1B,KAAK,KAAK,cAAc,KAAK,4BACT,KAAK,KAAK,eAAe,KAAK,mBAClD,KAAK,KAAK,MAAM,KAAK,2BAErB,KAAK,KAAK,cAAc,KAAK,6BACR,KAAK,KAAK,gBAAgB,KAAK;AAAA;AAAA;AAItD,UAAM,KAAK,IACR,6BAA6B,KAAK;AAErC,SAAK,iBAAiB;AAAA,EACxB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,UAAU;AACd,QAAI,KAAK,aAAa;AACpB;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB;AAAA,IACF;AAEA,UAAM,KAAK,IAAI,iCAAiC,KAAK;AAErD,SAAK,cAAc;AAAA,EACrB;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,IAAI,MAeO;AACf,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,iDAAiD;AAAA,IACnE;AAGA,UAAM,KAAK,IAAI,gDACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,mCACpB,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,+CACpB,KAAK,eAAe,KAAK,cACnB,KAAK,eAAe,MAAM;AAAA;AAAA;AAKlC,UAAM,KAAK,IACR,yDAAyD,KAAK;AAGjE,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,QAAQ,MAKO;AACnB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,qDAAqD;AAAA,IACvE;AAGA,UAAM,KAAK,IAAI,oDACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM;AAAA;AAAA;AAKtB,UAAM,KAAK,IACR,6DAA6D,KAAK;AAGrE,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,MAAM,MAeQ;AAClB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,mDAAmD;AAAA,IACrE;AAGA,UAAM,KAAK,IAAI,kDACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,mCACpB,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,+CACpB,KAAK,eAAe,KAAK,cACnB,KAAK,eAAe,MAAM;AAAA;AAAA;AAKlC,UAAM,KAAK,IACR,2DAA2D,KAAK;AAGnE,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EASA,MAAM,eAAe,MAKF;AACjB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IAAI,2DACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM;AAAA;AAAA;AAKtB,UAAM,KAAK,IACR,oEAAoE,KAAK;AAG5E,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,aAAa,MAeC;AAClB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IAAI,yDACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,mCACpB,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,+CACpB,KAAK,eAAe,KAAK,cACnB,KAAK,eAAe,MAAM;AAAA;AAAA;AAKlC,UAAM,KAAK,IACR,kEAAkE,KAAK;AAG1E,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,aAA2B;AAC7B,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,iEAAiE,KAAK;AAGzE,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,cAA4B;AAC9B,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,kEAAkE,KAAK;AAG1E,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,qBAAmC;AACrC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,yEAAyE,KAAK;AAGjF,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,oBAAkC;AACpC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,wEAAwE,KAAK;AAGhF,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,iBAAkC;AACpC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,qEAAqE,KAAK;AAG7E,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,oBAAsC;AACxC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,wEAAwE,KAAK;AAGhF,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AACF;;;ACnhBA,OAAOC,aAAY;AAeZ,IAAM,oBAAN,MAAwB;AAAA,EAQ7B,YAAY,MA8ET;AAjFH,0BAA0B;AAC1B,uBAAuB;AAiFrB,SAAK,KAAK,oBAAoBA,QAAO,WAAW,EAAE,MAAM,GAAG,EAAE,CAAC;AAC9D,SAAK,OAAO,QAAQ,CAAC;AAAA,EACvB;AAAA,EAEA,IAAI,KAAmB;AACrB,WAAO,KAAK;AAAA,EACd;AAAA,EAEA,IAAI,GAAG,cAA4B;AACjC,SAAK,MAAM;AAAA,EACb;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,KAAK,IAAiC;AAC1C,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,KAAK,gBAAgB;AACvB;AAAA,IACF;AAEA,QAAI,CAAC,IAAI;AACP,YAAM,IAAI,MAAM,yDAAyD;AAAA,IAC3E;AAEA,SAAK,MAAM;AAEX,UAAM,KAAK,IAAI;AAAA;AAAA;AAAA;AAAA;AAAA;AAQf,UAAM,KAAK,IAAI,4CACb,KAAK,KAAK,WAAW,KAAK,2BAE1B,KAAK,KAAK,cAAc,KAAK,0BAE7B,KAAK,KAAK,aAAa,KAAK,2BAE5B,KAAK,KAAK,cAAc,KAAK,wBAE7B,KAAK,KAAK,WAAW,KAAK,iCAE1B,KAAK,KAAK,oBAAoB,KAAK,wBACnB,KAAK,KAAK,WAAW,KAAK,yBAC1C,KAAK,KAAK,YAAY,KAAK,qBACd,KAAK,KAAK,QAAQ,KAAK,2BACpC,KAAK,KAAK,cAAc,KAAK,sBACf,KAAK,KAAK,SAAS,KAAK,6BACtC,KAAK,KAAK,gBAAgB,KAAK;AAAA;AAAA;AAKjC,UAAM,KAAK,IACR,6BAA6B,KAAK;AAErC,SAAK,iBAAiB;AAAA,EACxB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,UAAU;AACd,QAAI,KAAK,aAAa;AACpB;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB;AAAA,IACF;AAEA,UAAM,KAAK,IAAI,iCAAiC,KAAK;AAErD,SAAK,cAAc;AAAA,EACrB;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,kBAAkB,MAKD;AACrB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IACR,8DACD,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM;AAAA;AAAA;AAKtB,UAAM,KAAK,IACR,uEAAuE,KAAK;AAG/E,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,IAAI,MAeO;AACf,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,iDAAiD;AAAA,IACnE;AAGA,UAAM,KAAK,IAAI,gDACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,mCACpB,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,+CACpB,KAAK,eAAe,KAAK,cACnB,KAAK,eAAe,MAAM;AAAA;AAAA;AAKlC,UAAM,KAAK,IACR,yDAAyD,KAAK;AAGjE,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,QAAQ,MAKO;AACnB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,qDAAqD;AAAA,IACvE;AAGA,UAAM,KAAK,IAAI,oDACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM;AAAA;AAAA;AAKtB,UAAM,KAAK,IACR,6DAA6D,KAAK;AAGrE,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,kBAAkB,MAKD;AACrB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IACR,8DACD,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM;AAAA;AAAA;AAKtB,UAAM,KAAK,IACR,uEAAuE,KAAK;AAG/E,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,cAAc,MAKG;AACrB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IAAI,0DACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM;AAAA;AAAA;AAKtB,UAAM,KAAK,IACR,mEAAmE,KAAK;AAG3E,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,MAAM,MAeQ;AAClB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,mDAAmD;AAAA,IACrE;AAGA,UAAM,KAAK,IAAI,kDACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,mCACpB,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,+CACpB,KAAK,eAAe,KAAK,cACnB,KAAK,eAAe,MAAM;AAAA;AAAA;AAKlC,UAAM,KAAK,IACR,2DAA2D,KAAK;AAGnE,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,aAA2B;AAC7B,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,iEAAiE,KAAK;AAGzE,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,iBAAkC;AACpC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,qEAAqE,KAAK;AAG7E,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,oBAAsC;AACxC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,wEAAwE,KAAK;AAGhF,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,cAA4B;AAC9B,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,kEAAkE,KAAK;AAG1E,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,uBAAqC;AACvC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,2EAA2E,KAAK;AAGnF,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,WAA6B;AAC/B,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,+DAA+D,KAAK;AAGvE,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,aAAsC;AACxC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,iEAAiE,KAAK;AAGzE,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,aAA8B;AAChC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,iEAAiE,KAAK;AAGzE,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,yBAA6C;AAC/C,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,6EAA6E,KAAK;AAGrF,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AACF;;;AC/qBA,OAAOC,aAAY;AAeZ,IAAM,mBAAN,MAAuB;AAAA,EAQ5B,YAAY,MA8ET;AAjFH,0BAA0B;AAC1B,uBAAuB;AAiFrB,SAAK,KAAK,mBAAmBA,QAAO,WAAW,EAAE,MAAM,GAAG,EAAE,CAAC;AAC7D,SAAK,OAAO,QAAQ,CAAC;AAAA,EACvB;AAAA,EAEA,IAAI,KAAmB;AACrB,WAAO,KAAK;AAAA,EACd;AAAA,EAEA,IAAI,GAAG,cAA4B;AACjC,SAAK,MAAM;AAAA,EACb;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,KAAK,IAAiC;AAC1C,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,KAAK,gBAAgB;AACvB;AAAA,IACF;AAEA,QAAI,CAAC,IAAI;AACP,YAAM,IAAI,MAAM,wDAAwD;AAAA,IAC1E;AAEA,SAAK,MAAM;AAEX,UAAM,KAAK,IAAI;AAAA;AAAA;AAAA;AAAA;AAAA;AAQf,UAAM,KAAK,IAAI,2CACb,KAAK,KAAK,WAAW,KAAK,2BAE1B,KAAK,KAAK,cAAc,KAAK,0BAE7B,KAAK,KAAK,aAAa,KAAK,2BAE5B,KAAK,KAAK,cAAc,KAAK,wBAE7B,KAAK,KAAK,WAAW,KAAK,iCAE1B,KAAK,KAAK,oBAAoB,KAAK,wBACnB,KAAK,KAAK,WAAW,KAAK,yBAC1C,KAAK,KAAK,YAAY,KAAK,qBACd,KAAK,KAAK,QAAQ,KAAK,2BACpC,KAAK,KAAK,cAAc,KAAK,sBACf,KAAK,KAAK,SAAS,KAAK,6BACtC,KAAK,KAAK,gBAAgB,KAAK;AAAA;AAAA;AAKjC,UAAM,KAAK,IACR,4BAA4B,KAAK;AAEpC,SAAK,iBAAiB;AAAA,EACxB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,UAAU;AACd,QAAI,KAAK,aAAa;AACpB;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB;AAAA,IACF;AAEA,UAAM,KAAK,IAAI,gCAAgC,KAAK;AAEpD,SAAK,cAAc;AAAA,EACrB;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,IAAI,MAeO;AACf,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,gDAAgD;AAAA,IAClE;AAGA,UAAM,KAAK,IAAI,+CACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,mCACpB,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,+CACpB,KAAK,eAAe,KAAK,cACnB,KAAK,eAAe,MAAM;AAAA;AAAA;AAKlC,UAAM,KAAK,IACR,uDAAuD,KAAK;AAG/D,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,QAAQ,MAKO;AACnB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,oDAAoD;AAAA,IACtE;AAGA,UAAM,KAAK,IAAI,mDACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM;AAAA;AAAA;AAKtB,UAAM,KAAK,IACR,2DAA2D,KAAK;AAGnE,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,MAAM,MAeQ;AAClB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,kDAAkD;AAAA,IACpE;AAGA,UAAM,KAAK,IAAI,iDACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,mCACpB,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,+CACpB,KAAK,eAAe,KAAK,cACnB,KAAK,eAAe,MAAM;AAAA;AAAA;AAKlC,UAAM,KAAK,IACR,yDAAyD,KAAK;AAGjE,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,aAA2B;AAC7B,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,+DAA+D,KAAK;AAGvE,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,iBAAkC;AACpC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,mEAAmE,KAAK;AAG3E,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,oBAAsC;AACxC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,sEAAsE,KAAK;AAG9E,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,cAA4B;AAC9B,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,gEAAgE,KAAK;AAGxE,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,uBAAqC;AACvC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,yEAAyE,KAAK;AAGjF,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,aAA8B;AAChC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,+DAA+D,KAAK;AAGvE,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,kBAAoC;AACtC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,oEAAoE,KAAK;AAG5E,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AACF;;;ACpgBA,OAAOC,aAAY;AAaZ,IAAM,uBAAN,MAA2B;AAAA,EAQhC,YAAY,MA8HT;AAjIH,0BAA0B;AAC1B,uBAAuB;AAiIrB,SAAK,KAAK,uBAAuBA,QAAO,WAAW,EAAE,MAAM,GAAG,EAAE,CAAC;AACjE,SAAK,OAAO,QAAQ,CAAC;AAAA,EACvB;AAAA,EAEA,IAAI,KAAmB;AACrB,WAAO,KAAK;AAAA,EACd;AAAA,EAEA,IAAI,GAAG,cAA4B;AACjC,SAAK,MAAM;AAAA,EACb;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,KAAK,IAAiC;AAC1C,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,KAAK,gBAAgB;AACvB;AAAA,IACF;AAEA,QAAI,CAAC,IAAI;AACP,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,SAAK,MAAM;AAEX,UAAM,KAAK,IAAI;AAAA;AAAA;AAAA;AAAA;AAAA;AAQf,UAAM,KAAK,IAAI,kDACb,KAAK,KAAK,cAAc,KAAK,wBACb,KAAK,KAAK,WAAW,KAAK,wBAC1C,KAAK,KAAK,WAAW,KAAK,gCAE1B,KAAK,KAAK,mBAAmB,KAAK,+BAElC,KAAK,KAAK,kBAAkB,KAAK,uCAEjC,KAAK,KAAK,0BAA0B,KAAK,2BAEzC,KAAK,KAAK,cAAc,KAAK,6BAE7B,KAAK,KAAK,gBAAgB,KAAK,oCAE/B,KAAK,KAAK,uBAAuB,KAAK,wBACtB,KAAK,KAAK,WAAW,KAAK,wBAC1C,KAAK,KAAK,WAAW,KAAK,qBACb,KAAK,KAAK,QAAQ,KAAK,2BACpC,KAAK,KAAK,cAAc,KAAK,sBACf,KAAK,KAAK,SAAS,KAAK,yBACtC,KAAK,KAAK,YAAY,KAAK,2BAE3B,KAAK,KAAK,cAAc,KAAK,wBACb,KAAK,KAAK,WAAW,KAAK,0BAC1C,KAAK,KAAK,aAAa,KAAK;AAAA;AAAA;AAK9B,UAAM,KAAK,IACR,gCAAgC,KAAK;AAExC,SAAK,iBAAiB;AAAA,EACxB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,UAAU;AACd,QAAI,KAAK,aAAa;AACpB;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB;AAAA,IACF;AAEA,UAAM,KAAK,IAAI,oCAAoC,KAAK;AAExD,SAAK,cAAc;AAAA,EACrB;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,MAAM,MAKW;AACrB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,sDAAsD;AAAA,IACxE;AAGA,UAAM,KAAK,IAAI,qDACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM;AAAA;AAAA;AAKtB,UAAM,KAAK,IACR,iEAAiE,KAAK;AAGzE,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,cAAc,MAKQ;AAC1B,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IAAI,6DACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM;AAAA;AAAA;AAKtB,UAAM,KAAK,IACR,yEAAyE,KAAK;AAGjF,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,IAAI,MAeO;AACf,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,oDAAoD;AAAA,IACtE;AAGA,UAAM,KAAK,IAAI,mDACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,mCACpB,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,+CACpB,KAAK,eAAe,KAAK,cACnB,KAAK,eAAe,MAAM;AAAA;AAAA;AAKlC,UAAM,KAAK,IACR,+DAA+D,KAAK;AAGvE,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,QAAQ,MAKO;AACnB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,wDAAwD;AAAA,IAC1E;AAGA,UAAM,KAAK,IAAI,uDACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM;AAAA;AAAA;AAKtB,UAAM,KAAK,IACR,mEAAmE,KAAK;AAG3E,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,kBAAkB,MAKP;AACf,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IACR,iEACD,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM;AAAA;AAAA;AAKtB,UAAM,KAAK,IACR,6EAA6E,KAAK;AAGrF,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,cAAc,MAKH;AACf,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IAAI,6DACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM;AAAA;AAAA;AAKtB,UAAM,KAAK,IACR,yEAAyE,KAAK;AAGjF,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,MAAM,MAeQ;AAClB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,sDAAsD;AAAA,IACxE;AAGA,UAAM,KAAK,IAAI,qDACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,mCACpB,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,+CACpB,KAAK,eAAe,KAAK,cACnB,KAAK,eAAe,MAAM;AAAA;AAAA;AAKlC,UAAM,KAAK,IACR,iEAAiE,KAAK;AAGzE,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,aAA2B;AAC7B,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,uEAAuE,KAAK;AAG/E,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,cAA4B;AAC9B,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,wEAAwE,KAAK;AAGhF,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,WAA6B;AAC/B,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,qEAAqE,KAAK;AAG7E,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,aAAsC;AACxC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,uEAAuE,KAAK;AAG/E,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,iBAAkC;AACpC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,2EAA2E,KAAK;AAGnF,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,oBAAsC;AACxC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,8EAA8E,KAAK;AAGtF,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,aAA8B;AAChC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,uEAAuE,KAAK;AAG/E,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,aAA8B;AAChC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,uEAAuE,KAAK;AAG/E,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,yBAA6C;AAC/C,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,mFAAmF,KAAK;AAG3F,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AACF;;;ACzwBA,OAAOC,aAAY;AAaZ,IAAM,sBAAN,MAA0B;AAAA,EAQ/B,YAAY,MA+GT;AAlHH,0BAA0B;AAC1B,uBAAuB;AAkHrB,SAAK,KAAK,sBAAsBA,QAAO,WAAW,EAAE,MAAM,GAAG,EAAE,CAAC;AAChE,SAAK,OAAO,QAAQ,CAAC;AAAA,EACvB;AAAA,EAEA,IAAI,KAAmB;AACrB,WAAO,KAAK;AAAA,EACd;AAAA,EAEA,IAAI,GAAG,cAA4B;AACjC,SAAK,MAAM;AAAA,EACb;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,KAAK,IAAiC;AAC1C,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,KAAK,gBAAgB;AACvB;AAAA,IACF;AAEA,QAAI,CAAC,IAAI;AACP,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,SAAK,MAAM;AAEX,UAAM,KAAK,IAAI;AAAA;AAAA;AAAA;AAAA;AAAA;AAQf,UAAM,KAAK,IAAI,iDACb,KAAK,KAAK,cAAc,KAAK,wBACb,KAAK,KAAK,WAAW,KAAK,wBAC1C,KAAK,KAAK,WAAW,KAAK,gCAE1B,KAAK,KAAK,mBAAmB,KAAK,+BAElC,KAAK,KAAK,kBAAkB,KAAK,uCAEjC,KAAK,KAAK,0BAA0B,KAAK,2BAEzC,KAAK,KAAK,cAAc,KAAK,6BAE7B,KAAK,KAAK,gBAAgB,KAAK,oCAE/B,KAAK,KAAK,uBAAuB,KAAK,wBACtB,KAAK,KAAK,WAAW,KAAK,wBAC1C,KAAK,KAAK,WAAW,KAAK,qBACb,KAAK,KAAK,QAAQ,KAAK,2BACpC,KAAK,KAAK,cAAc,KAAK,sBACf,KAAK,KAAK,SAAS,KAAK,yBACtC,KAAK,KAAK,YAAY,KAAK,wBACX,KAAK,KAAK,WAAW,KAAK,0BAC1C,KAAK,KAAK,aAAa,KAAK;AAAA;AAAA;AAK9B,UAAM,KAAK,IACR,+BAA+B,KAAK;AAEvC,SAAK,iBAAiB;AAAA,EACxB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,UAAU;AACd,QAAI,KAAK,aAAa;AACpB;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB;AAAA,IACF;AAEA,UAAM,KAAK,IAAI,mCAAmC,KAAK;AAEvD,SAAK,cAAc;AAAA,EACrB;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,MAAM,MAKW;AACrB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,qDAAqD;AAAA,IACvE;AAGA,UAAM,KAAK,IAAI,oDACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM;AAAA;AAAA;AAKtB,UAAM,KAAK,IACR,+DAA+D,KAAK;AAGvE,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,cAAc,MAKQ;AAC1B,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IAAI,4DACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM;AAAA;AAAA;AAKtB,UAAM,KAAK,IACR,uEAAuE,KAAK;AAG/E,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,IAAI,MAeO;AACf,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,mDAAmD;AAAA,IACrE;AAGA,UAAM,KAAK,IAAI,kDACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,mCACpB,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,+CACpB,KAAK,eAAe,KAAK,cACnB,KAAK,eAAe,MAAM;AAAA;AAAA;AAKlC,UAAM,KAAK,IACR,6DAA6D,KAAK;AAGrE,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,QAAQ,MAKO;AACnB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,uDAAuD;AAAA,IACzE;AAGA,UAAM,KAAK,IAAI,sDACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM;AAAA;AAAA;AAKtB,UAAM,KAAK,IACR,iEAAiE,KAAK;AAGzE,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,MAAM,MAeQ;AAClB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,qDAAqD;AAAA,IACvE;AAGA,UAAM,KAAK,IAAI,oDACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,mCACpB,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,+CACpB,KAAK,eAAe,KAAK,cACnB,KAAK,eAAe,MAAM;AAAA;AAAA;AAKlC,UAAM,KAAK,IACR,+DAA+D,KAAK;AAGvE,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,aAA2B;AAC7B,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,qEAAqE,KAAK;AAG7E,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,cAA4B;AAC9B,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,sEAAsE,KAAK;AAG9E,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,iBAAkC;AACpC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,yEAAyE,KAAK;AAGjF,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,oBAAsC;AACxC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,4EAA4E,KAAK;AAGpF,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,aAA8B;AAChC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,qEAAqE,KAAK;AAG7E,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,aAA8B;AAChC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,qEAAqE,KAAK;AAG7E,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,kBAAoC;AACtC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,0EAA0E,KAAK;AAGlF,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AACF;;;ACnnBA,OAAOC,aAAY;AAeZ,IAAM,6BAAN,MAAiC;AAAA,EAQtC,YAAY,MAoIT;AAvIH,0BAA0B;AAC1B,uBAAuB;AAuIrB,SAAK,KAAK,6BAA6BA,QAAO,WAAW,EAAE,MAAM,GAAG,EAAE,CAAC;AACvE,SAAK,OAAO,QAAQ,CAAC;AAAA,EACvB;AAAA,EAEA,IAAI,KAAmB;AACrB,WAAO,KAAK;AAAA,EACd;AAAA,EAEA,IAAI,GAAG,cAA4B;AACjC,SAAK,MAAM;AAAA,EACb;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,KAAK,IAAiC;AAC1C,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,KAAK,gBAAgB;AACvB;AAAA,IACF;AAEA,QAAI,CAAC,IAAI;AACP,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,SAAK,MAAM;AAEX,UAAM,KAAK,IAAI;AAAA;AAAA;AAAA;AAAA;AAAA;AAQf,UAAM,KAAK,IAAI,gDACb,KAAK,KAAK,MAAM,KAAK,4BAErB,KAAK,KAAK,eAAe,KAAK,2BAE9B,KAAK,KAAK,cAAc,KAAK,wBACb,KAAK,KAAK,WAAW,KAAK,wBAC1C,KAAK,KAAK,WAAW,KAAK,gCAE1B,KAAK,KAAK,mBAAmB,KAAK,+BAElC,KAAK,KAAK,kBAAkB,KAAK,uCAEjC,KAAK,KAAK,0BAA0B,KAAK,wBAEzC,KAAK,KAAK,WAAW,KAAK,oCAE1B,KAAK,KAAK,uBAAuB,KAAK,mBAC3B,KAAK,KAAK,MAAM,KAAK,2BAChC,KAAK,KAAK,cAAc,KAAK,2BACV,KAAK,KAAK,cAAc,KAAK,sBAChD,KAAK,KAAK,SAAS,KAAK,6BAExB,KAAK,KAAK,gBAAgB,KAAK,yBAE/B,KAAK,KAAK,YAAY,KAAK,kCAE3B,KAAK,KAAK,qBAAqB,KAAK,+BAEpC,KAAK,KAAK,kBAAkB,KAAK,kBACvB,KAAK,KAAK,KAAK,KAAK,wBAC9B,KAAK,KAAK,WAAW,KAAK;AAAA;AAAA;AAK5B,UAAM,KAAK,IACR,sCAAsC,KAAK;AAE9C,SAAK,iBAAiB;AAAA,EACxB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,UAAU;AACd,QAAI,KAAK,aAAa;AACpB;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB;AAAA,IACF;AAEA,UAAM,KAAK,IAAI,0CAA0C,KAAK;AAE9D,SAAK,cAAc;AAAA,EACrB;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,MAAM,MAKe;AACzB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IAAI,2DACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM;AAAA;AAAA;AAKtB,UAAM,KAAK,IACR,6EAA6E,KAAK;AAGrF,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,kBAAkB,MAKD;AACrB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IACR,uEACD,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM;AAAA;AAAA;AAKtB,UAAM,KAAK,IACR,yFAAyF,KAAK;AAGjG,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,IAAI,MAoBO;AACf,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IAAI,yDACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,mCACpB,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,+CACpB,KAAK,eAAe,KAAK,cACnB,KAAK,eAAe,MAAM,gCAChC,KAAK,SAAS,KAAK;AAAA;AAAA;AAMrB,UAAM,KAAK,IACR,2EAA2E,KAAK;AAGnF,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,QAAQ,MAKO;AACnB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IAAI,6DACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM;AAAA;AAAA;AAKtB,UAAM,KAAK,IACR,+EAA+E,KAAK;AAGvF,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,kBAAkB,MAKD;AACrB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IACR,uEACD,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM;AAAA;AAAA;AAKtB,UAAM,KAAK,IACR,yFAAyF,KAAK;AAGjG,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,cAAc,MAKG;AACrB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IACR,mEACD,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM;AAAA;AAAA;AAKtB,UAAM,KAAK,IACR,qFAAqF,KAAK;AAG7F,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,MAAM,MAeQ;AAClB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IAAI,2DACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,mCACpB,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,+CACpB,KAAK,eAAe,KAAK,cACnB,KAAK,eAAe,MAAM;AAAA;AAAA;AAKlC,UAAM,KAAK,IACR,6EAA6E,KAAK;AAGrF,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,yBAAyB,MAKZ;AACjB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IACR,8EACD,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM;AAAA;AAAA;AAKtB,UAAM,KAAK,IACR,gGAAgG,KAAK;AAGxG,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,eAAe,MAKF;AACjB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IACR,oEACD,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM;AAAA;AAAA;AAKtB,UAAM,KAAK,IACR,sFAAsF,KAAK;AAG9F,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,qBAAqB,MAKR;AACjB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IACR,0EACD,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM;AAAA;AAAA;AAKtB,UAAM,KAAK,IACR,4FAA4F,KAAK;AAGpG,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,gBAAiC;AACnC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,sFAAsF,KAAK;AAG9F,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,mBAAqC;AACvC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,yFAAyF,KAAK;AAGjG,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,eAAiC;AACnC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,qFAAqF,KAAK;AAG7F,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,QAAsB;AACxB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,8EAA8E,KAAK;AAGtF,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,QAAsB;AACxB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,8EAA8E,KAAK;AAGtF,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,cAA8B;AAChC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,oFAAoF,KAAK;AAG5F,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,WAA6B;AAC/B,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,iFAAiF,KAAK;AAGzF,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,iBAAkC;AACpC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,uFAAuF,KAAK;AAG/F,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,oBAAsC;AACxC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,0FAA0F,KAAK;AAGlG,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,aAA8B;AAChC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,mFAAmF,KAAK;AAG3F,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,gBAAiC;AACnC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,sFAAsF,KAAK;AAG9F,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AACF;;;ACh9BA,OAAOC,aAAY;AAeZ,IAAM,4BAAN,MAAgC;AAAA,EAQrC,YAAY,MA2IT;AA9IH,0BAA0B;AAC1B,uBAAuB;AA8IrB,SAAK,KAAK,4BAA4BA,QAAO,WAAW,EAAE,MAAM,GAAG,EAAE,CAAC;AACtE,SAAK,OAAO,QAAQ,CAAC;AAAA,EACvB;AAAA,EAEA,IAAI,KAAmB;AACrB,WAAO,KAAK;AAAA,EACd;AAAA,EAEA,IAAI,GAAG,cAA4B;AACjC,SAAK,MAAM;AAAA,EACb;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,KAAK,IAAiC;AAC1C,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,KAAK,gBAAgB;AACvB;AAAA,IACF;AAEA,QAAI,CAAC,IAAI;AACP,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,SAAK,MAAM;AAEX,UAAM,KAAK,IAAI;AAAA;AAAA;AAAA;AAAA;AAAA;AAQf,UAAM,KAAK,IAAI,+CACb,KAAK,KAAK,MAAM,KAAK,4BAErB,KAAK,KAAK,eAAe,KAAK,2BAE9B,KAAK,KAAK,cAAc,KAAK,wBACb,KAAK,KAAK,WAAW,KAAK,wBAC1C,KAAK,KAAK,WAAW,KAAK,gCAE1B,KAAK,KAAK,mBAAmB,KAAK,+BAElC,KAAK,KAAK,kBAAkB,KAAK,uCAEjC,KAAK,KAAK,0BAA0B,KAAK,wBAEzC,KAAK,KAAK,WAAW,KAAK,oCAE1B,KAAK,KAAK,uBAAuB,KAAK,mBAC3B,KAAK,KAAK,MAAM,KAAK,2BAChC,KAAK,KAAK,cAAc,KAAK,2BACV,KAAK,KAAK,cAAc,KAAK,oBAChD,KAAK,KAAK,OAAO,KAAK,sBACR,KAAK,KAAK,SAAS,KAAK,6BACtC,KAAK,KAAK,gBAAgB,KAAK,yBAE/B,KAAK,KAAK,YAAY,KAAK,kCAE3B,KAAK,KAAK,qBAAqB,KAAK,+BAEpC,KAAK,KAAK,kBAAkB,KAAK,kBACvB,KAAK,KAAK,KAAK,KAAK,wBAC9B,KAAK,KAAK,WAAW,KAAK;AAAA;AAAA;AAK5B,UAAM,KAAK,IACR,qCAAqC,KAAK;AAE7C,SAAK,iBAAiB;AAAA,EACxB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,UAAU;AACd,QAAI,KAAK,aAAa;AACpB;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB;AAAA,IACF;AAEA,UAAM,KAAK,IAAI,yCAAyC,KAAK;AAE7D,SAAK,cAAc;AAAA,EACrB;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,MAAM,MAKa;AACvB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IAAI,0DACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM;AAAA;AAAA;AAKtB,UAAM,KAAK,IACR,2EAA2E,KAAK;AAGnF,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,IAAI,MAoBO;AACf,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,yDAAyD;AAAA,IAC3E;AAGA,UAAM,KAAK,IAAI,wDACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,mCACpB,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,+CACpB,KAAK,eAAe,KAAK,cACnB,KAAK,eAAe,MAAM,gCAChC,KAAK,SAAS,KAAK;AAAA;AAAA;AAMrB,UAAM,KAAK,IACR,yEAAyE,KAAK;AAGjF,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,QAAQ,MAKO;AACnB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IAAI,4DACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM;AAAA;AAAA;AAKtB,UAAM,KAAK,IACR,6EAA6E,KAAK;AAGrF,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,MAAM,MAeQ;AAClB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IAAI,0DACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,mCACpB,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,+CACpB,KAAK,eAAe,KAAK,cACnB,KAAK,eAAe,MAAM;AAAA;AAAA;AAKlC,UAAM,KAAK,IACR,2EAA2E,KAAK;AAGnF,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,eAAe,MAKF;AACjB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IACR,mEACD,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM;AAAA;AAAA;AAKtB,UAAM,KAAK,IACR,oFAAoF,KAAK;AAG5F,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,mBAAqC;AACvC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,uFAAuF,KAAK;AAG/F,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,eAAiC;AACnC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,mFAAmF,KAAK;AAG3F,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,QAAsB;AACxB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,4EAA4E,KAAK;AAGpF,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,QAAsB;AACxB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,4EAA4E,KAAK;AAGpF,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,cAA8B;AAChC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,kFAAkF,KAAK;AAG1F,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,gBAAiC;AACnC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,oFAAoF,KAAK;AAG5F,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,iBAAkC;AACpC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,qFAAqF,KAAK;AAG7F,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,oBAAsC;AACxC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,wFAAwF,KAAK;AAGhG,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,gBAAiC;AACnC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,oFAAoF,KAAK;AAG5F,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AACF;;;AC7tBA,OAAOC,aAAY;AAiBZ,IAAM,iCAAN,MAAqC;AAAA,EAQ1C,YAAY,MA8IT;AAjJH,0BAA0B;AAC1B,uBAAuB;AAiJrB,SAAK,KAAK,iCACRA,QAAO,WAAW,EAAE,MAAM,GAAG,EAAE,CAAC;AAElC,SAAK,OAAO,QAAQ,CAAC;AAAA,EACvB;AAAA,EAEA,IAAI,KAAmB;AACrB,WAAO,KAAK;AAAA,EACd;AAAA,EAEA,IAAI,GAAG,cAA4B;AACjC,SAAK,MAAM;AAAA,EACb;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,KAAK,IAAiC;AAC1C,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,KAAK,gBAAgB;AACvB;AAAA,IACF;AAEA,QAAI,CAAC,IAAI;AACP,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,SAAK,MAAM;AAEX,UAAM,KAAK,IAAI;AAAA;AAAA;AAAA;AAAA;AAAA;AAQf,UAAM,KAAK,IAAI,oDACb,KAAK,KAAK,MAAM,KAAK,4BAErB,KAAK,KAAK,eAAe,KAAK,uBACf,KAAK,KAAK,UAAU,KAAK,6BACxC,KAAK,KAAK,gBAAgB,KAAK,wBAE/B,KAAK,KAAK,WAAW,KAAK,+BAE1B,KAAK,KAAK,kBAAkB,KAAK,gCAEjC,KAAK,KAAK,mBAAmB,KAAK,uBAElC,KAAK,KAAK,UAAU,KAAK,4CAEzB,KAAK,KAAK,sBAAsB,KAAK,cAErC,KAAK,KAAK,sBAAsB,MAAM,+CAEtC,KAAK,KAAK,eAAe,KAAK,cAE9B,KAAK,KAAK,eAAe,MAAM,wCAE/B,KAAK,KAAK,iBAAiB,KAAK,yBAEhC,KAAK,KAAK,YAAY,KAAK,6BAE3B,KAAK,KAAK,gBAAgB,KAAK,sBAE/B,KAAK,KAAK,SAAS,KAAK,kCAExB,KAAK,KAAK,qBAAqB,KAAK,+BAEpC,KAAK,KAAK,kBAAkB,KAAK,kBACvB,KAAK,KAAK,KAAK,KAAK,sBAC9B,KAAK,KAAK,SAAS,KAAK,2BAExB,KAAK,KAAK,cAAc,KAAK,2BACV,KAAK,KAAK,cAAc,KAAK;AAAA;AAAA;AAIlD,UAAM,KAAK,IACR,0CAA0C,KAAK;AAElD,SAAK,iBAAiB;AAAA,EACxB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,UAAU;AACd,QAAI,KAAK,aAAa;AACpB;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB;AAAA,IACF;AAEA,UAAM,KAAK,IAAI,8CAA8C,KAAK;AAElE,SAAK,cAAc;AAAA,EACrB;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,kBAAkB,MAKH;AACnB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IACR,kEACD,KAAK,GAAG,KAAK;AAAA;AAAA;AAMf,UAAM,KAAK,IACR,iGAAiG,KAAK;AAGzG,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,IAAI,MAeO;AACf,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IAAI,6DACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,mCACpB,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,+CACpB,KAAK,eAAe,KAAK,cACnB,KAAK,eAAe,MAAM;AAAA;AAAA;AAKlC,UAAM,KAAK,IACR,mFAAmF,KAAK;AAG3F,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,QAAQ,MAKO;AACnB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IAAI,wDACb,KAAK,GAAG,KAAK;AAAA;AAAA;AAMf,UAAM,KAAK,IACR,uFAAuF,KAAK;AAG/F,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,cAAc,MAKC;AACnB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IACR,8DACD,KAAK,GAAG,KAAK;AAAA;AAAA;AAMf,UAAM,KAAK,IACR,6FAA6F,KAAK;AAGrG,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,MAAM,MAeQ;AAClB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IACR,+DACD,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,mCACpB,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,+CACpB,KAAK,eAAe,KAAK,cACnB,KAAK,eAAe,MAAM;AAAA;AAAA;AAKlC,UAAM,KAAK,IACR,qFAAqF,KAAK;AAG7F,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,yBAAyB,MAKZ;AACjB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IACR,kFACD,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM;AAAA;AAAA;AAKtB,UAAM,KAAK,IACR,wGAAwG,KAAK;AAGhH,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,eAAe,MAKF;AACjB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IACR,wEACD,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM;AAAA;AAAA;AAKtB,UAAM,KAAK,IACR,8FAA8F,KAAK;AAGtG,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,qBAAqB,MAKR;AACjB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IACR,8EACD,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM;AAAA;AAAA;AAKtB,UAAM,KAAK,IACR,oGAAoG,KAAK;AAG5G,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,WAAyB;AAC3B,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,yFAAyF,KAAK;AAGjG,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,qBAAuC;AACzC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,mGAAmG,KAAK;AAG3G,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,yBAA0C;AAC5C,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,uGAAuG,KAAK;AAG/G,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,eAAiC;AACnC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,6FAA6F,KAAK;AAGrG,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,oBAAsC;AACxC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,kGAAkG,KAAK;AAG1G,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,kBAAoC;AACtC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,gGAAgG,KAAK;AAGxG,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,iBAAkC;AACpC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,+FAA+F,KAAK;AAGvG,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,oBAAsC;AACxC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,kGAAkG,KAAK;AAG1G,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AACF;;;ACl0BA,OAAOC,cAAY;AAiBZ,IAAM,gCAAN,MAAoC;AAAA,EAQzC,YAAY,MAsIT;AAzIH,0BAA0B;AAC1B,uBAAuB;AAyIrB,SAAK,KAAK,gCACRA,SAAO,WAAW,EAAE,MAAM,GAAG,EAAE,CAAC;AAElC,SAAK,OAAO,QAAQ,CAAC;AAAA,EACvB;AAAA,EAEA,IAAI,KAAmB;AACrB,WAAO,KAAK;AAAA,EACd;AAAA,EAEA,IAAI,GAAG,cAA4B;AACjC,SAAK,MAAM;AAAA,EACb;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,KAAK,IAAiC;AAC1C,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,KAAK,gBAAgB;AACvB;AAAA,IACF;AAEA,QAAI,CAAC,IAAI;AACP,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,SAAK,MAAM;AAEX,UAAM,KAAK,IAAI;AAAA;AAAA;AAAA;AAAA;AAAA;AAQf,UAAM,KAAK,IAAI,mDACb,KAAK,KAAK,MAAM,KAAK,uBACN,KAAK,KAAK,UAAU,KAAK,4BACxC,KAAK,KAAK,eAAe,KAAK,uBACf,KAAK,KAAK,UAAU,KAAK,6BACxC,KAAK,KAAK,gBAAgB,KAAK,wBAE/B,KAAK,KAAK,WAAW,KAAK,+BAE1B,KAAK,KAAK,kBAAkB,KAAK,gCAEjC,KAAK,KAAK,mBAAmB,KAAK,uBAElC,KAAK,KAAK,UAAU,KAAK,4CAEzB,KAAK,KAAK,sBAAsB,KAAK,cAErC,KAAK,KAAK,sBAAsB,MAAM,+CAEtC,KAAK,KAAK,eAAe,KAAK,cAE9B,KAAK,KAAK,eAAe,MAAM,wCAE/B,KAAK,KAAK,iBAAiB,KAAK,yBAEhC,KAAK,KAAK,YAAY,KAAK,6BAE3B,KAAK,KAAK,gBAAgB,KAAK,sBAE/B,KAAK,KAAK,SAAS,KAAK,kCAExB,KAAK,KAAK,qBAAqB,KAAK,+BAEpC,KAAK,KAAK,kBAAkB,KAAK,kBACvB,KAAK,KAAK,KAAK,KAAK,sBAC9B,KAAK,KAAK,SAAS,KAAK,2BACL,KAAK,KAAK,cAAc,KAAK;AAAA;AAAA;AAIlD,UAAM,KAAK,IACR,yCAAyC,KAAK;AAEjD,SAAK,iBAAiB;AAAA,EACxB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,UAAU;AACd,QAAI,KAAK,aAAa;AACpB;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB;AAAA,IACF;AAEA,UAAM,KAAK,IAAI,6CAA6C,KAAK;AAEjE,SAAK,cAAc;AAAA,EACrB;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,IAAI,MAeO;AACf,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IAAI,4DACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,mCACpB,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,+CACpB,KAAK,eAAe,KAAK,cACnB,KAAK,eAAe,MAAM;AAAA;AAAA;AAKlC,UAAM,KAAK,IACR,iFAAiF,KAAK;AAGzF,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,QAAQ,MAKO;AACnB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IAAI,uDACb,KAAK,GAAG,KAAK;AAAA;AAAA;AAMf,UAAM,KAAK,IACR,qFAAqF,KAAK;AAG7F,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,MAAM,MAeQ;AAClB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IACR,8DACD,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,mCACpB,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,+CACpB,KAAK,eAAe,KAAK,cACnB,KAAK,eAAe,MAAM;AAAA;AAAA;AAKlC,UAAM,KAAK,IACR,mFAAmF,KAAK;AAG3F,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,eAAe,MAKF;AACjB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IACR,uEACD,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM;AAAA;AAAA;AAKtB,UAAM,KAAK,IACR,4FAA4F,KAAK;AAGpG,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,qBAAuC;AACzC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,iGAAiG,KAAK;AAGzG,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,yBAA0C;AAC5C,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,qGAAqG,KAAK;AAG7G,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,eAAiC;AACnC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,2FAA2F,KAAK;AAGnG,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,oBAAsC;AACxC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,gGAAgG,KAAK;AAGxG,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,kBAAoC;AACtC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,8FAA8F,KAAK;AAGtG,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,iBAAkC;AACpC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,6FAA6F,KAAK;AAGrG,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,oBAAsC;AACxC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,gGAAgG,KAAK;AAGxG,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AACF;;;ACjoBA,OAAOC,cAAY;AAqBZ,IAAM,kBAAN,MAAsB;AAAA,EAQ3B,YAAY,MA6DT;AAhEH,0BAA0B;AAC1B,uBAAuB;AAgErB,SAAK,KAAK,kBAAkBA,SAAO,WAAW,EAAE,MAAM,GAAG,EAAE,CAAC;AAC5D,SAAK,OAAO,QAAQ,CAAC;AAAA,EACvB;AAAA,EAEA,IAAI,KAAmB;AACrB,WAAO,KAAK;AAAA,EACd;AAAA,EAEA,IAAI,GAAG,cAA4B;AACjC,SAAK,MAAM;AAAA,EACb;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,KAAK,IAAiC;AAC1C,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI,MAAM,yDAAyD;AAAA,IAC3E;AAEA,QAAI,KAAK,gBAAgB;AACvB;AAAA,IACF;AAEA,QAAI,CAAC,IAAI;AACP,YAAM,IAAI,MAAM,uDAAuD;AAAA,IACzE;AAEA,SAAK,MAAM;AAEX,UAAM,KAAK,IAAI;AAAA;AAAA;AAAA;AAAA;AAAA;AAQf,UAAM,KAAK,IAAI,6CACb,KAAK,KAAK,cAAc,KAAK,0BAE7B,KAAK,KAAK,aAAa,KAAK,4BAE5B,KAAK,KAAK,eAAe,KAAK,2BAE9B,KAAK,KAAK,cAAc,KAAK,wBACb,KAAK,KAAK,WAAW,KAAK,qBAC1C,KAAK,KAAK,QAAQ,KAAK,2BACJ,KAAK,KAAK,cAAc,KAAK,sBAChD,KAAK,KAAK,SAAS,KAAK,yBACP,KAAK,KAAK,YAAY,KAAK;AAAA;AAAA;AAI9C,UAAM,KAAK,IACR,2BAA2B,KAAK;AAEnC,SAAK,iBAAiB;AAAA,EACxB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,UAAU;AACd,QAAI,KAAK,aAAa;AACpB;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB;AAAA,IACF;AAEA,UAAM,KAAK,IAAI,+BAA+B,KAAK;AAEnD,SAAK,cAAc;AAAA,EACrB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EASA,MAAM,kBAAkB,MAKH;AACnB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI,MAAM,yDAAyD;AAAA,IAC3E;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IAAI,4DACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM;AAAA;AAAA;AAKtB,UAAM,KAAK,IACR,mEAAmE,KAAK;AAG3E,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,IAAI,MAeO;AACf,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI,MAAM,yDAAyD;AAAA,IAC3E;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,+CAA+C;AAAA,IACjE;AAGA,UAAM,KAAK,IAAI,8CACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,0BACpB,KAAK,GAAG,KAAK,qCACgB,KAAK,eAAe,KAAK,cACtD,KAAK,eAAe,MAAM;AAAA;AAAA;AAM5B,UAAM,KAAK,IACR,qDAAqD,KAAK;AAG7D,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,YAAY,MAUG;AACnB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI,MAAM,yDAAyD;AAAA,IAC3E;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,uDAAuD;AAAA,IACzE;AAGA,UAAM,KAAK,IAAI,sDACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,0BAA6B,KAAK,GAAG,KAAK;AAAA;AAAA;AAKhE,UAAM,KAAK,IACR,6DAA6D,KAAK;AAGrE,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,QAAQ,MAKO;AACnB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI,MAAM,yDAAyD;AAAA,IAC3E;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,mDAAmD;AAAA,IACrE;AAGA,UAAM,KAAK,IAAI,kDACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM;AAAA;AAAA;AAKtB,UAAM,KAAK,IACR,yDAAyD,KAAK;AAGjE,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EASA,MAAM,cAAc,MAKC;AACnB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI,MAAM,yDAAyD;AAAA,IAC3E;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,yDAAyD;AAAA,IAC3E;AAGA,UAAM,KAAK,IAAI,wDACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM;AAAA;AAAA;AAKtB,UAAM,KAAK,IACR,+DAA+D,KAAK;AAGvE,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,aAA2B;AAC7B,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI,MAAM,yDAAyD;AAAA,IAC3E;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,6DAA6D,KAAK;AAGrE,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,cAA4B;AAC9B,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI,MAAM,yDAAyD;AAAA,IAC3E;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,8DAA8D,KAAK;AAGtE,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,uBAAqC;AACvC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI,MAAM,yDAAyD;AAAA,IAC3E;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,uEAAuE,KAAK;AAG/E,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,eAAgC;AAClC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI,MAAM,yDAAyD;AAAA,IAC3E;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,+DAA+D,KAAK;AAGvE,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,UAA2B;AAC7B,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI,MAAM,yDAAyD;AAAA,IAC3E;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,0DAA0D,KAAK;AAGlE,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,iBAAkC;AACpC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI,MAAM,yDAAyD;AAAA,IAC3E;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,iEAAiE,KAAK;AAGzE,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,oBAAsC;AACxC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI,MAAM,yDAAyD;AAAA,IAC3E;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,oEAAoE,KAAK;AAG5E,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AACF;;;AC9hBA,OAAOC,cAAY;AAaZ,IAAM,yBAAN,MAA6B;AAAA,EAQlC,YAAY,MA8HT;AAjIH,0BAA0B;AAC1B,uBAAuB;AAiIrB,SAAK,KAAK,yBAAyBA,SAAO,WAAW,EAAE,MAAM,GAAG,EAAE,CAAC;AACnE,SAAK,OAAO,QAAQ,CAAC;AAAA,EACvB;AAAA,EAEA,IAAI,KAAmB;AACrB,WAAO,KAAK;AAAA,EACd;AAAA,EAEA,IAAI,GAAG,cAA4B;AACjC,SAAK,MAAM;AAAA,EACb;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,KAAK,IAAiC;AAC1C,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,KAAK,gBAAgB;AACvB;AAAA,IACF;AAEA,QAAI,CAAC,IAAI;AACP,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,SAAK,MAAM;AAEX,UAAM,KAAK,IAAI;AAAA;AAAA;AAAA;AAAA;AAAA;AAQf,UAAM,KAAK,IAAI,oDACb,KAAK,KAAK,cAAc,KAAK,wBACb,KAAK,KAAK,WAAW,KAAK,wBAC1C,KAAK,KAAK,WAAW,KAAK,gCAE1B,KAAK,KAAK,mBAAmB,KAAK,+BAElC,KAAK,KAAK,kBAAkB,KAAK,uCAEjC,KAAK,KAAK,0BAA0B,KAAK,2BAEzC,KAAK,KAAK,cAAc,KAAK,6BAE7B,KAAK,KAAK,gBAAgB,KAAK,oCAE/B,KAAK,KAAK,uBAAuB,KAAK,wBACtB,KAAK,KAAK,WAAW,KAAK,wBAC1C,KAAK,KAAK,WAAW,KAAK,qBACb,KAAK,KAAK,QAAQ,KAAK,2BACpC,KAAK,KAAK,cAAc,KAAK,sBACf,KAAK,KAAK,SAAS,KAAK,yBACtC,KAAK,KAAK,YAAY,KAAK,2BAE3B,KAAK,KAAK,cAAc,KAAK,wBACb,KAAK,KAAK,WAAW,KAAK,0BAC1C,KAAK,KAAK,aAAa,KAAK;AAAA;AAAA;AAK9B,UAAM,KAAK,IACR,kCAAkC,KAAK;AAE1C,SAAK,iBAAiB;AAAA,EACxB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,UAAU;AACd,QAAI,KAAK,aAAa;AACpB;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB;AAAA,IACF;AAEA,UAAM,KAAK,IAAI,sCAAsC,KAAK;AAE1D,SAAK,cAAc;AAAA,EACrB;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,MAAM,MAKW;AACrB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,wDAAwD;AAAA,IAC1E;AAGA,UAAM,KAAK,IAAI,uDACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM;AAAA;AAAA;AAKtB,UAAM,KAAK,IACR,qEAAqE,KAAK;AAG7E,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,cAAc,MAKQ;AAC1B,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IACR,+DACD,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM;AAAA;AAAA;AAKtB,UAAM,KAAK,IACR,6EAA6E,KAAK;AAGrF,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,IAAI,MAeO;AACf,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,sDAAsD;AAAA,IACxE;AAGA,UAAM,KAAK,IAAI,qDACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,mCACpB,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,+CACpB,KAAK,eAAe,KAAK,cACnB,KAAK,eAAe,MAAM;AAAA;AAAA;AAKlC,UAAM,KAAK,IACR,mEAAmE,KAAK;AAG3E,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,QAAQ,MAKO;AACnB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IAAI,yDACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM;AAAA;AAAA;AAKtB,UAAM,KAAK,IACR,uEAAuE,KAAK;AAG/E,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,kBAAkB,MAKP;AACf,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IACR,mEACD,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM;AAAA;AAAA;AAKtB,UAAM,KAAK,IACR,iFAAiF,KAAK;AAGzF,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,cAAc,MAKH;AACf,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IACR,+DACD,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM;AAAA;AAAA;AAKtB,UAAM,KAAK,IACR,6EAA6E,KAAK;AAGrF,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,MAAM,MAeQ;AAClB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,wDAAwD;AAAA,IAC1E;AAGA,UAAM,KAAK,IAAI,uDACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,mCACpB,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,+CACpB,KAAK,eAAe,KAAK,cACnB,KAAK,eAAe,MAAM;AAAA;AAAA;AAKlC,UAAM,KAAK,IACR,qEAAqE,KAAK;AAG7E,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,aAA2B;AAC7B,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,2EAA2E,KAAK;AAGnF,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,cAA4B;AAC9B,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,4EAA4E,KAAK;AAGpF,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,WAA6B;AAC/B,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,yEAAyE,KAAK;AAGjF,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,aAAsC;AACxC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,2EAA2E,KAAK;AAGnF,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,iBAAkC;AACpC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,+EAA+E,KAAK;AAGvF,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,oBAAsC;AACxC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,kFAAkF,KAAK;AAG1F,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,aAA8B;AAChC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,2EAA2E,KAAK;AAGnF,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,aAA8B;AAChC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,2EAA2E,KAAK;AAGnF,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,yBAA6C;AAC/C,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,uFAAuF,KAAK;AAG/F,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AACF;;;AC7wBA,OAAOC,cAAY;AAaZ,IAAM,wBAAN,MAA4B;AAAA,EAQjC,YAAY,MA+GT;AAlHH,0BAA0B;AAC1B,uBAAuB;AAkHrB,SAAK,KAAK,wBAAwBA,SAAO,WAAW,EAAE,MAAM,GAAG,EAAE,CAAC;AAClE,SAAK,OAAO,QAAQ,CAAC;AAAA,EACvB;AAAA,EAEA,IAAI,KAAmB;AACrB,WAAO,KAAK;AAAA,EACd;AAAA,EAEA,IAAI,GAAG,cAA4B;AACjC,SAAK,MAAM;AAAA,EACb;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,KAAK,IAAiC;AAC1C,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,KAAK,gBAAgB;AACvB;AAAA,IACF;AAEA,QAAI,CAAC,IAAI;AACP,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,SAAK,MAAM;AAEX,UAAM,KAAK,IAAI;AAAA;AAAA;AAAA;AAAA;AAAA;AAQf,UAAM,KAAK,IAAI,mDACb,KAAK,KAAK,cAAc,KAAK,wBACb,KAAK,KAAK,WAAW,KAAK,wBAC1C,KAAK,KAAK,WAAW,KAAK,gCAE1B,KAAK,KAAK,mBAAmB,KAAK,+BAElC,KAAK,KAAK,kBAAkB,KAAK,uCAEjC,KAAK,KAAK,0BAA0B,KAAK,2BAEzC,KAAK,KAAK,cAAc,KAAK,6BAE7B,KAAK,KAAK,gBAAgB,KAAK,oCAE/B,KAAK,KAAK,uBAAuB,KAAK,wBACtB,KAAK,KAAK,WAAW,KAAK,wBAC1C,KAAK,KAAK,WAAW,KAAK,qBACb,KAAK,KAAK,QAAQ,KAAK,2BACpC,KAAK,KAAK,cAAc,KAAK,sBACf,KAAK,KAAK,SAAS,KAAK,yBACtC,KAAK,KAAK,YAAY,KAAK,wBACX,KAAK,KAAK,WAAW,KAAK,0BAC1C,KAAK,KAAK,aAAa,KAAK;AAAA;AAAA;AAK9B,UAAM,KAAK,IACR,iCAAiC,KAAK;AAEzC,SAAK,iBAAiB;AAAA,EACxB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,UAAU;AACd,QAAI,KAAK,aAAa;AACpB;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB;AAAA,IACF;AAEA,UAAM,KAAK,IAAI,qCAAqC,KAAK;AAEzD,SAAK,cAAc;AAAA,EACrB;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,MAAM,MAKW;AACrB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,uDAAuD;AAAA,IACzE;AAGA,UAAM,KAAK,IAAI,sDACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM;AAAA;AAAA;AAKtB,UAAM,KAAK,IACR,mEAAmE,KAAK;AAG3E,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,cAAc,MAKQ;AAC1B,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IACR,8DACD,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM;AAAA;AAAA;AAKtB,UAAM,KAAK,IACR,2EAA2E,KAAK;AAGnF,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,IAAI,MAeO;AACf,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,qDAAqD;AAAA,IACvE;AAGA,UAAM,KAAK,IAAI,oDACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,mCACpB,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,+CACpB,KAAK,eAAe,KAAK,cACnB,KAAK,eAAe,MAAM;AAAA;AAAA;AAKlC,UAAM,KAAK,IACR,iEAAiE,KAAK;AAGzE,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,QAAQ,MAKO;AACnB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,yDAAyD;AAAA,IAC3E;AAGA,UAAM,KAAK,IAAI,wDACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM;AAAA;AAAA;AAKtB,UAAM,KAAK,IACR,qEAAqE,KAAK;AAG7E,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,MAAM,MAeQ;AAClB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,uDAAuD;AAAA,IACzE;AAGA,UAAM,KAAK,IAAI,sDACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,mCACpB,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,+CACpB,KAAK,eAAe,KAAK,cACnB,KAAK,eAAe,MAAM;AAAA;AAAA;AAKlC,UAAM,KAAK,IACR,mEAAmE,KAAK;AAG3E,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,aAA2B;AAC7B,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,yEAAyE,KAAK;AAGjF,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,cAA4B;AAC9B,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,0EAA0E,KAAK;AAGlF,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,iBAAkC;AACpC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,6EAA6E,KAAK;AAGrF,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,oBAAsC;AACxC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,gFAAgF,KAAK;AAGxF,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,aAA8B;AAChC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,yEAAyE,KAAK;AAGjF,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,aAA8B;AAChC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,yEAAyE,KAAK;AAGjF,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,kBAAoC;AACtC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,8EAA8E,KAAK;AAGtF,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AACF;;;ACpnBA,OAAOC,cAAY;AAeZ,IAAM,uBAAN,MAA2B;AAAA,EAQhC,YAAY,MAgFT;AAnFH,0BAA0B;AAC1B,uBAAuB;AAmFrB,SAAK,KAAK,uBAAuBA,SAAO,WAAW,EAAE,MAAM,GAAG,EAAE,CAAC;AACjE,SAAK,OAAO,QAAQ,CAAC;AAAA,EACvB;AAAA,EAEA,IAAI,KAAmB;AACrB,WAAO,KAAK;AAAA,EACd;AAAA,EAEA,IAAI,GAAG,cAA4B;AACjC,SAAK,MAAM;AAAA,EACb;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,KAAK,IAAiC;AAC1C,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,KAAK,gBAAgB;AACvB;AAAA,IACF;AAEA,QAAI,CAAC,IAAI;AACP,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,SAAK,MAAM;AAEX,UAAM,KAAK,IAAI;AAAA;AAAA;AAAA;AAAA;AAAA;AAQf,UAAM,KAAK,IAAI,kDACb,KAAK,KAAK,cAAc,KAAK,wBAE7B,KAAK,KAAK,WAAW,KAAK,gCAE1B,KAAK,KAAK,mBAAmB,KAAK,+BAElC,KAAK,KAAK,kBAAkB,KAAK,uCAEjC,KAAK,KAAK,0BAA0B,KAAK,6BAEzC,KAAK,KAAK,gBAAgB,KAAK,oCAE/B,KAAK,KAAK,uBAAuB,KAAK,4BAClB,KAAK,KAAK,eAAe,KAAK,qBAClD,KAAK,KAAK,QAAQ,KAAK,2BACJ,KAAK,KAAK,cAAc,KAAK,sBAChD,KAAK,KAAK,SAAS,KAAK,yBACP,KAAK,KAAK,YAAY,KAAK;AAAA;AAAA;AAI9C,UAAM,KAAK,IACR,gCAAgC,KAAK;AAExC,SAAK,iBAAiB;AAAA,EACxB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,UAAU;AACd,QAAI,KAAK,aAAa;AACpB;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB;AAAA,IACF;AAEA,UAAM,KAAK,IAAI,oCAAoC,KAAK;AAExD,SAAK,cAAc;AAAA,EACrB;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,MAAM,MAKW;AACrB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,sDAAsD;AAAA,IACxE;AAGA,UAAM,KAAK,IAAI,qDACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM;AAAA;AAAA;AAKtB,UAAM,KAAK,IACR,iEAAiE,KAAK;AAGzE,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,cAAc,MAKQ;AAC1B,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IAAI,6DACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM;AAAA;AAAA;AAKtB,UAAM,KAAK,IACR,yEAAyE,KAAK;AAGjF,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,IAAI,MAeO;AACf,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,oDAAoD;AAAA,IACtE;AAGA,UAAM,KAAK,IAAI,mDACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,0BACpB,KAAK,GAAG,KAAK,qCACgB,KAAK,eAAe,KAAK,cACtD,KAAK,eAAe,MAAM;AAAA;AAAA;AAM5B,UAAM,KAAK,IACR,+DAA+D,KAAK;AAGvE,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,cAAc,MAeQ;AAC1B,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IAAI,6DACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,0BACpB,KAAK,GAAG,KAAK,qCACgB,KAAK,eAAe,KAAK,cACtD,KAAK,eAAe,MAAM;AAAA;AAAA;AAM5B,UAAM,KAAK,IACR,yEAAyE,KAAK;AAGjF,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,sBAAsB,MAKX;AACf,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IACR,yEACD,KAAK,gBAAgB,KAAK;AAAA;AAAA;AAM5B,UAAM,KAAK,IACR,iFAAiF,KAAK;AAGzF,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,WAAW,MAKA;AACf,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IAAI,yDACb,KAAK,WAAW,KAAK;AAAA;AAAA;AAMvB,UAAM,KAAK,IACR,sEAAsE,KAAK;AAG9E,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,UAAU,MAKY;AAC1B,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IAAI,yDACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM;AAAA;AAAA;AAKtB,UAAM,KAAK,IACR,qEAAqE,KAAK;AAG7E,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,aAA2B;AAC7B,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,uEAAuE,KAAK;AAG/E,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,cAA4B;AAC9B,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,wEAAwE,KAAK;AAGhF,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,iBAAkC;AACpC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,2EAA2E,KAAK;AAGnF,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,oBAAsC;AACxC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,8EAA8E,KAAK;AAGtF,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,aAA8B;AAChC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,uEAAuE,KAAK;AAG/E,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,mBAAiC;AACnC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,6EAA6E,KAAK;AAGrF,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AACF;;;ACloBA,OAAOC,cAAY;AAeZ,IAAM,qBAAN,MAAyB;AAAA,EAQ9B,YAAY,MA2CT;AA9CH,0BAA0B;AAC1B,uBAAuB;AA8CrB,SAAK,KAAK,qBAAqBA,SAAO,WAAW,EAAE,MAAM,GAAG,EAAE,CAAC;AAC/D,SAAK,OAAO,QAAQ,CAAC;AAAA,EACvB;AAAA,EAEA,IAAI,KAAmB;AACrB,WAAO,KAAK;AAAA,EACd;AAAA,EAEA,IAAI,GAAG,cAA4B;AACjC,SAAK,MAAM;AAAA,EACb;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,KAAK,IAAiC;AAC1C,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,KAAK,gBAAgB;AACvB;AAAA,IACF;AAEA,QAAI,CAAC,IAAI;AACP,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,SAAK,MAAM;AAEX,UAAM,KAAK,IAAI;AAAA;AAAA;AAAA;AAAA;AAAA;AAQf,UAAM,KAAK,IAAI,8CACb,KAAK,KAAK,YAAY,KAAK,8BACL,KAAK,KAAK,iBAAiB,KAAK,iBACtD,KAAK,KAAK,IAAI,KAAK,2BACA,KAAK,KAAK,cAAc,KAAK,qBAChD,KAAK,KAAK,QAAQ,KAAK,0BACL,KAAK,KAAK,aAAa,KAAK,sBAC9C,KAAK,KAAK,SAAS,KAAK;AAAA;AAAA;AAK1B,UAAM,KAAK,IACR,8BAA8B,KAAK;AAEtC,SAAK,iBAAiB;AAAA,EACxB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,UAAU;AACd,QAAI,KAAK,aAAa;AACpB;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB;AAAA,IACF;AAEA,UAAM,KAAK,IAAI,kCAAkC,KAAK;AAEtD,SAAK,cAAc;AAAA,EACrB;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,kBAAkB,MAKH;AACnB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IACR,+DACD,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM;AAAA;AAAA;AAKtB,UAAM,KAAK,IACR,yEAAyE,KAAK;AAGjF,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,IAAI,MAeO;AACf,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,kDAAkD;AAAA,IACpE;AAGA,UAAM,KAAK,IAAI,iDACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,mCACpB,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,+CACpB,KAAK,eAAe,KAAK,cACnB,KAAK,eAAe,MAAM;AAAA;AAAA;AAKlC,UAAM,KAAK,IACR,2DAA2D,KAAK;AAGnE,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,cAAc,MAeD;AACjB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IAAI,2DACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,mCACpB,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,mCACpB,KAAK,YAAY,KAAK;AAAA;AAAA;AAMxB,UAAM,KAAK,IACR,qEAAqE,KAAK;AAG7E,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,sBAAsB,MAKX;AACf,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IACR,uEACD,KAAK,gBAAgB,KAAK;AAAA;AAAA;AAM5B,UAAM,KAAK,IACR,6EAA6E,KAAK;AAGrF,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,QAAQ,MAUO;AACnB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,sDAAsD;AAAA,IACxE;AAGA,UAAM,KAAK,IAAI,qDACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,uCACpB,KAAK,gBAAgB,KAAK;AAAA;AAAA;AAM5B,UAAM,KAAK,IACR,+DAA+D,KAAK;AAGvE,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,cAAc,MAKW;AAC7B,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IAAI,2DACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM;AAAA;AAAA;AAKtB,UAAM,KAAK,IACR,qEAAqE,KAAK;AAG7E,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,MAAM,MAeQ;AAClB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,oDAAoD;AAAA,IACtE;AAGA,UAAM,KAAK,IAAI,mDACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,mCACpB,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,+CACpB,KAAK,eAAe,KAAK,cACnB,KAAK,eAAe,MAAM;AAAA;AAAA;AAKlC,UAAM,KAAK,IACR,6DAA6D,KAAK;AAGrE,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,WAAW,MAKA;AACf,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,yDAAyD;AAAA,IAC3E;AAGA,UAAM,KAAK,IAAI,uDACb,KAAK,WAAW,KAAK;AAAA;AAAA;AAMvB,UAAM,KAAK,IACR,kEAAkE,KAAK;AAG1E,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,UAAU,MAKO;AACrB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,wDAAwD;AAAA,IAC1E;AAGA,UAAM,KAAK,IAAI,uDACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM;AAAA;AAAA;AAKtB,UAAM,KAAK,IACR,iEAAiE,KAAK;AAGzE,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,WAA6B;AAC/B,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,iEAAiE,KAAK;AAGzE,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,cAA4B;AAC9B,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,oEAAoE,KAAK;AAG5E,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,oBAAkC;AACpC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,0EAA0E,KAAK;AAGlF,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,oBAAsC;AACxC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,0EAA0E,KAAK;AAGlF,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,mBAAiC;AACnC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,yEAAyE,KAAK;AAGjF,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,gBAA8B;AAChC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,sEAAsE,KAAK;AAG9E,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AACF;;;ACjrBA,OAAOC,cAAY;AAeZ,IAAM,oBAAN,MAAwB;AAAA,EAQ7B,YAAY,MAkCT;AArCH,0BAA0B;AAC1B,uBAAuB;AAqCrB,SAAK,KAAK,oBAAoBA,SAAO,WAAW,EAAE,MAAM,GAAG,EAAE,CAAC;AAC9D,SAAK,OAAO,QAAQ,CAAC;AAAA,EACvB;AAAA,EAEA,IAAI,KAAmB;AACrB,WAAO,KAAK;AAAA,EACd;AAAA,EAEA,IAAI,GAAG,cAA4B;AACjC,SAAK,MAAM;AAAA,EACb;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,KAAK,IAAiC;AAC1C,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,KAAK,gBAAgB;AACvB;AAAA,IACF;AAEA,QAAI,CAAC,IAAI;AACP,YAAM,IAAI,MAAM,yDAAyD;AAAA,IAC3E;AAEA,SAAK,MAAM;AAEX,UAAM,KAAK,IAAI;AAAA;AAAA;AAAA;AAAA;AAAA;AAQf,UAAM,KAAK,IAAI,6CACb,KAAK,KAAK,YAAY,KAAK,8BACL,KAAK,KAAK,iBAAiB,KAAK,iBACtD,KAAK,KAAK,IAAI,KAAK,qBACN,KAAK,KAAK,QAAQ,KAAK,0BACpC,KAAK,KAAK,aAAa,KAAK,sBACd,KAAK,KAAK,SAAS,KAAK;AAAA;AAAA;AAIxC,UAAM,KAAK,IACR,6BAA6B,KAAK;AAErC,SAAK,iBAAiB;AAAA,EACxB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,UAAU;AACd,QAAI,KAAK,aAAa;AACpB;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB;AAAA,IACF;AAEA,UAAM,KAAK,IAAI,iCAAiC,KAAK;AAErD,SAAK,cAAc;AAAA,EACrB;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,IAAI,MAeO;AACf,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,iDAAiD;AAAA,IACnE;AAGA,UAAM,KAAK,IAAI,gDACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,mCACpB,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,+CACpB,KAAK,eAAe,KAAK,cACnB,KAAK,eAAe,MAAM;AAAA;AAAA;AAKlC,UAAM,KAAK,IACR,yDAAyD,KAAK;AAGjE,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,cAAc,MAeG;AACrB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IAAI,0DACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,mCACpB,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,+CACpB,KAAK,eAAe,KAAK,cACnB,KAAK,eAAe,MAAM;AAAA;AAAA;AAKlC,UAAM,KAAK,IACR,mEAAmE,KAAK;AAG3E,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,sBAAsB,MAKX;AACf,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IACR,sEACD,KAAK,gBAAgB,KAAK;AAAA;AAAA;AAM5B,UAAM,KAAK,IACR,2EAA2E,KAAK;AAGnF,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,QAAQ,MAUO;AACnB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,qDAAqD;AAAA,IACvE;AAGA,UAAM,KAAK,IAAI,oDACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,uCACpB,KAAK,gBAAgB,KAAK;AAAA;AAAA;AAM5B,UAAM,KAAK,IACR,6DAA6D,KAAK;AAGrE,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,MAAM,MAeQ;AAClB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,mDAAmD;AAAA,IACrE;AAGA,UAAM,KAAK,IAAI,kDACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,mCACpB,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,+CACpB,KAAK,eAAe,KAAK,cACnB,KAAK,eAAe,MAAM;AAAA;AAAA;AAKlC,UAAM,KAAK,IACR,2DAA2D,KAAK;AAGnE,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,WAAW,MAKA;AACf,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,wDAAwD;AAAA,IAC1E;AAGA,UAAM,KAAK,IAAI,sDACb,KAAK,WAAW,KAAK;AAAA;AAAA;AAMvB,UAAM,KAAK,IACR,gEAAgE,KAAK;AAGxE,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,UAAU,MAKO;AACrB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,uDAAuD;AAAA,IACzE;AAGA,UAAM,KAAK,IAAI,sDACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM;AAAA;AAAA;AAKtB,UAAM,KAAK,IACR,+DAA+D,KAAK;AAGvE,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,cAA4B;AAC9B,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,kEAAkE,KAAK;AAG1E,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,oBAAkC;AACpC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,wEAAwE,KAAK;AAGhF,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,oBAAsC;AACxC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,wEAAwE,KAAK;AAGhF,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,mBAAiC;AACnC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,uEAAuE,KAAK;AAG/E,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,gBAA8B;AAChC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,oEAAoE,KAAK;AAG5E,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AACF;;;AC5jBA,OAAOC,cAAY;AAWZ,IAAM,mBAAN,MAAuB;AAAA,EAQ5B,YAAY,MAoCT;AAvCH,0BAA0B;AAC1B,uBAAuB;AAuCrB,SAAK,KAAK,mBAAmBA,SAAO,WAAW,EAAE,MAAM,GAAG,EAAE,CAAC;AAC7D,SAAK,OAAO,QAAQ,CAAC;AAAA,EACvB;AAAA,EAEA,IAAI,KAAmB;AACrB,WAAO,KAAK;AAAA,EACd;AAAA,EAEA,IAAI,GAAG,cAA4B;AACjC,SAAK,MAAM;AAAA,EACb;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,KAAK,IAAiC;AAC1C,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,KAAK,gBAAgB;AACvB;AAAA,IACF;AAEA,QAAI,CAAC,IAAI;AACP,YAAM,IAAI,MAAM,wDAAwD;AAAA,IAC1E;AAEA,SAAK,MAAM;AAEX,UAAM,KAAK,IAAI;AAAA;AAAA;AAAA;AAAA;AAAA;AAQf,UAAM,KAAK,IAAI,4CACb,KAAK,KAAK,YAAY,KAAK,qBACd,KAAK,KAAK,QAAQ,KAAK,+BACpC,KAAK,KAAK,SAAS,KAAK,cAClB,KAAK,KAAK,SAAS,MAAM,+BAC/B,KAAK,KAAK,QAAQ,KAAK,gCAEvB,KAAK,KAAK,mBAAmB,KAAK,sBACpB,KAAK,KAAK,SAAS,KAAK;AAAA;AAAA;AAIxC,UAAM,KAAK,IACR,4BAA4B,KAAK;AAEpC,SAAK,iBAAiB;AAAA,EACxB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,UAAU;AACd,QAAI,KAAK,aAAa;AACpB;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB;AAAA,IACF;AAEA,UAAM,KAAK,IAAI,gCAAgC,KAAK;AAEpD,SAAK,cAAc;AAAA,EACrB;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,IAAI,MAeO;AACf,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,gDAAgD;AAAA,IAClE;AAGA,UAAM,KAAK,IAAI,+CACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,mCACpB,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,+CACpB,KAAK,eAAe,KAAK,cACnB,KAAK,eAAe,MAAM;AAAA;AAAA;AAKlC,UAAM,KAAK,IACR,uDAAuD,KAAK;AAG/D,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,cAAc,MAeD;AACjB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IAAI,yDACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,mCACpB,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,mCACpB,KAAK,YAAY,KAAK;AAAA;AAAA;AAMxB,UAAM,KAAK,IACR,iEAAiE,KAAK;AAGzE,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,sBAAsB,MAKX;AACf,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IACR,qEACD,KAAK,gBAAgB,KAAK;AAAA;AAAA;AAM5B,UAAM,KAAK,IACR,yEAAyE,KAAK;AAGjF,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,QAAQ,MAKS;AACrB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,oDAAoD;AAAA,IACtE;AAGA,UAAM,KAAK,IAAI,mDACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM;AAAA;AAAA;AAKtB,UAAM,KAAK,IACR,2DAA2D,KAAK;AAGnE,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,cAAc,MAKK;AACvB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IAAI,yDACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM;AAAA;AAAA;AAKtB,UAAM,KAAK,IACR,iEAAiE,KAAK;AAGzE,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,MAAM,MAeQ;AAClB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,kDAAkD;AAAA,IACpE;AAGA,UAAM,KAAK,IAAI,iDACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,mCACpB,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,+CACpB,KAAK,eAAe,KAAK,cACnB,KAAK,eAAe,MAAM;AAAA;AAAA;AAKlC,UAAM,KAAK,IACR,yDAAyD,KAAK;AAGjE,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,WAAW,MAKA;AACf,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,uDAAuD;AAAA,IACzE;AAGA,UAAM,KAAK,IAAI,qDACb,KAAK,WAAW,KAAK;AAAA;AAAA;AAMvB,UAAM,KAAK,IACR,8DAA8D,KAAK;AAGtE,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,UAAU,MAKC;AACf,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,sDAAsD;AAAA,IACxE;AAGA,UAAM,KAAK,IAAI,qDACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM;AAAA;AAAA;AAKtB,UAAM,KAAK,IACR,6DAA6D,KAAK;AAGrE,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,cAA4B;AAC9B,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,gEAAgE,KAAK;AAGxE,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,oBAAkC;AACpC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,sEAAsE,KAAK;AAG9E,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,MAAoB;AACtB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,wDAAwD;AAAA,IAC1E;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,wDAAwD,KAAK;AAGhE,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,WAA6B;AAC/B,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,6DAA6D,KAAK;AAGrE,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,oBAAsC;AACxC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,sEAAsE,KAAK;AAG9E,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AACF;;;AC1lBA,OAAOC,cAAY;AAaZ,IAAM,kBAAN,MAAsB;AAAA,EAQ3B,YAAY,MAsBT;AAzBH,0BAA0B;AAC1B,uBAAuB;AAyBrB,SAAK,KAAK,kBAAkBA,SAAO,WAAW,EAAE,MAAM,GAAG,EAAE,CAAC;AAC5D,SAAK,OAAO,QAAQ,CAAC;AAAA,EACvB;AAAA,EAEA,IAAI,KAAmB;AACrB,WAAO,KAAK;AAAA,EACd;AAAA,EAEA,IAAI,GAAG,cAA4B;AACjC,SAAK,MAAM;AAAA,EACb;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,KAAK,IAAiC;AAC1C,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI,MAAM,yDAAyD;AAAA,IAC3E;AAEA,QAAI,KAAK,gBAAgB;AACvB;AAAA,IACF;AAEA,QAAI,CAAC,IAAI;AACP,YAAM,IAAI,MAAM,uDAAuD;AAAA,IACzE;AAEA,SAAK,MAAM;AAEX,UAAM,KAAK,IAAI;AAAA;AAAA;AAAA;AAAA;AAAA;AAQf,UAAM,KAAK,IAAI,2CACb,KAAK,KAAK,YAAY,KAAK,+BACJ,KAAK,KAAK,SAAS,KAAK,cAC/C,KAAK,KAAK,SAAS,MAAM,+BACF,KAAK,KAAK,QAAQ,KAAK,sBAC9C,KAAK,KAAK,SAAS,KAAK;AAAA;AAAA;AAK1B,UAAM,KAAK,IACR,2BAA2B,KAAK;AAEnC,SAAK,iBAAiB;AAAA,EACxB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,UAAU;AACd,QAAI,KAAK,aAAa;AACpB;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB;AAAA,IACF;AAEA,UAAM,KAAK,IAAI,+BAA+B,KAAK;AAEnD,SAAK,cAAc;AAAA,EACrB;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,IAAI,MAeO;AACf,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI,MAAM,yDAAyD;AAAA,IAC3E;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,+CAA+C;AAAA,IACjE;AAGA,UAAM,KAAK,IAAI,8CACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,mCACpB,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,+CACpB,KAAK,eAAe,KAAK,cACnB,KAAK,eAAe,MAAM;AAAA;AAAA;AAKlC,UAAM,KAAK,IACR,qDAAqD,KAAK;AAG7D,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,cAAc,MAeD;AACjB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI,MAAM,yDAAyD;AAAA,IAC3E;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,yDAAyD;AAAA,IAC3E;AAGA,UAAM,KAAK,IAAI,wDACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,mCACpB,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,mCACpB,KAAK,YAAY,KAAK;AAAA;AAAA;AAMxB,UAAM,KAAK,IACR,+DAA+D,KAAK;AAGvE,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,sBAAsB,MAKX;AACf,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI,MAAM,yDAAyD;AAAA,IAC3E;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAGA,UAAM,KAAK,IACR,oEACD,KAAK,gBAAgB,KAAK;AAAA;AAAA;AAM5B,UAAM,KAAK,IACR,uEAAuE,KAAK;AAG/E,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,QAAQ,MAKO;AACnB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI,MAAM,yDAAyD;AAAA,IAC3E;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,mDAAmD;AAAA,IACrE;AAGA,UAAM,KAAK,IAAI,kDACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM;AAAA;AAAA;AAKtB,UAAM,KAAK,IACR,yDAAyD,KAAK;AAGjE,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,MAAM,MAeQ;AAClB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI,MAAM,yDAAyD;AAAA,IAC3E;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,iDAAiD;AAAA,IACnE;AAGA,UAAM,KAAK,IAAI,gDACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,mCACpB,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,+CACpB,KAAK,eAAe,KAAK,cACnB,KAAK,eAAe,MAAM;AAAA;AAAA;AAKlC,UAAM,KAAK,IACR,uDAAuD,KAAK;AAG/D,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,WAAW,MAKA;AACf,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI,MAAM,yDAAyD;AAAA,IAC3E;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,sDAAsD;AAAA,IACxE;AAGA,UAAM,KAAK,IAAI,oDACb,KAAK,WAAW,KAAK;AAAA;AAAA;AAMvB,UAAM,KAAK,IACR,4DAA4D,KAAK;AAGpE,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,UAAU,MAKO;AACrB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI,MAAM,yDAAyD;AAAA,IAC3E;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,qDAAqD;AAAA,IACvE;AAGA,UAAM,KAAK,IAAI,oDACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM;AAAA;AAAA;AAKtB,UAAM,KAAK,IACR,2DAA2D,KAAK;AAGnE,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,cAA4B;AAC9B,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI,MAAM,yDAAyD;AAAA,IAC3E;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,8DAA8D,KAAK;AAGtE,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,oBAAkC;AACpC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI,MAAM,yDAAyD;AAAA,IAC3E;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,oEAAoE,KAAK;AAG5E,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,oBAAsC;AACxC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI,MAAM,yDAAyD;AAAA,IAC3E;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,oEAAoE,KAAK;AAG5E,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AACF;","names":["crypto","crypto","crypto","crypto","crypto","crypto","crypto","crypto","crypto","crypto","crypto","crypto","crypto","crypto","crypto","crypto","crypto"]}