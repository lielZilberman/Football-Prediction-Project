import { PythonBridge, NDArray, ArrayLike, SparseMatrix } from '@/sklearn/types';
/**
  Nu-Support Vector Classification.

  Similar to SVC but uses a parameter to control the number of support vectors.

  The implementation is based on libsvm.

  Read more in the [User Guide](../svm.html#svm-classification).

  [Python Reference](https://scikit-learn.org/stable/modules/generated/sklearn.svm.NuSVC.html)
 */
export declare class NuSVC {
    id: string;
    opts: any;
    _py: PythonBridge;
    _isInitialized: boolean;
    _isDisposed: boolean;
    constructor(opts?: {
        /**
          An upper bound on the fraction of margin errors (see [User Guide](../svm.html#nu-svc)) and a lower bound of the fraction of support vectors. Should be in the interval (0, 1\].
    
          @defaultValue `0.5`
         */
        nu?: number;
        /**
          Specifies the kernel type to be used in the algorithm. If none is given, ‘rbf’ will be used. If a callable is given it is used to precompute the kernel matrix.
    
          @defaultValue `'rbf'`
         */
        kernel?: 'linear' | 'poly' | 'rbf' | 'sigmoid' | 'precomputed';
        /**
          Degree of the polynomial kernel function (‘poly’). Must be non-negative. Ignored by all other kernels.
    
          @defaultValue `3`
         */
        degree?: number;
        /**
          Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’.
    
          @defaultValue `'scale'`
         */
        gamma?: 'scale' | 'auto' | number;
        /**
          Independent term in kernel function. It is only significant in ‘poly’ and ‘sigmoid’.
    
          @defaultValue `0`
         */
        coef0?: number;
        /**
          Whether to use the shrinking heuristic. See the [User Guide](../svm.html#shrinking-svm).
    
          @defaultValue `true`
         */
        shrinking?: boolean;
        /**
          Whether to enable probability estimates. This must be enabled prior to calling `fit`, will slow down that method as it internally uses 5-fold cross-validation, and `predict\_proba` may be inconsistent with `predict`. Read more in the [User Guide](../svm.html#scores-probabilities).
    
          @defaultValue `false`
         */
        probability?: boolean;
        /**
          Tolerance for stopping criterion.
    
          @defaultValue `0.001`
         */
        tol?: number;
        /**
          Specify the size of the kernel cache (in MB).
    
          @defaultValue `200`
         */
        cache_size?: number;
        /**
          Set the parameter C of class i to class\_weight\[i\]\*C for SVC. If not given, all classes are supposed to have weight one. The “balanced” mode uses the values of y to automatically adjust weights inversely proportional to class frequencies as `n\_samples / (n\_classes \* np.bincount(y))`.
         */
        class_weight?: any | 'balanced';
        /**
          Enable verbose output. Note that this setting takes advantage of a per-process runtime setting in libsvm that, if enabled, may not work properly in a multithreaded context.
    
          @defaultValue `false`
         */
        verbose?: boolean;
        /**
          Hard limit on iterations within solver, or -1 for no limit.
    
          @defaultValue `-1`
         */
        max_iter?: number;
        /**
          Whether to return a one-vs-rest (‘ovr’) decision function of shape (n\_samples, n\_classes) as all other classifiers, or the original one-vs-one (‘ovo’) decision function of libsvm which has shape (n\_samples, n\_classes \* (n\_classes - 1) / 2). However, one-vs-one (‘ovo’) is always used as multi-class strategy. The parameter is ignored for binary classification.
    
          @defaultValue `'ovr'`
         */
        decision_function_shape?: 'ovo' | 'ovr';
        /**
          If true, `decision\_function\_shape='ovr'`, and number of classes > 2, [predict](../../glossary.html#term-predict) will break ties according to the confidence values of [decision\_function](../../glossary.html#term-decision_function); otherwise the first class among the tied classes is returned. Please note that breaking ties comes at a relatively high computational cost compared to a simple predict.
    
          @defaultValue `false`
         */
        break_ties?: boolean;
        /**
          Controls the pseudo random number generation for shuffling the data for probability estimates. Ignored when `probability` is `false`. Pass an int for reproducible output across multiple function calls. See [Glossary](../../glossary.html#term-random_state).
         */
        random_state?: number;
    });
    get py(): PythonBridge;
    set py(pythonBridge: PythonBridge);
    /**
      Initializes the underlying Python resources.
  
      This instance is not usable until the `Promise` returned by `init()` resolves.
     */
    init(py: PythonBridge): Promise<void>;
    /**
      Disposes of the underlying Python resources.
  
      Once `dispose()` is called, the instance is no longer usable.
     */
    dispose(): Promise<void>;
    /**
      Evaluate the decision function for the samples in X.
     */
    decision_function(opts: {
        /**
          The input samples.
         */
        X?: ArrayLike[];
    }): Promise<NDArray[]>;
    /**
      Fit the SVM model according to the given training data.
     */
    fit(opts: {
        /**
          Training vectors, where `n\_samples` is the number of samples and `n\_features` is the number of features. For kernel=”precomputed”, the expected shape of X is (n\_samples, n\_samples).
         */
        X?: ArrayLike | SparseMatrix[];
        /**
          Target values (class labels in classification, real numbers in regression).
         */
        y?: ArrayLike;
        /**
          Per-sample weights. Rescale C per sample. Higher weights force the classifier to put more emphasis on these points.
         */
        sample_weight?: ArrayLike;
    }): Promise<any>;
    /**
      Perform classification on samples in X.
  
      For an one-class model, +1 or -1 is returned.
     */
    predict(opts: {
        /**
          For kernel=”precomputed”, the expected shape of X is (n\_samples\_test, n\_samples\_train).
         */
        X?: ArrayLike | SparseMatrix[];
    }): Promise<NDArray>;
    /**
      Compute log probabilities of possible outcomes for samples in X.
  
      The model need to have probability information computed at training time: fit with attribute `probability` set to `true`.
     */
    predict_log_proba(opts: {
        /**
          For kernel=”precomputed”, the expected shape of X is (n\_samples\_test, n\_samples\_train).
         */
        X?: ArrayLike[];
    }): Promise<NDArray[]>;
    /**
      Compute probabilities of possible outcomes for samples in X.
  
      The model need to have probability information computed at training time: fit with attribute `probability` set to `true`.
     */
    predict_proba(opts: {
        /**
          For kernel=”precomputed”, the expected shape of X is (n\_samples\_test, n\_samples\_train).
         */
        X?: ArrayLike[];
    }): Promise<NDArray[]>;
    /**
      Return the mean accuracy on the given test data and labels.
  
      In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted.
     */
    score(opts: {
        /**
          Test samples.
         */
        X?: ArrayLike[];
        /**
          True labels for `X`.
         */
        y?: ArrayLike;
        /**
          Sample weights.
         */
        sample_weight?: ArrayLike;
    }): Promise<number>;
    /**
      Multipliers of parameter C of each class. Computed based on the `class\_weight` parameter.
     */
    get class_weight_(): Promise<NDArray>;
    /**
      The unique classes labels.
     */
    get classes_(): Promise<NDArray>;
    /**
      Dual coefficients of the support vector in the decision function (see [Mathematical formulation](../sgd.html#sgd-mathematical-formulation)), multiplied by their targets. For multiclass, coefficient for all 1-vs-1 classifiers. The layout of the coefficients in the multiclass case is somewhat non-trivial. See the [multi-class section of the User Guide](../svm.html#svm-multi-class) for details.
     */
    get dual_coef_(): Promise<NDArray[]>;
    /**
      0 if correctly fitted, 1 if the algorithm did not converge.
     */
    get fit_status_(): Promise<number>;
    /**
      Constants in decision function.
     */
    get intercept_(): Promise<NDArray>;
    /**
      Number of features seen during [fit](../../glossary.html#term-fit).
     */
    get n_features_in_(): Promise<number>;
    /**
      Names of features seen during [fit](../../glossary.html#term-fit). Defined only when `X` has feature names that are all strings.
     */
    get feature_names_in_(): Promise<NDArray>;
    /**
      Number of iterations run by the optimization routine to fit the model. The shape of this attribute depends on the number of models optimized which in turn depends on the number of classes.
     */
    get n_iter_(): Promise<NDArray>;
    /**
      Indices of support vectors.
     */
    get support_(): Promise<NDArray>;
    /**
      Support vectors.
     */
    get support_vectors_(): Promise<NDArray[]>;
    /**
      Array dimensions of training vector `X`.
     */
    get shape_fit_(): Promise<any[]>;
}
//# sourceMappingURL=NuSVC.d.ts.map