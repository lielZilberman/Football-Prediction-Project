import { PythonBridge, NDArray, ArrayLike, SparseMatrix } from '@/sklearn/types';
/**
  Logistic Regression (aka logit, MaxEnt) classifier.

  In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the ‘multi\_class’ option is set to ‘ovr’, and uses the cross-entropy loss if the ‘multi\_class’ option is set to ‘multinomial’. (Currently the ‘multinomial’ option is supported only by the ‘lbfgs’, ‘sag’, ‘saga’ and ‘newton-cg’ solvers.)

  This class implements regularized logistic regression using the ‘liblinear’ library, ‘newton-cg’, ‘sag’, ‘saga’ and ‘lbfgs’ solvers. **Note that regularization is applied by default**. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied).

  The ‘newton-cg’, ‘sag’, and ‘lbfgs’ solvers support only L2 regularization with primal formulation, or no regularization. The ‘liblinear’ solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty. The Elastic-Net regularization is only supported by the ‘saga’ solver.

  Read more in the [User Guide](../linear_model.html#logistic-regression).

  [Python Reference](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)
 */
export declare class LogisticRegression {
    id: string;
    opts: any;
    _py: PythonBridge;
    _isInitialized: boolean;
    _isDisposed: boolean;
    constructor(opts?: {
        /**
          Specify the norm of the penalty:
    
          @defaultValue `'l2'`
         */
        penalty?: 'l1' | 'l2' | 'elasticnet';
        /**
          Dual or primal formulation. Dual formulation is only implemented for l2 penalty with liblinear solver. Prefer dual=`false` when n\_samples > n\_features.
    
          @defaultValue `false`
         */
        dual?: boolean;
        /**
          Tolerance for stopping criteria.
    
          @defaultValue `0.0001`
         */
        tol?: number;
        /**
          Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization.
    
          @defaultValue `1`
         */
        C?: number;
        /**
          Specifies if a constant (a.k.a. bias or intercept) should be added to the decision function.
    
          @defaultValue `true`
         */
        fit_intercept?: boolean;
        /**
          Useful only when the solver ‘liblinear’ is used and self.fit\_intercept is set to `true`. In this case, x becomes \[x, self.intercept\_scaling\], i.e. a “synthetic” feature with constant value equal to intercept\_scaling is appended to the instance vector. The intercept becomes `intercept\_scaling \* synthetic\_feature\_weight`.
    
          Note! the synthetic feature weight is subject to l1/l2 regularization as all other features. To lessen the effect of regularization on synthetic feature weight (and therefore on the intercept) intercept\_scaling has to be increased.
    
          @defaultValue `1`
         */
        intercept_scaling?: number;
        /**
          Weights associated with classes in the form `{class\_label: weight}`. If not given, all classes are supposed to have weight one.
    
          The “balanced” mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as `n\_samples / (n\_classes \* np.bincount(y))`.
    
          Note that these weights will be multiplied with sample\_weight (passed through the fit method) if sample\_weight is specified.
         */
        class_weight?: any | 'balanced';
        /**
          Used when `solver` == ‘sag’, ‘saga’ or ‘liblinear’ to shuffle the data. See [Glossary](../../glossary.html#term-random_state) for details.
         */
        random_state?: number;
        /**
          Algorithm to use in the optimization problem. Default is ‘lbfgs’. To choose a solver, you might want to consider the following aspects:
    
          @defaultValue `'lbfgs'`
         */
        solver?: 'lbfgs' | 'liblinear' | 'newton-cg' | 'newton-cholesky' | 'sag' | 'saga';
        /**
          Maximum number of iterations taken for the solvers to converge.
    
          @defaultValue `100`
         */
        max_iter?: number;
        /**
          If the option chosen is ‘ovr’, then a binary problem is fit for each label. For ‘multinomial’ the loss minimised is the multinomial loss fit across the entire probability distribution, *even when the data is binary*. ‘multinomial’ is unavailable when solver=’liblinear’. ‘auto’ selects ‘ovr’ if the data is binary, or if solver=’liblinear’, and otherwise selects ‘multinomial’.
    
          @defaultValue `'auto'`
         */
        multi_class?: 'auto' | 'ovr' | 'multinomial';
        /**
          For the liblinear and lbfgs solvers set verbose to any positive number for verbosity.
    
          @defaultValue `0`
         */
        verbose?: number;
        /**
          When set to `true`, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. Useless for liblinear solver. See [the Glossary](../../glossary.html#term-warm_start).
    
          @defaultValue `false`
         */
        warm_start?: boolean;
        /**
          Number of CPU cores used when parallelizing over classes if multi\_class=’ovr’”. This parameter is ignored when the `solver` is set to ‘liblinear’ regardless of whether ‘multi\_class’ is specified or not. `undefined` means 1 unless in a [`joblib.parallel\_backend`](https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend "(in joblib v1.3.0.dev0)") context. `\-1` means using all processors. See [Glossary](../../glossary.html#term-n_jobs) for more details.
         */
        n_jobs?: number;
        /**
          The Elastic-Net mixing parameter, with `0 <= l1\_ratio <= 1`. Only used if `penalty='elasticnet'`. Setting `l1\_ratio=0` is equivalent to using `penalty='l2'`, while setting `l1\_ratio=1` is equivalent to using `penalty='l1'`. For `0 < l1\_ratio <1`, the penalty is a combination of L1 and L2.
         */
        l1_ratio?: number;
    });
    get py(): PythonBridge;
    set py(pythonBridge: PythonBridge);
    /**
      Initializes the underlying Python resources.
  
      This instance is not usable until the `Promise` returned by `init()` resolves.
     */
    init(py: PythonBridge): Promise<void>;
    /**
      Disposes of the underlying Python resources.
  
      Once `dispose()` is called, the instance is no longer usable.
     */
    dispose(): Promise<void>;
    /**
      Predict confidence scores for samples.
  
      The confidence score for a sample is proportional to the signed distance of that sample to the hyperplane.
     */
    decision_function(opts: {
        /**
          The data matrix for which we want to get the confidence scores.
         */
        X?: ArrayLike | SparseMatrix[];
    }): Promise<NDArray>;
    /**
      Convert coefficient matrix to dense array format.
  
      Converts the `coef\_` member (back) to a numpy.ndarray. This is the default format of `coef\_` and is required for fitting, so calling this method is only required on models that have previously been sparsified; otherwise, it is a no-op.
     */
    densify(opts: {}): Promise<any>;
    /**
      Fit the model according to the given training data.
     */
    fit(opts: {
        /**
          Training vector, where `n\_samples` is the number of samples and `n\_features` is the number of features.
         */
        X?: ArrayLike | SparseMatrix[];
        /**
          Target vector relative to X.
         */
        y?: ArrayLike;
        /**
          Array of weights that are assigned to individual samples. If not provided, then each sample is given unit weight.
         */
        sample_weight?: any;
    }): Promise<any>;
    /**
      Predict class labels for samples in X.
     */
    predict(opts: {
        /**
          The data matrix for which we want to get the predictions.
         */
        X?: ArrayLike | SparseMatrix[];
    }): Promise<NDArray>;
    /**
      Predict logarithm of probability estimates.
  
      The returned estimates for all classes are ordered by the label of classes.
     */
    predict_log_proba(opts: {
        /**
          Vector to be scored, where `n\_samples` is the number of samples and `n\_features` is the number of features.
         */
        X?: ArrayLike[];
    }): Promise<ArrayLike[]>;
    /**
      Probability estimates.
  
      The returned estimates for all classes are ordered by the label of classes.
  
      For a multi\_class problem, if multi\_class is set to be “multinomial” the softmax function is used to find the predicted probability of each class. Else use a one-vs-rest approach, i.e calculate the probability of each class assuming it to be positive using the logistic function. and normalize these values across all the classes.
     */
    predict_proba(opts: {
        /**
          Vector to be scored, where `n\_samples` is the number of samples and `n\_features` is the number of features.
         */
        X?: ArrayLike[];
    }): Promise<ArrayLike[]>;
    /**
      Return the mean accuracy on the given test data and labels.
  
      In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted.
     */
    score(opts: {
        /**
          Test samples.
         */
        X?: ArrayLike[];
        /**
          True labels for `X`.
         */
        y?: ArrayLike;
        /**
          Sample weights.
         */
        sample_weight?: ArrayLike;
    }): Promise<number>;
    /**
      Convert coefficient matrix to sparse format.
  
      Converts the `coef\_` member to a scipy.sparse matrix, which for L1-regularized models can be much more memory- and storage-efficient than the usual numpy.ndarray representation.
  
      The `intercept\_` member is not converted.
     */
    sparsify(opts: {}): Promise<any>;
    /**
      A list of class labels known to the classifier.
     */
    get classes_(): Promise<NDArray>;
    /**
      Coefficient of the features in the decision function.
  
      `coef\_` is of shape (1, n\_features) when the given problem is binary. In particular, when `multi\_class='multinomial'`, `coef\_` corresponds to outcome 1 (`true`) and `\-coef\_` corresponds to outcome 0 (`false`).
     */
    get coef_(): Promise<NDArray[]>;
    /**
      Intercept (a.k.a. bias) added to the decision function.
  
      If `fit\_intercept` is set to `false`, the intercept is set to zero. `intercept\_` is of shape (1,) when the given problem is binary. In particular, when `multi\_class='multinomial'`, `intercept\_` corresponds to outcome 1 (`true`) and `\-intercept\_` corresponds to outcome 0 (`false`).
     */
    get intercept_(): Promise<NDArray>;
    /**
      Number of features seen during [fit](../../glossary.html#term-fit).
     */
    get n_features_in_(): Promise<number>;
    /**
      Names of features seen during [fit](../../glossary.html#term-fit). Defined only when `X` has feature names that are all strings.
     */
    get feature_names_in_(): Promise<NDArray>;
    /**
      Actual number of iterations for all classes. If binary or multinomial, it returns only 1 element. For liblinear solver, only the maximum number of iteration across all classes is given.
     */
    get n_iter_(): Promise<NDArray>;
}
//# sourceMappingURL=LogisticRegression.d.ts.map