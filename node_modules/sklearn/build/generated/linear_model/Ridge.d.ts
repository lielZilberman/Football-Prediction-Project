import { PythonBridge, NDArray, ArrayLike, SparseMatrix } from '@/sklearn/types';
/**
  Linear least squares with l2 regularization.

  Minimizes the objective function:

  [Python Reference](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html)
 */
export declare class Ridge {
    id: string;
    opts: any;
    _py: PythonBridge;
    _isInitialized: boolean;
    _isDisposed: boolean;
    constructor(opts?: {
        /**
          Constant that multiplies the L2 term, controlling regularization strength. `alpha` must be a non-negative float i.e. in `\[0, inf)`.
    
          When `alpha \= 0`, the objective is equivalent to ordinary least squares, solved by the [`LinearRegression`](sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression "sklearn.linear_model.LinearRegression") object. For numerical reasons, using `alpha \= 0` with the `Ridge` object is not advised. Instead, you should use the [`LinearRegression`](sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression "sklearn.linear_model.LinearRegression") object.
    
          If an array is passed, penalties are assumed to be specific to the targets. Hence they must correspond in number.
    
          @defaultValue `1`
         */
        alpha?: number;
        /**
          Whether to fit the intercept for this model. If set to false, no intercept will be used in calculations (i.e. `X` and `y` are expected to be centered).
    
          @defaultValue `true`
         */
        fit_intercept?: boolean;
        /**
          If `true`, X will be copied; else, it may be overwritten.
    
          @defaultValue `true`
         */
        copy_X?: boolean;
        /**
          Maximum number of iterations for conjugate gradient solver. For ‘sparse\_cg’ and ‘lsqr’ solvers, the default value is determined by scipy.sparse.linalg. For ‘sag’ solver, the default value is 1000. For ‘lbfgs’ solver, the default value is 15000.
         */
        max_iter?: number;
        /**
          Precision of the solution. Note that `tol` has no effect for solvers ‘svd’ and ‘cholesky’.
    
          @defaultValue `0.0001`
         */
        tol?: number;
        /**
          Solver to use in the computational routines:
    
          @defaultValue `'auto'`
         */
        solver?: 'auto' | 'svd' | 'cholesky' | 'lsqr' | 'sparse_cg' | 'sag' | 'saga' | 'lbfgs';
        /**
          When set to `true`, forces the coefficients to be positive. Only ‘lbfgs’ solver is supported in this case.
    
          @defaultValue `false`
         */
        positive?: boolean;
        /**
          Used when `solver` == ‘sag’ or ‘saga’ to shuffle the data. See [Glossary](../../glossary.html#term-random_state) for details.
         */
        random_state?: number;
    });
    get py(): PythonBridge;
    set py(pythonBridge: PythonBridge);
    /**
      Initializes the underlying Python resources.
  
      This instance is not usable until the `Promise` returned by `init()` resolves.
     */
    init(py: PythonBridge): Promise<void>;
    /**
      Disposes of the underlying Python resources.
  
      Once `dispose()` is called, the instance is no longer usable.
     */
    dispose(): Promise<void>;
    /**
      Fit Ridge regression model.
     */
    fit(opts: {
        /**
          Training data.
         */
        X?: NDArray | SparseMatrix[];
        /**
          Target values.
         */
        y?: NDArray;
        /**
          Individual weights for each sample. If given a float, every sample will have the same weight.
         */
        sample_weight?: number | NDArray;
    }): Promise<any>;
    /**
      Predict using the linear model.
     */
    predict(opts: {
        /**
          Samples.
         */
        X?: ArrayLike | SparseMatrix;
    }): Promise<any>;
    /**
      Return the coefficient of determination of the prediction.
  
      The coefficient of determination \\(R^2\\) is defined as \\((1 - \\frac{u}{v})\\), where \\(u\\) is the residual sum of squares `((y\_true \- y\_pred)\*\* 2).sum()` and \\(v\\) is the total sum of squares `((y\_true \- y\_true.mean()) \*\* 2).sum()`. The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of `y`, disregarding the input features, would get a \\(R^2\\) score of 0.0.
     */
    score(opts: {
        /**
          Test samples. For some estimators this may be a precomputed kernel matrix or a list of generic objects instead with shape `(n\_samples, n\_samples\_fitted)`, where `n\_samples\_fitted` is the number of samples used in the fitting for the estimator.
         */
        X?: ArrayLike[];
        /**
          True values for `X`.
         */
        y?: ArrayLike;
        /**
          Sample weights.
         */
        sample_weight?: ArrayLike;
    }): Promise<number>;
    /**
      Weight vector(s).
     */
    get coef_(): Promise<NDArray>;
    /**
      Independent term in decision function. Set to 0.0 if `fit\_intercept \= False`.
     */
    get intercept_(): Promise<number | NDArray>;
    /**
      Actual number of iterations for each target. Available only for sag and lsqr solvers. Other solvers will return `undefined`.
     */
    get n_iter_(): Promise<NDArray>;
    /**
      Number of features seen during [fit](../../glossary.html#term-fit).
     */
    get n_features_in_(): Promise<number>;
    /**
      Names of features seen during [fit](../../glossary.html#term-fit). Defined only when `X` has feature names that are all strings.
     */
    get feature_names_in_(): Promise<NDArray>;
}
//# sourceMappingURL=Ridge.d.ts.map