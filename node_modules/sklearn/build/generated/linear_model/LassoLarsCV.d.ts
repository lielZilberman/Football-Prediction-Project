import { PythonBridge, NDArray, ArrayLike, SparseMatrix } from '@/sklearn/types';
/**
  Cross-validated Lasso, using the LARS algorithm.

  See glossary entry for [cross-validation estimator](../../glossary.html#term-cross-validation-estimator).

  The optimization objective for Lasso is:

  [Python Reference](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoLarsCV.html)
 */
export declare class LassoLarsCV {
    id: string;
    opts: any;
    _py: PythonBridge;
    _isInitialized: boolean;
    _isDisposed: boolean;
    constructor(opts?: {
        /**
          Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be centered).
    
          @defaultValue `true`
         */
        fit_intercept?: boolean;
        /**
          Sets the verbosity amount.
    
          @defaultValue `false`
         */
        verbose?: boolean | number;
        /**
          Maximum number of iterations to perform.
    
          @defaultValue `500`
         */
        max_iter?: number;
        /**
          This parameter is ignored when `fit\_intercept` is set to `false`. If `true`, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use [`StandardScaler`](sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler "sklearn.preprocessing.StandardScaler") before calling `fit` on an estimator with `normalize=False`.
    
          @defaultValue `false`
         */
        normalize?: boolean;
        /**
          Whether to use a precomputed Gram matrix to speed up calculations. If set to `'auto'` let us decide. The Gram matrix cannot be passed as argument since we will use only subsets of X.
    
          @defaultValue `'auto'`
         */
        precompute?: boolean | 'auto';
        /**
          Determines the cross-validation splitting strategy. Possible inputs for cv are:
         */
        cv?: number;
        /**
          The maximum number of points on the path used to compute the residuals in the cross-validation.
    
          @defaultValue `1000`
         */
        max_n_alphas?: number;
        /**
          Number of CPUs to use during the cross validation. `undefined` means 1 unless in a [`joblib.parallel\_backend`](https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend "(in joblib v1.3.0.dev0)") context. `\-1` means using all processors. See [Glossary](../../glossary.html#term-n_jobs) for more details.
         */
        n_jobs?: number;
        /**
          The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems. Unlike the `tol` parameter in some iterative optimization-based algorithms, this parameter does not control the tolerance of the optimization.
         */
        eps?: number;
        /**
          If `true`, X will be copied; else, it may be overwritten.
    
          @defaultValue `true`
         */
        copy_X?: boolean;
        /**
          Restrict coefficients to be >= 0. Be aware that you might want to remove fit\_intercept which is set `true` by default. Under the positive restriction the model coefficients do not converge to the ordinary-least-squares solution for small values of alpha. Only coefficients up to the smallest alpha value (`alphas\_\[alphas\_ > 0.\].min()` when fit\_path=`true`) reached by the stepwise Lars-Lasso algorithm are typically in congruence with the solution of the coordinate descent Lasso estimator. As a consequence using LassoLarsCV only makes sense for problems where a sparse solution is expected and/or reached.
    
          @defaultValue `false`
         */
        positive?: boolean;
    });
    get py(): PythonBridge;
    set py(pythonBridge: PythonBridge);
    /**
      Initializes the underlying Python resources.
  
      This instance is not usable until the `Promise` returned by `init()` resolves.
     */
    init(py: PythonBridge): Promise<void>;
    /**
      Disposes of the underlying Python resources.
  
      Once `dispose()` is called, the instance is no longer usable.
     */
    dispose(): Promise<void>;
    /**
      Fit the model using X, y as training data.
     */
    fit(opts: {
        /**
          Training data.
         */
        X?: ArrayLike[];
        /**
          Target values.
         */
        y?: ArrayLike;
    }): Promise<any>;
    /**
      Predict using the linear model.
     */
    predict(opts: {
        /**
          Samples.
         */
        X?: ArrayLike | SparseMatrix;
    }): Promise<any>;
    /**
      Return the coefficient of determination of the prediction.
  
      The coefficient of determination \\(R^2\\) is defined as \\((1 - \\frac{u}{v})\\), where \\(u\\) is the residual sum of squares `((y\_true \- y\_pred)\*\* 2).sum()` and \\(v\\) is the total sum of squares `((y\_true \- y\_true.mean()) \*\* 2).sum()`. The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of `y`, disregarding the input features, would get a \\(R^2\\) score of 0.0.
     */
    score(opts: {
        /**
          Test samples. For some estimators this may be a precomputed kernel matrix or a list of generic objects instead with shape `(n\_samples, n\_samples\_fitted)`, where `n\_samples\_fitted` is the number of samples used in the fitting for the estimator.
         */
        X?: ArrayLike[];
        /**
          True values for `X`.
         */
        y?: ArrayLike;
        /**
          Sample weights.
         */
        sample_weight?: ArrayLike;
    }): Promise<number>;
    /**
      parameter vector (w in the formulation formula)
     */
    get coef_(): Promise<ArrayLike>;
    /**
      independent term in decision function.
     */
    get intercept_(): Promise<number>;
    /**
      the varying values of the coefficients along the path
     */
    get coef_path_(): Promise<ArrayLike[]>;
    /**
      the estimated regularization parameter alpha
     */
    get alpha_(): Promise<number>;
    /**
      the different values of alpha along the path
     */
    get alphas_(): Promise<ArrayLike>;
    /**
      all the values of alpha along the path for the different folds
     */
    get cv_alphas_(): Promise<ArrayLike>;
    /**
      the mean square error on left-out for each fold along the path (alpha values given by `cv\_alphas`)
     */
    get mse_path_(): Promise<ArrayLike[]>;
    /**
      the number of iterations run by Lars with the optimal alpha.
     */
    get n_iter_(): Promise<ArrayLike | number>;
    /**
      Indices of active variables at the end of the path.
     */
    get active_(): Promise<any>;
    /**
      Number of features seen during [fit](../../glossary.html#term-fit).
     */
    get n_features_in_(): Promise<number>;
    /**
      Names of features seen during [fit](../../glossary.html#term-fit). Defined only when `X` has feature names that are all strings.
     */
    get feature_names_in_(): Promise<NDArray>;
}
//# sourceMappingURL=LassoLarsCV.d.ts.map