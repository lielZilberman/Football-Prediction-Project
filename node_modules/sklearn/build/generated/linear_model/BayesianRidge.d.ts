import { PythonBridge, NDArray, ArrayLike, SparseMatrix } from '@/sklearn/types';
/**
  Bayesian ridge regression.

  Fit a Bayesian ridge model. See the Notes section for details on this implementation and the optimization of the regularization parameters lambda (precision of the weights) and alpha (precision of the noise).

  Read more in the [User Guide](../linear_model.html#bayesian-regression).

  [Python Reference](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.BayesianRidge.html)
 */
export declare class BayesianRidge {
    id: string;
    opts: any;
    _py: PythonBridge;
    _isInitialized: boolean;
    _isDisposed: boolean;
    constructor(opts?: {
        /**
          Maximum number of iterations. Should be greater than or equal to 1.
    
          @defaultValue `300`
         */
        n_iter?: number;
        /**
          Stop the algorithm if w has converged.
    
          @defaultValue `0.001`
         */
        tol?: number;
        /**
          Hyper-parameter : shape parameter for the Gamma distribution prior over the alpha parameter.
    
          @defaultValue `0.000001`
         */
        alpha_1?: number;
        /**
          Hyper-parameter : inverse scale parameter (rate parameter) for the Gamma distribution prior over the alpha parameter.
    
          @defaultValue `0.000001`
         */
        alpha_2?: number;
        /**
          Hyper-parameter : shape parameter for the Gamma distribution prior over the lambda parameter.
    
          @defaultValue `0.000001`
         */
        lambda_1?: number;
        /**
          Hyper-parameter : inverse scale parameter (rate parameter) for the Gamma distribution prior over the lambda parameter.
    
          @defaultValue `0.000001`
         */
        lambda_2?: number;
        /**
          Initial value for alpha (precision of the noise). If not set, alpha\_init is 1/Var(y).
         */
        alpha_init?: number;
        /**
          Initial value for lambda (precision of the weights). If not set, lambda\_init is 1.
         */
        lambda_init?: number;
        /**
          If `true`, compute the log marginal likelihood at each iteration of the optimization.
    
          @defaultValue `false`
         */
        compute_score?: boolean;
        /**
          Whether to calculate the intercept for this model. The intercept is not treated as a probabilistic parameter and thus has no associated variance. If set to `false`, no intercept will be used in calculations (i.e. data is expected to be centered).
    
          @defaultValue `true`
         */
        fit_intercept?: boolean;
        /**
          If `true`, X will be copied; else, it may be overwritten.
    
          @defaultValue `true`
         */
        copy_X?: boolean;
        /**
          Verbose mode when fitting the model.
    
          @defaultValue `false`
         */
        verbose?: boolean;
    });
    get py(): PythonBridge;
    set py(pythonBridge: PythonBridge);
    /**
      Initializes the underlying Python resources.
  
      This instance is not usable until the `Promise` returned by `init()` resolves.
     */
    init(py: PythonBridge): Promise<void>;
    /**
      Disposes of the underlying Python resources.
  
      Once `dispose()` is called, the instance is no longer usable.
     */
    dispose(): Promise<void>;
    /**
      Fit the model.
     */
    fit(opts: {
        /**
          Training data.
         */
        X?: NDArray[];
        /**
          Target values. Will be cast to Xâ€™s dtype if necessary.
         */
        y?: NDArray;
        /**
          Individual weights for each sample.
         */
        sample_weight?: NDArray;
    }): Promise<any>;
    /**
      Predict using the linear model.
  
      In addition to the mean of the predictive distribution, also its standard deviation can be returned.
     */
    predict(opts: {
        /**
          Samples.
         */
        X?: ArrayLike | SparseMatrix[];
        /**
          Whether to return the standard deviation of posterior prediction.
    
          @defaultValue `false`
         */
        return_std?: boolean;
    }): Promise<ArrayLike>;
    /**
      Return the coefficient of determination of the prediction.
  
      The coefficient of determination \\(R^2\\) is defined as \\((1 - \\frac{u}{v})\\), where \\(u\\) is the residual sum of squares `((y\_true \- y\_pred)\*\* 2).sum()` and \\(v\\) is the total sum of squares `((y\_true \- y\_true.mean()) \*\* 2).sum()`. The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of `y`, disregarding the input features, would get a \\(R^2\\) score of 0.0.
     */
    score(opts: {
        /**
          Test samples. For some estimators this may be a precomputed kernel matrix or a list of generic objects instead with shape `(n\_samples, n\_samples\_fitted)`, where `n\_samples\_fitted` is the number of samples used in the fitting for the estimator.
         */
        X?: ArrayLike[];
        /**
          True values for `X`.
         */
        y?: ArrayLike;
        /**
          Sample weights.
         */
        sample_weight?: ArrayLike;
    }): Promise<number>;
    /**
      Coefficients of the regression model (mean of distribution)
     */
    get coef_(): Promise<ArrayLike>;
    /**
      Independent term in decision function. Set to 0.0 if `fit\_intercept \= False`.
     */
    get intercept_(): Promise<number>;
    /**
      Estimated precision of the noise.
     */
    get alpha_(): Promise<number>;
    /**
      Estimated precision of the weights.
     */
    get lambda_(): Promise<number>;
    /**
      Estimated variance-covariance matrix of the weights
     */
    get sigma_(): Promise<ArrayLike[]>;
    /**
      If computed\_score is `true`, value of the log marginal likelihood (to be maximized) at each iteration of the optimization. The array starts with the value of the log marginal likelihood obtained for the initial values of alpha and lambda and ends with the value obtained for the estimated alpha and lambda.
     */
    get scores_(): Promise<ArrayLike>;
    /**
      The actual number of iterations to reach the stopping criterion.
     */
    get n_iter_(): Promise<number>;
    /**
      If `fit\_intercept=True`, offset subtracted for centering data to a zero mean. Set to np.zeros(n\_features) otherwise.
     */
    get X_offset_(): Promise<NDArray>;
    /**
      Set to np.ones(n\_features).
     */
    get X_scale_(): Promise<NDArray>;
    /**
      Number of features seen during [fit](../../glossary.html#term-fit).
     */
    get n_features_in_(): Promise<number>;
    /**
      Names of features seen during [fit](../../glossary.html#term-fit). Defined only when `X` has feature names that are all strings.
     */
    get feature_names_in_(): Promise<NDArray>;
}
//# sourceMappingURL=BayesianRidge.d.ts.map