{"version":3,"sources":["../../../src/generated/linear_model/ElasticNet.ts"],"sourcesContent":["/* eslint-disable */\n/* NOTE: This file is auto-generated. Do not edit it directly. */\n\nimport crypto from 'node:crypto'\n\nimport { PythonBridge, NDArray, ArrayLike, SparseMatrix } from '@/sklearn/types'\n\n/**\n  Linear regression with combined L1 and L2 priors as regularizer.\n\n  Minimizes the objective function:\n\n  [Python Reference](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html)\n */\nexport class ElasticNet {\n  id: string\n  opts: any\n\n  _py: PythonBridge\n  _isInitialized: boolean = false\n  _isDisposed: boolean = false\n\n  constructor(opts?: {\n    /**\n      Constant that multiplies the penalty terms. Defaults to 1.0. See the notes for the exact mathematical meaning of this parameter. `alpha \\= 0` is equivalent to an ordinary least square, solved by the [`LinearRegression`](sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression \"sklearn.linear_model.LinearRegression\") object. For numerical reasons, using `alpha \\= 0` with the `Lasso` object is not advised. Given this, you should use the [`LinearRegression`](sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression \"sklearn.linear_model.LinearRegression\") object.\n\n      @defaultValue `1`\n     */\n    alpha?: number\n\n    /**\n      The ElasticNet mixing parameter, with `0 <= l1\\_ratio <= 1`. For `l1\\_ratio \\= 0` the penalty is an L2 penalty. `For l1\\_ratio \\= 1` it is an L1 penalty. For `0 < l1\\_ratio < 1`, the penalty is a combination of L1 and L2.\n\n      @defaultValue `0.5`\n     */\n    l1_ratio?: number\n\n    /**\n      Whether the intercept should be estimated or not. If `false`, the data is assumed to be already centered.\n\n      @defaultValue `true`\n     */\n    fit_intercept?: boolean\n\n    /**\n      Whether to use a precomputed Gram matrix to speed up calculations. The Gram matrix can also be passed as argument. For sparse input this option is always `false` to preserve sparsity.\n\n      @defaultValue `false`\n     */\n    precompute?: boolean | ArrayLike[]\n\n    /**\n      The maximum number of iterations.\n\n      @defaultValue `1000`\n     */\n    max_iter?: number\n\n    /**\n      If `true`, X will be copied; else, it may be overwritten.\n\n      @defaultValue `true`\n     */\n    copy_X?: boolean\n\n    /**\n      The tolerance for the optimization: if the updates are smaller than `tol`, the optimization code checks the dual gap for optimality and continues until it is smaller than `tol`, see Notes below.\n\n      @defaultValue `0.0001`\n     */\n    tol?: number\n\n    /**\n      When set to `true`, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. See [the Glossary](../../glossary.html#term-warm_start).\n\n      @defaultValue `false`\n     */\n    warm_start?: boolean\n\n    /**\n      When set to `true`, forces the coefficients to be positive.\n\n      @defaultValue `false`\n     */\n    positive?: boolean\n\n    /**\n      The seed of the pseudo random number generator that selects a random feature to update. Used when `selection` == ‘random’. Pass an int for reproducible output across multiple function calls. See [Glossary](../../glossary.html#term-random_state).\n     */\n    random_state?: number\n\n    /**\n      If set to ‘random’, a random coefficient is updated every iteration rather than looping over features sequentially by default. This (setting to ‘random’) often leads to significantly faster convergence especially when tol is higher than 1e-4.\n\n      @defaultValue `'cyclic'`\n     */\n    selection?: 'cyclic' | 'random'\n  }) {\n    this.id = `ElasticNet${crypto.randomUUID().split('-')[0]}`\n    this.opts = opts || {}\n  }\n\n  get py(): PythonBridge {\n    return this._py\n  }\n\n  set py(pythonBridge: PythonBridge) {\n    this._py = pythonBridge\n  }\n\n  /**\n    Initializes the underlying Python resources.\n\n    This instance is not usable until the `Promise` returned by `init()` resolves.\n   */\n  async init(py: PythonBridge): Promise<void> {\n    if (this._isDisposed) {\n      throw new Error('This ElasticNet instance has already been disposed')\n    }\n\n    if (this._isInitialized) {\n      return\n    }\n\n    if (!py) {\n      throw new Error('ElasticNet.init requires a PythonBridge instance')\n    }\n\n    this._py = py\n\n    await this._py.ex`\nimport numpy as np\nfrom sklearn.linear_model import ElasticNet\ntry: bridgeElasticNet\nexcept NameError: bridgeElasticNet = {}\n`\n\n    // set up constructor params\n    await this._py.ex`ctor_ElasticNet = {'alpha': ${\n      this.opts['alpha'] ?? undefined\n    }, 'l1_ratio': ${this.opts['l1_ratio'] ?? undefined}, 'fit_intercept': ${\n      this.opts['fit_intercept'] ?? undefined\n    }, 'precompute': np.array(${this.opts['precompute'] ?? undefined}) if ${\n      this.opts['precompute'] !== undefined\n    } else None, 'max_iter': ${this.opts['max_iter'] ?? undefined}, 'copy_X': ${\n      this.opts['copy_X'] ?? undefined\n    }, 'tol': ${this.opts['tol'] ?? undefined}, 'warm_start': ${\n      this.opts['warm_start'] ?? undefined\n    }, 'positive': ${this.opts['positive'] ?? undefined}, 'random_state': ${\n      this.opts['random_state'] ?? undefined\n    }, 'selection': ${this.opts['selection'] ?? undefined}}\n\nctor_ElasticNet = {k: v for k, v in ctor_ElasticNet.items() if v is not None}`\n\n    await this._py\n      .ex`bridgeElasticNet[${this.id}] = ElasticNet(**ctor_ElasticNet)`\n\n    this._isInitialized = true\n  }\n\n  /**\n    Disposes of the underlying Python resources.\n\n    Once `dispose()` is called, the instance is no longer usable.\n   */\n  async dispose() {\n    if (this._isDisposed) {\n      return\n    }\n\n    if (!this._isInitialized) {\n      return\n    }\n\n    await this._py.ex`del bridgeElasticNet[${this.id}]`\n\n    this._isDisposed = true\n  }\n\n  /**\n    Fit model with coordinate descent.\n   */\n  async fit(opts: {\n    /**\n      Data.\n     */\n    X?: any\n\n    /**\n      Target. Will be cast to X’s dtype if necessary.\n     */\n    y?: NDArray | SparseMatrix\n\n    /**\n      Sample weights. Internally, the `sample\\_weight` vector will be rescaled to sum to `n\\_samples`.\n     */\n    sample_weight?: number | ArrayLike\n\n    /**\n      Allow to bypass several input checking. Don’t use this parameter unless you know what you do.\n\n      @defaultValue `true`\n     */\n    check_input?: boolean\n  }): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error('This ElasticNet instance has already been disposed')\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('ElasticNet must call init() before fit()')\n    }\n\n    // set up method params\n    await this._py.ex`pms_ElasticNet_fit = {'X': ${\n      opts['X'] ?? undefined\n    }, 'y': np.array(${opts['y'] ?? undefined}) if ${\n      opts['y'] !== undefined\n    } else None, 'sample_weight': np.array(${\n      opts['sample_weight'] ?? undefined\n    }) if ${opts['sample_weight'] !== undefined} else None, 'check_input': ${\n      opts['check_input'] ?? undefined\n    }}\n\npms_ElasticNet_fit = {k: v for k, v in pms_ElasticNet_fit.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_ElasticNet_fit = bridgeElasticNet[${this.id}].fit(**pms_ElasticNet_fit)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_ElasticNet_fit.tolist() if hasattr(res_ElasticNet_fit, 'tolist') else res_ElasticNet_fit`\n  }\n\n  /**\n    Compute elastic net path with coordinate descent.\n\n    The elastic net optimization function varies for mono and multi-outputs.\n\n    For mono-output tasks it is:\n   */\n  async path(opts: {\n    /**\n      Training data. Pass directly as Fortran-contiguous data to avoid unnecessary memory duplication. If `y` is mono-output then `X` can be sparse.\n     */\n    X?: ArrayLike | SparseMatrix[]\n\n    /**\n      Target values.\n     */\n    y?: ArrayLike | SparseMatrix\n\n    /**\n      Number between 0 and 1 passed to elastic net (scaling between l1 and l2 penalties). `l1\\_ratio=1` corresponds to the Lasso.\n\n      @defaultValue `0.5`\n     */\n    l1_ratio?: number\n\n    /**\n      Length of the path. `eps=1e-3` means that `alpha\\_min / alpha\\_max \\= 1e-3`.\n\n      @defaultValue `0.001`\n     */\n    eps?: number\n\n    /**\n      Number of alphas along the regularization path.\n\n      @defaultValue `100`\n     */\n    n_alphas?: number\n\n    /**\n      List of alphas where to compute the models. If `undefined` alphas are set automatically.\n     */\n    alphas?: NDArray\n\n    /**\n      Whether to use a precomputed Gram matrix to speed up calculations. If set to `'auto'` let us decide. The Gram matrix can also be passed as argument.\n\n      @defaultValue `'auto'`\n     */\n    precompute?: 'auto' | boolean | ArrayLike[]\n\n    /**\n      Xy = np.dot(X.T, y) that can be precomputed. It is useful only when the Gram matrix is precomputed.\n     */\n    Xy?: ArrayLike\n\n    /**\n      If `true`, X will be copied; else, it may be overwritten.\n\n      @defaultValue `true`\n     */\n    copy_X?: boolean\n\n    /**\n      The initial values of the coefficients.\n     */\n    coef_init?: NDArray\n\n    /**\n      Amount of verbosity.\n\n      @defaultValue `false`\n     */\n    verbose?: boolean | number\n\n    /**\n      Whether to return the number of iterations or not.\n\n      @defaultValue `false`\n     */\n    return_n_iter?: boolean\n\n    /**\n      If set to `true`, forces coefficients to be positive. (Only allowed when `y.ndim \\== 1`).\n\n      @defaultValue `false`\n     */\n    positive?: boolean\n\n    /**\n      If set to `false`, the input validation checks are skipped (including the Gram matrix when provided). It is assumed that they are handled by the caller.\n\n      @defaultValue `true`\n     */\n    check_input?: boolean\n\n    /**\n      Keyword arguments passed to the coordinate descent solver.\n     */\n    params?: any\n  }): Promise<NDArray> {\n    if (this._isDisposed) {\n      throw new Error('This ElasticNet instance has already been disposed')\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('ElasticNet must call init() before path()')\n    }\n\n    // set up method params\n    await this._py.ex`pms_ElasticNet_path = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None, 'y': np.array(${\n      opts['y'] ?? undefined\n    }) if ${opts['y'] !== undefined} else None, 'l1_ratio': ${\n      opts['l1_ratio'] ?? undefined\n    }, 'eps': ${opts['eps'] ?? undefined}, 'n_alphas': ${\n      opts['n_alphas'] ?? undefined\n    }, 'alphas': np.array(${opts['alphas'] ?? undefined}) if ${\n      opts['alphas'] !== undefined\n    } else None, 'precompute': np.array(${\n      opts['precompute'] ?? undefined\n    }) if ${opts['precompute'] !== undefined} else None, 'Xy': np.array(${\n      opts['Xy'] ?? undefined\n    }) if ${opts['Xy'] !== undefined} else None, 'copy_X': ${\n      opts['copy_X'] ?? undefined\n    }, 'coef_init': np.array(${opts['coef_init'] ?? undefined}) if ${\n      opts['coef_init'] !== undefined\n    } else None, 'verbose': ${opts['verbose'] ?? undefined}, 'return_n_iter': ${\n      opts['return_n_iter'] ?? undefined\n    }, 'positive': ${opts['positive'] ?? undefined}, 'check_input': ${\n      opts['check_input'] ?? undefined\n    }, 'params': ${opts['params'] ?? undefined}}\n\npms_ElasticNet_path = {k: v for k, v in pms_ElasticNet_path.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_ElasticNet_path = bridgeElasticNet[${this.id}].path(**pms_ElasticNet_path)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_ElasticNet_path.tolist() if hasattr(res_ElasticNet_path, 'tolist') else res_ElasticNet_path`\n  }\n\n  /**\n    Predict using the linear model.\n   */\n  async predict(opts: {\n    /**\n      Samples.\n     */\n    X?: ArrayLike | SparseMatrix\n  }): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error('This ElasticNet instance has already been disposed')\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('ElasticNet must call init() before predict()')\n    }\n\n    // set up method params\n    await this._py.ex`pms_ElasticNet_predict = {'X': ${opts['X'] ?? undefined}}\n\npms_ElasticNet_predict = {k: v for k, v in pms_ElasticNet_predict.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_ElasticNet_predict = bridgeElasticNet[${this.id}].predict(**pms_ElasticNet_predict)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_ElasticNet_predict.tolist() if hasattr(res_ElasticNet_predict, 'tolist') else res_ElasticNet_predict`\n  }\n\n  /**\n    Return the coefficient of determination of the prediction.\n\n    The coefficient of determination \\\\(R^2\\\\) is defined as \\\\((1 - \\\\frac{u}{v})\\\\), where \\\\(u\\\\) is the residual sum of squares `((y\\_true \\- y\\_pred)\\*\\* 2).sum()` and \\\\(v\\\\) is the total sum of squares `((y\\_true \\- y\\_true.mean()) \\*\\* 2).sum()`. The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of `y`, disregarding the input features, would get a \\\\(R^2\\\\) score of 0.0.\n   */\n  async score(opts: {\n    /**\n      Test samples. For some estimators this may be a precomputed kernel matrix or a list of generic objects instead with shape `(n\\_samples, n\\_samples\\_fitted)`, where `n\\_samples\\_fitted` is the number of samples used in the fitting for the estimator.\n     */\n    X?: ArrayLike[]\n\n    /**\n      True values for `X`.\n     */\n    y?: ArrayLike\n\n    /**\n      Sample weights.\n     */\n    sample_weight?: ArrayLike\n  }): Promise<number> {\n    if (this._isDisposed) {\n      throw new Error('This ElasticNet instance has already been disposed')\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('ElasticNet must call init() before score()')\n    }\n\n    // set up method params\n    await this._py.ex`pms_ElasticNet_score = {'X': np.array(${\n      opts['X'] ?? undefined\n    }) if ${opts['X'] !== undefined} else None, 'y': np.array(${\n      opts['y'] ?? undefined\n    }) if ${opts['y'] !== undefined} else None, 'sample_weight': np.array(${\n      opts['sample_weight'] ?? undefined\n    }) if ${opts['sample_weight'] !== undefined} else None}\n\npms_ElasticNet_score = {k: v for k, v in pms_ElasticNet_score.items() if v is not None}`\n\n    // invoke method\n    await this._py\n      .ex`res_ElasticNet_score = bridgeElasticNet[${this.id}].score(**pms_ElasticNet_score)`\n\n    // convert the result from python to node.js\n    return this\n      ._py`res_ElasticNet_score.tolist() if hasattr(res_ElasticNet_score, 'tolist') else res_ElasticNet_score`\n  }\n\n  /**\n    Parameter vector (w in the cost function formula).\n   */\n  get coef_(): Promise<NDArray> {\n    if (this._isDisposed) {\n      throw new Error('This ElasticNet instance has already been disposed')\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('ElasticNet must call init() before accessing coef_')\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_ElasticNet_coef_ = bridgeElasticNet[${this.id}].coef_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_ElasticNet_coef_.tolist() if hasattr(attr_ElasticNet_coef_, 'tolist') else attr_ElasticNet_coef_`\n    })()\n  }\n\n  /**\n    Independent term in decision function.\n   */\n  get intercept_(): Promise<number | NDArray> {\n    if (this._isDisposed) {\n      throw new Error('This ElasticNet instance has already been disposed')\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('ElasticNet must call init() before accessing intercept_')\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_ElasticNet_intercept_ = bridgeElasticNet[${this.id}].intercept_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_ElasticNet_intercept_.tolist() if hasattr(attr_ElasticNet_intercept_, 'tolist') else attr_ElasticNet_intercept_`\n    })()\n  }\n\n  /**\n    Number of iterations run by the coordinate descent solver to reach the specified tolerance.\n   */\n  get n_iter_(): Promise<any> {\n    if (this._isDisposed) {\n      throw new Error('This ElasticNet instance has already been disposed')\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('ElasticNet must call init() before accessing n_iter_')\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_ElasticNet_n_iter_ = bridgeElasticNet[${this.id}].n_iter_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_ElasticNet_n_iter_.tolist() if hasattr(attr_ElasticNet_n_iter_, 'tolist') else attr_ElasticNet_n_iter_`\n    })()\n  }\n\n  /**\n    Given param alpha, the dual gaps at the end of the optimization, same shape as each observation of y.\n   */\n  get dual_gap_(): Promise<number | NDArray> {\n    if (this._isDisposed) {\n      throw new Error('This ElasticNet instance has already been disposed')\n    }\n\n    if (!this._isInitialized) {\n      throw new Error('ElasticNet must call init() before accessing dual_gap_')\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_ElasticNet_dual_gap_ = bridgeElasticNet[${this.id}].dual_gap_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_ElasticNet_dual_gap_.tolist() if hasattr(attr_ElasticNet_dual_gap_, 'tolist') else attr_ElasticNet_dual_gap_`\n    })()\n  }\n\n  /**\n    Number of features seen during [fit](../../glossary.html#term-fit).\n   */\n  get n_features_in_(): Promise<number> {\n    if (this._isDisposed) {\n      throw new Error('This ElasticNet instance has already been disposed')\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'ElasticNet must call init() before accessing n_features_in_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_ElasticNet_n_features_in_ = bridgeElasticNet[${this.id}].n_features_in_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_ElasticNet_n_features_in_.tolist() if hasattr(attr_ElasticNet_n_features_in_, 'tolist') else attr_ElasticNet_n_features_in_`\n    })()\n  }\n\n  /**\n    Names of features seen during [fit](../../glossary.html#term-fit). Defined only when `X` has feature names that are all strings.\n   */\n  get feature_names_in_(): Promise<NDArray> {\n    if (this._isDisposed) {\n      throw new Error('This ElasticNet instance has already been disposed')\n    }\n\n    if (!this._isInitialized) {\n      throw new Error(\n        'ElasticNet must call init() before accessing feature_names_in_'\n      )\n    }\n\n    return (async () => {\n      // invoke accessor\n      await this._py\n        .ex`attr_ElasticNet_feature_names_in_ = bridgeElasticNet[${this.id}].feature_names_in_`\n\n      // convert the result from python to node.js\n      return this\n        ._py`attr_ElasticNet_feature_names_in_.tolist() if hasattr(attr_ElasticNet_feature_names_in_, 'tolist') else attr_ElasticNet_feature_names_in_`\n    })()\n  }\n}\n"],"mappings":";AAGA,OAAO,YAAY;AAWZ,IAAM,aAAN,MAAiB;AAAA,EAQtB,YAAY,MA2ET;AA9EH,0BAA0B;AAC1B,uBAAuB;AA8ErB,SAAK,KAAK,aAAa,OAAO,WAAW,EAAE,MAAM,GAAG,EAAE,CAAC;AACvD,SAAK,OAAO,QAAQ,CAAC;AAAA,EACvB;AAAA,EAEA,IAAI,KAAmB;AACrB,WAAO,KAAK;AAAA,EACd;AAAA,EAEA,IAAI,GAAG,cAA4B;AACjC,SAAK,MAAM;AAAA,EACb;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,KAAK,IAAiC;AAC1C,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI,MAAM,oDAAoD;AAAA,IACtE;AAEA,QAAI,KAAK,gBAAgB;AACvB;AAAA,IACF;AAEA,QAAI,CAAC,IAAI;AACP,YAAM,IAAI,MAAM,kDAAkD;AAAA,IACpE;AAEA,SAAK,MAAM;AAEX,UAAM,KAAK,IAAI;AAAA;AAAA;AAAA;AAAA;AAAA;AAQf,UAAM,KAAK,IAAI,iCACb,KAAK,KAAK,OAAO,KAAK,uBACP,KAAK,KAAK,UAAU,KAAK,4BACxC,KAAK,KAAK,eAAe,KAAK,kCACJ,KAAK,KAAK,YAAY,KAAK,cACrD,KAAK,KAAK,YAAY,MAAM,iCACH,KAAK,KAAK,UAAU,KAAK,qBAClD,KAAK,KAAK,QAAQ,KAAK,kBACb,KAAK,KAAK,KAAK,KAAK,yBAC9B,KAAK,KAAK,YAAY,KAAK,uBACZ,KAAK,KAAK,UAAU,KAAK,2BACxC,KAAK,KAAK,cAAc,KAAK,wBACb,KAAK,KAAK,WAAW,KAAK;AAAA;AAAA;AAI5C,UAAM,KAAK,IACR,sBAAsB,KAAK;AAE9B,SAAK,iBAAiB;AAAA,EACxB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,UAAU;AACd,QAAI,KAAK,aAAa;AACpB;AAAA,IACF;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB;AAAA,IACF;AAEA,UAAM,KAAK,IAAI,0BAA0B,KAAK;AAE9C,SAAK,cAAc;AAAA,EACrB;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,IAAI,MAsBO;AACf,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI,MAAM,oDAAoD;AAAA,IACtE;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,0CAA0C;AAAA,IAC5D;AAGA,UAAM,KAAK,IAAI,gCACb,KAAK,GAAG,KAAK,yBACI,KAAK,GAAG,KAAK,cAC9B,KAAK,GAAG,MAAM,+CAEd,KAAK,eAAe,KAAK,cACnB,KAAK,eAAe,MAAM,oCAChC,KAAK,aAAa,KAAK;AAAA;AAAA;AAMzB,UAAM,KAAK,IACR,2CAA2C,KAAK;AAGnD,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EASA,MAAM,KAAK,MA6FU;AACnB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI,MAAM,oDAAoD;AAAA,IACtE;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,2CAA2C;AAAA,IAC7D;AAGA,UAAM,KAAK,IAAI,0CACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,mCACpB,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,iCACpB,KAAK,UAAU,KAAK,kBACV,KAAK,KAAK,KAAK,uBACzB,KAAK,UAAU,KAAK,8BACE,KAAK,QAAQ,KAAK,cACxC,KAAK,QAAQ,MAAM,4CAEnB,KAAK,YAAY,KAAK,cAChB,KAAK,YAAY,MAAM,oCAC7B,KAAK,IAAI,KAAK,cACR,KAAK,IAAI,MAAM,+BACrB,KAAK,QAAQ,KAAK,iCACO,KAAK,WAAW,KAAK,cAC9C,KAAK,WAAW,MAAM,gCACE,KAAK,SAAS,KAAK,4BAC3C,KAAK,eAAe,KAAK,uBACV,KAAK,UAAU,KAAK,0BACnC,KAAK,aAAa,KAAK,qBACV,KAAK,QAAQ,KAAK;AAAA;AAAA;AAKjC,UAAM,KAAK,IACR,4CAA4C,KAAK;AAGpD,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,QAAQ,MAKG;AACf,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI,MAAM,oDAAoD;AAAA,IACtE;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,8CAA8C;AAAA,IAChE;AAGA,UAAM,KAAK,IAAI,oCAAoC,KAAK,GAAG,KAAK;AAAA;AAAA;AAKhE,UAAM,KAAK,IACR,+CAA+C,KAAK;AAGvD,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,MAAM,MAeQ;AAClB,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI,MAAM,oDAAoD;AAAA,IACtE;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,4CAA4C;AAAA,IAC9D;AAGA,UAAM,KAAK,IAAI,2CACb,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,mCACpB,KAAK,GAAG,KAAK,cACP,KAAK,GAAG,MAAM,+CACpB,KAAK,eAAe,KAAK,cACnB,KAAK,eAAe,MAAM;AAAA;AAAA;AAKlC,UAAM,KAAK,IACR,6CAA6C,KAAK;AAGrD,WAAO,KACJ;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,QAA0B;AAC5B,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI,MAAM,oDAAoD;AAAA,IACtE;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,oDAAoD;AAAA,IACtE;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,8CAA8C,KAAK;AAGtD,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,aAAwC;AAC1C,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI,MAAM,oDAAoD;AAAA,IACtE;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,yDAAyD;AAAA,IAC3E;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,mDAAmD,KAAK;AAG3D,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,UAAwB;AAC1B,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI,MAAM,oDAAoD;AAAA,IACtE;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,sDAAsD;AAAA,IACxE;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,gDAAgD,KAAK;AAGxD,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,YAAuC;AACzC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI,MAAM,oDAAoD;AAAA,IACtE;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI,MAAM,wDAAwD;AAAA,IAC1E;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,kDAAkD,KAAK;AAG1D,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,iBAAkC;AACpC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI,MAAM,oDAAoD;AAAA,IACtE;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,uDAAuD,KAAK;AAG/D,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AAAA;AAAA;AAAA;AAAA,EAKA,IAAI,oBAAsC;AACxC,QAAI,KAAK,aAAa;AACpB,YAAM,IAAI,MAAM,oDAAoD;AAAA,IACtE;AAEA,QAAI,CAAC,KAAK,gBAAgB;AACxB,YAAM,IAAI;AAAA,QACR;AAAA,MACF;AAAA,IACF;AAEA,YAAQ,YAAY;AAElB,YAAM,KAAK,IACR,0DAA0D,KAAK;AAGlE,aAAO,KACJ;AAAA,IACL,GAAG;AAAA,EACL;AACF;","names":[]}