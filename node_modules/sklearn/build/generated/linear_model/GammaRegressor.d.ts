import { PythonBridge, NDArray, ArrayLike, SparseMatrix } from '@/sklearn/types';
/**
  Generalized Linear Model with a Gamma distribution.

  This regressor uses the ‘log’ link function.

  Read more in the [User Guide](../linear_model.html#generalized-linear-models).

  [Python Reference](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.GammaRegressor.html)
 */
export declare class GammaRegressor {
    id: string;
    opts: any;
    _py: PythonBridge;
    _isInitialized: boolean;
    _isDisposed: boolean;
    constructor(opts?: {
        /**
          Constant that multiplies the L2 penalty term and determines the regularization strength. `alpha \= 0` is equivalent to unpenalized GLMs. In this case, the design matrix `X` must have full column rank (no collinearities). Values of `alpha` must be in the range `\[0.0, inf)`.
    
          @defaultValue `1`
         */
        alpha?: number;
        /**
          Specifies if a constant (a.k.a. bias or intercept) should be added to the linear predictor `X @ coef\_ + intercept\_`.
    
          @defaultValue `true`
         */
        fit_intercept?: boolean;
        /**
          Algorithm to use in the optimization problem:
    
          @defaultValue `'lbfgs'`
         */
        solver?: 'lbfgs' | 'newton-cholesky';
        /**
          The maximal number of iterations for the solver. Values must be in the range `\[1, inf)`.
    
          @defaultValue `100`
         */
        max_iter?: number;
        /**
          Stopping criterion. For the lbfgs solver, the iteration will stop when `max{|g\_j|, j \= 1, ..., d} <= tol` where `g\_j` is the j-th component of the gradient (derivative) of the objective function. Values must be in the range `(0.0, inf)`.
    
          @defaultValue `0.0001`
         */
        tol?: number;
        /**
          If set to `true`, reuse the solution of the previous call to `fit` as initialization for `coef\_` and `intercept\_`.
    
          @defaultValue `false`
         */
        warm_start?: boolean;
        /**
          For the lbfgs solver set verbose to any positive number for verbosity. Values must be in the range `\[0, inf)`.
    
          @defaultValue `0`
         */
        verbose?: number;
    });
    get py(): PythonBridge;
    set py(pythonBridge: PythonBridge);
    /**
      Initializes the underlying Python resources.
  
      This instance is not usable until the `Promise` returned by `init()` resolves.
     */
    init(py: PythonBridge): Promise<void>;
    /**
      Disposes of the underlying Python resources.
  
      Once `dispose()` is called, the instance is no longer usable.
     */
    dispose(): Promise<void>;
    /**
      Fit a Generalized Linear Model.
     */
    fit(opts: {
        /**
          Training data.
         */
        X?: ArrayLike | SparseMatrix[];
        /**
          Target values.
         */
        y?: ArrayLike;
        /**
          Sample weights.
         */
        sample_weight?: ArrayLike;
    }): Promise<any>;
    /**
      Predict using GLM with feature matrix X.
     */
    predict(opts: {
        /**
          Samples.
         */
        X?: ArrayLike | SparseMatrix[];
    }): Promise<any[]>;
    /**
      Compute D^2, the percentage of deviance explained.
  
      D^2 is a generalization of the coefficient of determination R^2. R^2 uses squared error and D^2 uses the deviance of this GLM, see the [User Guide](../model_evaluation.html#regression-metrics).
  
      D^2 is defined as \\(D^2 = 1-\\frac{D(y\_{true},y\_{pred})}{D\_{null}}\\), \\(D\_{null}\\) is the null deviance, i.e. the deviance of a model with intercept alone, which corresponds to \\(y\_{pred} = \\bar{y}\\). The mean \\(\\bar{y}\\) is averaged by sample\_weight. Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse).
     */
    score(opts: {
        /**
          Test samples.
         */
        X?: ArrayLike | SparseMatrix[];
        /**
          True values of target.
         */
        y?: ArrayLike;
        /**
          Sample weights.
         */
        sample_weight?: ArrayLike;
    }): Promise<number>;
    /**
      Estimated coefficients for the linear predictor (`X @ coef\_ + intercept\_`) in the GLM.
     */
    get coef_(): Promise<any[]>;
    /**
      Intercept (a.k.a. bias) added to linear predictor.
     */
    get intercept_(): Promise<number>;
    /**
      Number of features seen during [fit](../../glossary.html#term-fit).
     */
    get n_features_in_(): Promise<number>;
    /**
      Actual number of iterations used in the solver.
     */
    get n_iter_(): Promise<number>;
    /**
      Names of features seen during [fit](../../glossary.html#term-fit). Defined only when `X` has feature names that are all strings.
     */
    get feature_names_in_(): Promise<NDArray>;
}
//# sourceMappingURL=GammaRegressor.d.ts.map