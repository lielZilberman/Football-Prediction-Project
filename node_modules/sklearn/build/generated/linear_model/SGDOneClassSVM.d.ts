import { PythonBridge, NDArray, ArrayLike, SparseMatrix } from '@/sklearn/types';
/**
  Solves linear One-Class SVM using Stochastic Gradient Descent.

  This implementation is meant to be used with a kernel approximation technique (e.g. `sklearn.kernel\_approximation.Nystroem`) to obtain results similar to `sklearn.svm.OneClassSVM` which uses a Gaussian kernel by default.

  Read more in the [User Guide](../sgd.html#sgd-online-one-class-svm).

  [Python Reference](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDOneClassSVM.html)
 */
export declare class SGDOneClassSVM {
    id: string;
    opts: any;
    _py: PythonBridge;
    _isInitialized: boolean;
    _isDisposed: boolean;
    constructor(opts?: {
        /**
          The nu parameter of the One Class SVM: an upper bound on the fraction of training errors and a lower bound of the fraction of support vectors. Should be in the interval (0, 1\]. By default 0.5 will be taken.
    
          @defaultValue `0.5`
         */
        nu?: number;
        /**
          Whether the intercept should be estimated or not. Defaults to `true`.
    
          @defaultValue `true`
         */
        fit_intercept?: boolean;
        /**
          The maximum number of passes over the training data (aka epochs). It only impacts the behavior in the `fit` method, and not the `partial\_fit`. Defaults to 1000.
    
          @defaultValue `1000`
         */
        max_iter?: number;
        /**
          The stopping criterion. If it is not `undefined`, the iterations will stop when (loss > previous\_loss - tol). Defaults to 1e-3.
    
          @defaultValue `0.001`
         */
        tol?: number;
        /**
          Whether or not the training data should be shuffled after each epoch. Defaults to `true`.
    
          @defaultValue `true`
         */
        shuffle?: boolean;
        /**
          The verbosity level.
    
          @defaultValue `0`
         */
        verbose?: number;
        /**
          The seed of the pseudo random number generator to use when shuffling the data. If int, random\_state is the seed used by the random number generator; If RandomState instance, random\_state is the random number generator; If `undefined`, the random number generator is the RandomState instance used by `np.random`.
         */
        random_state?: number;
        /**
          The learning rate schedule to use with `fit`. (If using `partial\_fit`, learning rate must be controlled directly).
    
          @defaultValue `'optimal'`
         */
        learning_rate?: 'constant' | 'optimal' | 'invscaling' | 'adaptive';
        /**
          The initial learning rate for the ‘constant’, ‘invscaling’ or ‘adaptive’ schedules. The default value is 0.0 as eta0 is not used by the default schedule ‘optimal’.
    
          @defaultValue `0`
         */
        eta0?: number;
        /**
          The exponent for inverse scaling learning rate \[default 0.5\].
    
          @defaultValue `0.5`
         */
        power_t?: number;
        /**
          When set to `true`, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. See [the Glossary](../../glossary.html#term-warm_start).
    
          Repeatedly calling fit or partial\_fit when warm\_start is `true` can result in a different solution than when calling fit a single time because of the way the data is shuffled. If a dynamic learning rate is used, the learning rate is adapted depending on the number of samples already seen. Calling `fit` resets this counter, while `partial\_fit` will result in increasing the existing counter.
    
          @defaultValue `false`
         */
        warm_start?: boolean;
        /**
          When set to `true`, computes the averaged SGD weights and stores the result in the `coef\_` attribute. If set to an int greater than 1, averaging will begin once the total number of samples seen reaches average. So `average=10` will begin averaging after seeing 10 samples.
    
          @defaultValue `false`
         */
        average?: boolean | number;
    });
    get py(): PythonBridge;
    set py(pythonBridge: PythonBridge);
    /**
      Initializes the underlying Python resources.
  
      This instance is not usable until the `Promise` returned by `init()` resolves.
     */
    init(py: PythonBridge): Promise<void>;
    /**
      Disposes of the underlying Python resources.
  
      Once `dispose()` is called, the instance is no longer usable.
     */
    dispose(): Promise<void>;
    /**
      Signed distance to the separating hyperplane.
  
      Signed distance is positive for an inlier and negative for an outlier.
     */
    decision_function(opts: {
        /**
          Testing data.
         */
        X?: ArrayLike | SparseMatrix;
    }): Promise<ArrayLike>;
    /**
      Convert coefficient matrix to dense array format.
  
      Converts the `coef\_` member (back) to a numpy.ndarray. This is the default format of `coef\_` and is required for fitting, so calling this method is only required on models that have previously been sparsified; otherwise, it is a no-op.
     */
    densify(opts: {}): Promise<any>;
    /**
      Fit linear One-Class SVM with Stochastic Gradient Descent.
  
      This solves an equivalent optimization problem of the One-Class SVM primal optimization problem and returns a weight vector w and an offset rho such that the decision function is given by <w, x> - rho.
     */
    fit(opts: {
        /**
          Training data.
         */
        X?: ArrayLike | SparseMatrix;
        /**
          Not used, present for API consistency by convention.
         */
        y?: any;
        /**
          The initial coefficients to warm-start the optimization.
         */
        coef_init?: any;
        /**
          The initial offset to warm-start the optimization.
         */
        offset_init?: any;
        /**
          Weights applied to individual samples. If not provided, uniform weights are assumed. These weights will be multiplied with class\_weight (passed through the constructor) if class\_weight is specified.
         */
        sample_weight?: ArrayLike;
    }): Promise<any>;
    /**
      Perform fit on X and returns labels for X.
  
      Returns -1 for outliers and 1 for inliers.
     */
    fit_predict(opts: {
        /**
          The input samples.
         */
        X?: ArrayLike | SparseMatrix[];
        /**
          Not used, present for API consistency by convention.
         */
        y?: any;
    }): Promise<NDArray>;
    /**
      Fit linear One-Class SVM with Stochastic Gradient Descent.
     */
    partial_fit(opts: {
        /**
          Subset of the training data.
         */
        X?: ArrayLike | SparseMatrix;
        /**
          Not used, present for API consistency by convention.
         */
        y?: any;
        /**
          Weights applied to individual samples. If not provided, uniform weights are assumed.
         */
        sample_weight?: ArrayLike;
    }): Promise<any>;
    /**
      Return labels (1 inlier, -1 outlier) of the samples.
     */
    predict(opts: {
        /**
          Testing data.
         */
        X?: ArrayLike | SparseMatrix;
    }): Promise<any>;
    /**
      Raw scoring function of the samples.
     */
    score_samples(opts: {
        /**
          Testing data.
         */
        X?: ArrayLike | SparseMatrix;
    }): Promise<ArrayLike>;
    /**
      Convert coefficient matrix to sparse format.
  
      Converts the `coef\_` member to a scipy.sparse matrix, which for L1-regularized models can be much more memory- and storage-efficient than the usual numpy.ndarray representation.
  
      The `intercept\_` member is not converted.
     */
    sparsify(opts: {}): Promise<any>;
    /**
      Weights assigned to the features.
     */
    get coef_(): Promise<NDArray[]>;
    /**
      Offset used to define the decision function from the raw scores. We have the relation: decision\_function = score\_samples - offset.
     */
    get offset_(): Promise<NDArray>;
    /**
      The actual number of iterations to reach the stopping criterion.
     */
    get n_iter_(): Promise<number>;
    /**
      Number of weight updates performed during training. Same as `(n\_iter\_ \* n\_samples + 1)`.
     */
    get t_(): Promise<number>;
    get loss_function_(): Promise<any>;
    /**
      Number of features seen during [fit](../../glossary.html#term-fit).
     */
    get n_features_in_(): Promise<number>;
    /**
      Names of features seen during [fit](../../glossary.html#term-fit). Defined only when `X` has feature names that are all strings.
     */
    get feature_names_in_(): Promise<NDArray>;
}
//# sourceMappingURL=SGDOneClassSVM.d.ts.map