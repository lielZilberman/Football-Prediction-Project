import { PythonBridge, NDArray, ArrayLike, SparseMatrix } from '@/sklearn/types';
/**
  Linear perceptron classifier.

  Read more in the [User Guide](../linear_model.html#perceptron).

  [Python Reference](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html)
 */
export declare class Perceptron {
    id: string;
    opts: any;
    _py: PythonBridge;
    _isInitialized: boolean;
    _isDisposed: boolean;
    constructor(opts?: {
        /**
          The penalty (aka regularization term) to be used.
         */
        penalty?: 'l2' | 'l1' | 'elasticnet';
        /**
          Constant that multiplies the regularization term if regularization is used.
    
          @defaultValue `0.0001`
         */
        alpha?: number;
        /**
          The Elastic Net mixing parameter, with `0 <= l1\_ratio <= 1`. `l1\_ratio=0` corresponds to L2 penalty, `l1\_ratio=1` to L1. Only used if `penalty='elasticnet'`.
    
          @defaultValue `0.15`
         */
        l1_ratio?: number;
        /**
          Whether the intercept should be estimated or not. If `false`, the data is assumed to be already centered.
    
          @defaultValue `true`
         */
        fit_intercept?: boolean;
        /**
          The maximum number of passes over the training data (aka epochs). It only impacts the behavior in the `fit` method, and not the [`partial\_fit`](#sklearn.linear_model.Perceptron.partial_fit "sklearn.linear_model.Perceptron.partial_fit") method.
    
          @defaultValue `1000`
         */
        max_iter?: number;
        /**
          The stopping criterion. If it is not `undefined`, the iterations will stop when (loss > previous\_loss - tol).
    
          @defaultValue `0.001`
         */
        tol?: number;
        /**
          Whether or not the training data should be shuffled after each epoch.
    
          @defaultValue `true`
         */
        shuffle?: boolean;
        /**
          The verbosity level.
    
          @defaultValue `0`
         */
        verbose?: number;
        /**
          Constant by which the updates are multiplied.
    
          @defaultValue `1`
         */
        eta0?: number;
        /**
          The number of CPUs to use to do the OVA (One Versus All, for multi-class problems) computation. `undefined` means 1 unless in a [`joblib.parallel\_backend`](https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend "(in joblib v1.3.0.dev0)") context. `\-1` means using all processors. See [Glossary](../../glossary.html#term-n_jobs) for more details.
         */
        n_jobs?: number;
        /**
          Used to shuffle the training data, when `shuffle` is set to `true`. Pass an int for reproducible output across multiple function calls. See [Glossary](../../glossary.html#term-random_state).
    
          @defaultValue `0`
         */
        random_state?: number;
        /**
          Whether to use early stopping to terminate training when validation. score is not improving. If set to `true`, it will automatically set aside a stratified fraction of training data as validation and terminate training when validation score is not improving by at least tol for n\_iter\_no\_change consecutive epochs.
    
          @defaultValue `false`
         */
        early_stopping?: boolean;
        /**
          The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if early\_stopping is `true`.
    
          @defaultValue `0.1`
         */
        validation_fraction?: number;
        /**
          Number of iterations with no improvement to wait before early stopping.
    
          @defaultValue `5`
         */
        n_iter_no_change?: number;
        /**
          Preset for the class\_weight fit parameter.
    
          Weights associated with classes. If not given, all classes are supposed to have weight one.
    
          The “balanced” mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as `n\_samples / (n\_classes \* np.bincount(y))`.
         */
        class_weight?: any | 'balanced';
        /**
          When set to `true`, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. See [the Glossary](../../glossary.html#term-warm_start).
    
          @defaultValue `false`
         */
        warm_start?: boolean;
    });
    get py(): PythonBridge;
    set py(pythonBridge: PythonBridge);
    /**
      Initializes the underlying Python resources.
  
      This instance is not usable until the `Promise` returned by `init()` resolves.
     */
    init(py: PythonBridge): Promise<void>;
    /**
      Disposes of the underlying Python resources.
  
      Once `dispose()` is called, the instance is no longer usable.
     */
    dispose(): Promise<void>;
    /**
      Predict confidence scores for samples.
  
      The confidence score for a sample is proportional to the signed distance of that sample to the hyperplane.
     */
    decision_function(opts: {
        /**
          The data matrix for which we want to get the confidence scores.
         */
        X?: ArrayLike | SparseMatrix[];
    }): Promise<NDArray>;
    /**
      Convert coefficient matrix to dense array format.
  
      Converts the `coef\_` member (back) to a numpy.ndarray. This is the default format of `coef\_` and is required for fitting, so calling this method is only required on models that have previously been sparsified; otherwise, it is a no-op.
     */
    densify(opts: {}): Promise<any>;
    /**
      Fit linear model with Stochastic Gradient Descent.
     */
    fit(opts: {
        /**
          Training data.
         */
        X?: ArrayLike | SparseMatrix;
        /**
          Target values.
         */
        y?: NDArray;
        /**
          The initial coefficients to warm-start the optimization.
         */
        coef_init?: NDArray[];
        /**
          The initial intercept to warm-start the optimization.
         */
        intercept_init?: NDArray;
        /**
          Weights applied to individual samples. If not provided, uniform weights are assumed. These weights will be multiplied with class\_weight (passed through the constructor) if class\_weight is specified.
         */
        sample_weight?: ArrayLike;
    }): Promise<any>;
    /**
      Perform one epoch of stochastic gradient descent on given samples.
  
      Internally, this method uses `max\_iter \= 1`. Therefore, it is not guaranteed that a minimum of the cost function is reached after calling it once. Matters such as objective convergence, early stopping, and learning rate adjustments should be handled by the user.
     */
    partial_fit(opts: {
        /**
          Subset of the training data.
         */
        X?: ArrayLike | SparseMatrix;
        /**
          Subset of the target values.
         */
        y?: NDArray;
        /**
          Classes across all calls to partial\_fit. Can be obtained by via `np.unique(y\_all)`, where y\_all is the target vector of the entire dataset. This argument is required for the first call to partial\_fit and can be omitted in the subsequent calls. Note that y doesn’t need to contain all labels in `classes`.
         */
        classes?: NDArray;
        /**
          Weights applied to individual samples. If not provided, uniform weights are assumed.
         */
        sample_weight?: ArrayLike;
    }): Promise<any>;
    /**
      Predict class labels for samples in X.
     */
    predict(opts: {
        /**
          The data matrix for which we want to get the predictions.
         */
        X?: ArrayLike | SparseMatrix[];
    }): Promise<NDArray>;
    /**
      Return the mean accuracy on the given test data and labels.
  
      In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted.
     */
    score(opts: {
        /**
          Test samples.
         */
        X?: ArrayLike[];
        /**
          True labels for `X`.
         */
        y?: ArrayLike;
        /**
          Sample weights.
         */
        sample_weight?: ArrayLike;
    }): Promise<number>;
    /**
      Convert coefficient matrix to sparse format.
  
      Converts the `coef\_` member to a scipy.sparse matrix, which for L1-regularized models can be much more memory- and storage-efficient than the usual numpy.ndarray representation.
  
      The `intercept\_` member is not converted.
     */
    sparsify(opts: {}): Promise<any>;
    /**
      The unique classes labels.
     */
    get classes_(): Promise<NDArray>;
    /**
      Weights assigned to the features.
     */
    get coef_(): Promise<NDArray[][]>;
    /**
      Constants in decision function.
     */
    get intercept_(): Promise<NDArray[]>;
    /**
      The function that determines the loss, or difference between the output of the algorithm and the target values.
     */
    get loss_function_(): Promise<any>;
    /**
      Number of features seen during [fit](../../glossary.html#term-fit).
     */
    get n_features_in_(): Promise<number>;
    /**
      Names of features seen during [fit](../../glossary.html#term-fit). Defined only when `X` has feature names that are all strings.
     */
    get feature_names_in_(): Promise<NDArray>;
    /**
      The actual number of iterations to reach the stopping criterion. For multiclass fits, it is the maximum over every binary fit.
     */
    get n_iter_(): Promise<number>;
    /**
      Number of weight updates performed during training. Same as `(n\_iter\_ \* n\_samples + 1)`.
     */
    get t_(): Promise<number>;
}
//# sourceMappingURL=Perceptron.d.ts.map