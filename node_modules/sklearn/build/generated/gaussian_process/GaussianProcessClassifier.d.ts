import { PythonBridge, NDArray, ArrayLike } from '@/sklearn/types';
/**
  Gaussian process classification (GPC) based on Laplace approximation.

  The implementation is based on Algorithm 3.1, 3.2, and 5.1 from [\[RW2006\]](#r2da648a61a73-rw2006).

  Internally, the Laplace approximation is used for approximating the non-Gaussian posterior by a Gaussian.

  Currently, the implementation is restricted to using the logistic link function. For multi-class classification, several binary one-versus rest classifiers are fitted. Note that this class thus does not implement a true multi-class Laplace approximation.

  Read more in the [User Guide](../gaussian_process.html#gaussian-process).

  [Python Reference](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessClassifier.html)
 */
export declare class GaussianProcessClassifier {
    id: string;
    opts: any;
    _py: PythonBridge;
    _isInitialized: boolean;
    _isDisposed: boolean;
    constructor(opts?: {
        /**
          The kernel specifying the covariance function of the GP. If `undefined` is passed, the kernel “1.0 \* RBF(1.0)” is used as default. Note that the kernel’s hyperparameters are optimized during fitting. Also kernel cannot be a `CompoundKernel`.
         */
        kernel?: any;
        /**
          Can either be one of the internally supported optimizers for optimizing the kernel’s parameters, specified by a string, or an externally defined optimizer passed as a callable. If a callable is passed, it must have the signature:
    
          @defaultValue `'fmin_l_bfgs_b'`
         */
        optimizer?: 'fmin_l_bfgs_b';
        /**
          The number of restarts of the optimizer for finding the kernel’s parameters which maximize the log-marginal likelihood. The first run of the optimizer is performed from the kernel’s initial parameters, the remaining ones (if any) from thetas sampled log-uniform randomly from the space of allowed theta-values. If greater than 0, all bounds must be finite. Note that n\_restarts\_optimizer=0 implies that one run is performed.
    
          @defaultValue `0`
         */
        n_restarts_optimizer?: number;
        /**
          The maximum number of iterations in Newton’s method for approximating the posterior during predict. Smaller values will reduce computation time at the cost of worse results.
    
          @defaultValue `100`
         */
        max_iter_predict?: number;
        /**
          If warm-starts are enabled, the solution of the last Newton iteration on the Laplace approximation of the posterior mode is used as initialization for the next call of \_posterior\_mode(). This can speed up convergence when \_posterior\_mode is called several times on similar problems as in hyperparameter optimization. See [the Glossary](../../glossary.html#term-warm_start).
    
          @defaultValue `false`
         */
        warm_start?: boolean;
        /**
          If `true`, a persistent copy of the training data is stored in the object. Otherwise, just a reference to the training data is stored, which might cause predictions to change if the data is modified externally.
    
          @defaultValue `true`
         */
        copy_X_train?: boolean;
        /**
          Determines random number generation used to initialize the centers. Pass an int for reproducible results across multiple function calls. See [Glossary](../../glossary.html#term-random_state).
         */
        random_state?: number;
        /**
          Specifies how multi-class classification problems are handled. Supported are ‘one\_vs\_rest’ and ‘one\_vs\_one’. In ‘one\_vs\_rest’, one binary Gaussian process classifier is fitted for each class, which is trained to separate this class from the rest. In ‘one\_vs\_one’, one binary Gaussian process classifier is fitted for each pair of classes, which is trained to separate these two classes. The predictions of these binary predictors are combined into multi-class predictions. Note that ‘one\_vs\_one’ does not support predicting probability estimates.
    
          @defaultValue `'one_vs_rest'`
         */
        multi_class?: 'one_vs_rest' | 'one_vs_one';
        /**
          The number of jobs to use for the computation: the specified multiclass problems are computed in parallel. `undefined` means 1 unless in a [`joblib.parallel\_backend`](https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend "(in joblib v1.3.0.dev0)") context. `\-1` means using all processors. See [Glossary](../../glossary.html#term-n_jobs) for more details.
         */
        n_jobs?: number;
    });
    get py(): PythonBridge;
    set py(pythonBridge: PythonBridge);
    /**
      Initializes the underlying Python resources.
  
      This instance is not usable until the `Promise` returned by `init()` resolves.
     */
    init(py: PythonBridge): Promise<void>;
    /**
      Disposes of the underlying Python resources.
  
      Once `dispose()` is called, the instance is no longer usable.
     */
    dispose(): Promise<void>;
    /**
      Fit Gaussian process classification model.
     */
    fit(opts: {
        /**
          Feature vectors or other representations of training data.
         */
        X?: ArrayLike[];
        /**
          Target values, must be binary.
         */
        y?: ArrayLike;
    }): Promise<any>;
    /**
      Return log-marginal likelihood of theta for training data.
  
      In the case of multi-class classification, the mean log-marginal likelihood of the one-versus-rest classifiers are returned.
     */
    log_marginal_likelihood(opts: {
        /**
          Kernel hyperparameters for which the log-marginal likelihood is evaluated. In the case of multi-class classification, theta may be the hyperparameters of the compound kernel or of an individual kernel. In the latter case, all individual kernel get assigned the same theta values. If `undefined`, the precomputed log\_marginal\_likelihood of `self.kernel\_.theta` is returned.
         */
        theta?: ArrayLike;
        /**
          If `true`, the gradient of the log-marginal likelihood with respect to the kernel hyperparameters at position theta is returned additionally. Note that gradient computation is not supported for non-binary classification. If `true`, theta must not be `undefined`.
    
          @defaultValue `false`
         */
        eval_gradient?: boolean;
        /**
          If `true`, the kernel attribute is copied. If `false`, the kernel attribute is modified, but may result in a performance improvement.
    
          @defaultValue `true`
         */
        clone_kernel?: boolean;
    }): Promise<number>;
    /**
      Perform classification on an array of test vectors X.
     */
    predict(opts: {
        /**
          Query points where the GP is evaluated for classification.
         */
        X?: ArrayLike[];
    }): Promise<NDArray>;
    /**
      Return probability estimates for the test vector X.
     */
    predict_proba(opts: {
        /**
          Query points where the GP is evaluated for classification.
         */
        X?: ArrayLike[];
    }): Promise<ArrayLike[]>;
    /**
      Return the mean accuracy on the given test data and labels.
  
      In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted.
     */
    score(opts: {
        /**
          Test samples.
         */
        X?: ArrayLike[];
        /**
          True labels for `X`.
         */
        y?: ArrayLike;
        /**
          Sample weights.
         */
        sample_weight?: ArrayLike;
    }): Promise<number>;
    /**
      The estimator instance that defines the likelihood function using the observed data.
     */
    get base_estimator_(): Promise<any>;
    /**
      The log-marginal-likelihood of `self.kernel\_.theta`
     */
    get log_marginal_likelihood_value_(): Promise<number>;
    /**
      Unique class labels.
     */
    get classes_(): Promise<ArrayLike>;
    /**
      The number of classes in the training data
     */
    get n_classes_(): Promise<number>;
    /**
      Number of features seen during [fit](../../glossary.html#term-fit).
     */
    get n_features_in_(): Promise<number>;
    /**
      Names of features seen during [fit](../../glossary.html#term-fit). Defined only when `X` has feature names that are all strings.
     */
    get feature_names_in_(): Promise<NDArray>;
}
//# sourceMappingURL=GaussianProcessClassifier.d.ts.map