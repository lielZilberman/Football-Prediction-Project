import { PythonBridge, NDArray, ArrayLike, SparseMatrix } from '@/sklearn/types';
/**
  Mini-Batch K-Means clustering.

  Read more in the [User Guide](../clustering.html#mini-batch-kmeans).

  [Python Reference](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.MiniBatchKMeans.html)
 */
export declare class MiniBatchKMeans {
    id: string;
    opts: any;
    _py: PythonBridge;
    _isInitialized: boolean;
    _isDisposed: boolean;
    constructor(opts?: {
        /**
          The number of clusters to form as well as the number of centroids to generate.
    
          @defaultValue `8`
         */
        n_clusters?: number;
        /**
          Method for initialization:
    
          ‘k-means++’ : selects initial cluster centroids using sampling based on an empirical probability distribution of the points’ contribution to the overall inertia. This technique speeds up convergence. The algorithm implemented is “greedy k-means++”. It differs from the vanilla k-means++ by making several trials at each sampling step and choosing the best centroid among them.
    
          ‘random’: choose `n\_clusters` observations (rows) at random from data for the initial centroids.
    
          If an array is passed, it should be of shape (n\_clusters, n\_features) and gives the initial centers.
    
          If a callable is passed, it should take arguments X, n\_clusters and a random state and return an initialization.
    
          @defaultValue `'k-means++'`
         */
        init?: 'k-means++' | 'random' | ArrayLike[];
        /**
          Maximum number of iterations over the complete dataset before stopping independently of any early stopping criterion heuristics.
    
          @defaultValue `100`
         */
        max_iter?: number;
        /**
          Size of the mini batches. For faster computations, you can set the `batch\_size` greater than 256 \* number of cores to enable parallelism on all cores.
    
          @defaultValue `1024`
         */
        batch_size?: number;
        /**
          Verbosity mode.
    
          @defaultValue `0`
         */
        verbose?: number;
        /**
          Compute label assignment and inertia for the complete dataset once the minibatch optimization has converged in fit.
    
          @defaultValue `true`
         */
        compute_labels?: boolean;
        /**
          Determines random number generation for centroid initialization and random reassignment. Use an int to make the randomness deterministic. See [Glossary](../../glossary.html#term-random_state).
         */
        random_state?: number;
        /**
          Control early stopping based on the relative center changes as measured by a smoothed, variance-normalized of the mean center squared position changes. This early stopping heuristics is closer to the one used for the batch variant of the algorithms but induces a slight computational and memory overhead over the inertia heuristic.
    
          To disable convergence detection based on normalized center change, set tol to 0.0 (default).
    
          @defaultValue `0`
         */
        tol?: number;
        /**
          Control early stopping based on the consecutive number of mini batches that does not yield an improvement on the smoothed inertia.
    
          To disable convergence detection based on inertia, set max\_no\_improvement to `undefined`.
    
          @defaultValue `10`
         */
        max_no_improvement?: number;
        /**
          Number of samples to randomly sample for speeding up the initialization (sometimes at the expense of accuracy): the only algorithm is initialized by running a batch KMeans on a random subset of the data. This needs to be larger than n\_clusters.
    
          If `undefined`, the heuristic is `init\_size \= 3 \* batch\_size` if `3 \* batch\_size < n\_clusters`, else `init\_size \= 3 \* n\_clusters`.
         */
        init_size?: number;
        /**
          Number of random initializations that are tried. In contrast to KMeans, the algorithm is only run once, using the best of the `n\_init` initializations as measured by inertia. Several runs are recommended for sparse high-dimensional problems (see [Clustering sparse data with k-means](../../auto_examples/text/plot_document_clustering.html#kmeans-sparse-high-dim)).
    
          When `n\_init='auto'`, the number of runs depends on the value of init: 3 if using `init='random'`, 1 if using `init='k-means++'`.
    
          @defaultValue `3`
         */
        n_init?: 'auto' | number;
        /**
          Control the fraction of the maximum number of counts for a center to be reassigned. A higher value means that low count centers are more easily reassigned, which means that the model will take longer to converge, but should converge in a better clustering. However, too high a value may cause convergence issues, especially with a small batch size.
    
          @defaultValue `0.01`
         */
        reassignment_ratio?: number;
    });
    get py(): PythonBridge;
    set py(pythonBridge: PythonBridge);
    /**
      Initializes the underlying Python resources.
  
      This instance is not usable until the `Promise` returned by `init()` resolves.
     */
    init(py: PythonBridge): Promise<void>;
    /**
      Disposes of the underlying Python resources.
  
      Once `dispose()` is called, the instance is no longer usable.
     */
    dispose(): Promise<void>;
    /**
      Compute the centroids on X by chunking it into mini-batches.
     */
    fit(opts: {
        /**
          Training instances to cluster. It must be noted that the data will be converted to C ordering, which will cause a memory copy if the given data is not C-contiguous. If a sparse matrix is passed, a copy will be made if it’s not in CSR format.
         */
        X?: ArrayLike | SparseMatrix[];
        /**
          Not used, present here for API consistency by convention.
         */
        y?: any;
        /**
          The weights for each observation in X. If `undefined`, all observations are assigned equal weight.
         */
        sample_weight?: ArrayLike;
    }): Promise<any>;
    /**
      Compute cluster centers and predict cluster index for each sample.
  
      Convenience method; equivalent to calling fit(X) followed by predict(X).
     */
    fit_predict(opts: {
        /**
          New data to transform.
         */
        X?: ArrayLike | SparseMatrix[];
        /**
          Not used, present here for API consistency by convention.
         */
        y?: any;
        /**
          The weights for each observation in X. If `undefined`, all observations are assigned equal weight.
         */
        sample_weight?: ArrayLike;
    }): Promise<NDArray>;
    /**
      Compute clustering and transform X to cluster-distance space.
  
      Equivalent to fit(X).transform(X), but more efficiently implemented.
     */
    fit_transform(opts: {
        /**
          New data to transform.
         */
        X?: ArrayLike | SparseMatrix[];
        /**
          Not used, present here for API consistency by convention.
         */
        y?: any;
        /**
          The weights for each observation in X. If `undefined`, all observations are assigned equal weight.
         */
        sample_weight?: ArrayLike;
    }): Promise<NDArray[]>;
    /**
      Get output feature names for transformation.
  
      The feature names out will prefixed by the lowercased class name. For example, if the transformer outputs 3 features, then the feature names out are: `\["class\_name0", "class\_name1", "class\_name2"\]`.
     */
    get_feature_names_out(opts: {
        /**
          Only used to validate feature names with the names seen in [`fit`](#sklearn.cluster.MiniBatchKMeans.fit "sklearn.cluster.MiniBatchKMeans.fit").
         */
        input_features?: any;
    }): Promise<any>;
    /**
      Update k means estimate on a single mini-batch X.
     */
    partial_fit(opts: {
        /**
          Training instances to cluster. It must be noted that the data will be converted to C ordering, which will cause a memory copy if the given data is not C-contiguous. If a sparse matrix is passed, a copy will be made if it’s not in CSR format.
         */
        X?: ArrayLike | SparseMatrix[];
        /**
          Not used, present here for API consistency by convention.
         */
        y?: any;
        /**
          The weights for each observation in X. If `undefined`, all observations are assigned equal weight.
         */
        sample_weight?: ArrayLike;
    }): Promise<any>;
    /**
      Predict the closest cluster each sample in X belongs to.
  
      In the vector quantization literature, `cluster\_centers\_` is called the code book and each value returned by `predict` is the index of the closest code in the code book.
     */
    predict(opts: {
        /**
          New data to predict.
         */
        X?: ArrayLike | SparseMatrix[];
        /**
          The weights for each observation in X. If `undefined`, all observations are assigned equal weight.
         */
        sample_weight?: ArrayLike;
    }): Promise<NDArray>;
    /**
      Opposite of the value of X on the K-means objective.
     */
    score(opts: {
        /**
          New data.
         */
        X?: ArrayLike | SparseMatrix[];
        /**
          Not used, present here for API consistency by convention.
         */
        y?: any;
        /**
          The weights for each observation in X. If `undefined`, all observations are assigned equal weight.
         */
        sample_weight?: ArrayLike;
    }): Promise<number>;
    /**
      Set output container.
  
      See [Introducing the set\_output API](../../auto_examples/miscellaneous/plot_set_output.html#sphx-glr-auto-examples-miscellaneous-plot-set-output-py) for an example on how to use the API.
     */
    set_output(opts: {
        /**
          Configure output of `transform` and `fit\_transform`.
         */
        transform?: 'default' | 'pandas';
    }): Promise<any>;
    /**
      Transform X to a cluster-distance space.
  
      In the new space, each dimension is the distance to the cluster centers. Note that even if X is sparse, the array returned by `transform` will typically be dense.
     */
    transform(opts: {
        /**
          New data to transform.
         */
        X?: ArrayLike | SparseMatrix[];
    }): Promise<NDArray[]>;
    /**
      Coordinates of cluster centers.
     */
    get cluster_centers_(): Promise<NDArray[]>;
    /**
      Labels of each point (if compute\_labels is set to `true`).
     */
    get labels_(): Promise<NDArray>;
    /**
      The value of the inertia criterion associated with the chosen partition if compute\_labels is set to `true`. If compute\_labels is set to `false`, it’s an approximation of the inertia based on an exponentially weighted average of the batch inertiae. The inertia is defined as the sum of square distances of samples to their cluster center, weighted by the sample weights if provided.
     */
    get inertia_(): Promise<number>;
    /**
      Number of iterations over the full dataset.
     */
    get n_iter_(): Promise<number>;
    /**
      Number of minibatches processed.
     */
    get n_steps_(): Promise<number>;
    /**
      Number of features seen during [fit](../../glossary.html#term-fit).
     */
    get n_features_in_(): Promise<number>;
    /**
      Names of features seen during [fit](../../glossary.html#term-fit). Defined only when `X` has feature names that are all strings.
     */
    get feature_names_in_(): Promise<NDArray>;
}
//# sourceMappingURL=MiniBatchKMeans.d.ts.map