import { PythonBridge, NDArray, ArrayLike } from '@/sklearn/types';
/**
  Quadratic Discriminant Analysis.

  A classifier with a quadratic decision boundary, generated by fitting class conditional densities to the data and using Bayesâ€™ rule.

  The model fits a Gaussian density to each class.

  [Python Reference](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.html)
 */
export declare class QuadraticDiscriminantAnalysis {
    id: string;
    opts: any;
    _py: PythonBridge;
    _isInitialized: boolean;
    _isDisposed: boolean;
    constructor(opts?: {
        /**
          Class priors. By default, the class proportions are inferred from the training data.
         */
        priors?: ArrayLike;
        /**
          Regularizes the per-class covariance estimates by transforming S2 as `S2 \= (1 \- reg\_param) \* S2 + reg\_param \* np.eye(n\_features)`, where S2 corresponds to the `scaling\_` attribute of a given class.
    
          @defaultValue `0`
         */
        reg_param?: number;
        /**
          If `true`, the class covariance matrices are explicitly computed and stored in the `self.covariance\_` attribute.
    
          @defaultValue `false`
         */
        store_covariance?: boolean;
        /**
          Absolute threshold for a singular value to be considered significant, used to estimate the rank of `Xk` where `Xk` is the centered matrix of samples in class k. This parameter does not affect the predictions. It only controls a warning that is raised when features are considered to be colinear.
    
          @defaultValue `0.0001`
         */
        tol?: number;
    });
    get py(): PythonBridge;
    set py(pythonBridge: PythonBridge);
    /**
      Initializes the underlying Python resources.
  
      This instance is not usable until the `Promise` returned by `init()` resolves.
     */
    init(py: PythonBridge): Promise<void>;
    /**
      Disposes of the underlying Python resources.
  
      Once `dispose()` is called, the instance is no longer usable.
     */
    dispose(): Promise<void>;
    /**
      Apply decision function to an array of samples.
  
      The decision function is equal (up to a constant factor) to the log-posterior of the model, i.e. `log p(y \= k | x)`. In a binary classification setting this instead corresponds to the difference `log p(y \= 1 | x) \- log p(y \= 0 | x)`. See [Mathematical formulation of the LDA and QDA classifiers](../lda_qda.html#lda-qda-math).
     */
    decision_function(opts: {
        /**
          Array of samples (test vectors).
         */
        X?: ArrayLike[];
    }): Promise<NDArray>;
    /**
      Fit the model according to the given training data and parameters.
     */
    fit(opts: {
        /**
          Training vector, where `n\_samples` is the number of samples and `n\_features` is the number of features.
         */
        X?: ArrayLike[];
        /**
          Target values (integers).
         */
        y?: ArrayLike;
    }): Promise<any>;
    /**
      Perform classification on an array of test vectors X.
  
      The predicted class C for each sample in X is returned.
     */
    predict(opts: {
        /**
          Vector to be scored, where `n\_samples` is the number of samples and `n\_features` is the number of features.
         */
        X?: ArrayLike[];
    }): Promise<NDArray>;
    /**
      Return log of posterior probabilities of classification.
     */
    predict_log_proba(opts: {
        /**
          Array of samples/test vectors.
         */
        X?: ArrayLike[];
    }): Promise<NDArray[]>;
    /**
      Return posterior probabilities of classification.
     */
    predict_proba(opts: {
        /**
          Array of samples/test vectors.
         */
        X?: ArrayLike[];
    }): Promise<NDArray[]>;
    /**
      Return the mean accuracy on the given test data and labels.
  
      In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted.
     */
    score(opts: {
        /**
          Test samples.
         */
        X?: ArrayLike[];
        /**
          True labels for `X`.
         */
        y?: ArrayLike;
        /**
          Sample weights.
         */
        sample_weight?: ArrayLike;
    }): Promise<number>;
    /**
      For each class, gives the covariance matrix estimated using the samples of that class. The estimations are unbiased. Only present if `store\_covariance` is `true`.
     */
    get covariance_(): Promise<any[]>;
    /**
      Class-wise means.
     */
    get means_(): Promise<ArrayLike[]>;
    /**
      Class priors (sum to 1).
     */
    get priors_(): Promise<ArrayLike>;
    /**
      For each class k an array of shape (n\_features, n\_k), where `n\_k \= min(n\_features, number of elements in class k)` It is the rotation of the Gaussian distribution, i.e. its principal axis. It corresponds to `V`, the matrix of eigenvectors coming from the SVD of `Xk \= U S Vt` where `Xk` is the centered matrix of samples from class k.
     */
    get rotations_(): Promise<any[]>;
    /**
      For each class, contains the scaling of the Gaussian distributions along its principal axes, i.e. the variance in the rotated coordinate system. It corresponds to `S^2 / (n\_samples \- 1)`, where `S` is the diagonal matrix of singular values from the SVD of `Xk`, where `Xk` is the centered matrix of samples from class k.
     */
    get scalings_(): Promise<any[]>;
    /**
      Unique class labels.
     */
    get classes_(): Promise<NDArray>;
    /**
      Number of features seen during [fit](../../glossary.html#term-fit).
     */
    get n_features_in_(): Promise<number>;
    /**
      Names of features seen during [fit](../../glossary.html#term-fit). Defined only when `X` has feature names that are all strings.
     */
    get feature_names_in_(): Promise<NDArray>;
}
//# sourceMappingURL=QuadraticDiscriminantAnalysis.d.ts.map